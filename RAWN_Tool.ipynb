{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-10-23, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model to list the upstream catchments of each manhole (in flow splits, each split direction will both get 100% of the upstream flow). It then uses the MPF sheets to summarise population and residential/ICI areas for each catchment. Aggregated catchment contributions to each manhole are then used as input to RAWN calculations. For each manhole, the upstream catchments are dissolved together and added to a layer 'Node_Catchment'. This is used as input to 'Map Series' (similar to 'Data Driven Pages' in ArcGIS Desktop). This is used to create one jpg image per manhole, highlighting the manhole and all its upstream catchments. The RAWN calculations and images are added to RAWN spreadsheets which are similar in layout to the original RAWN sheets which are derived from Neural Network. One important difference in RAWN calculations to previous is that no peaking factor is allowed to go below 1.5, as per official RAWN guidelines. Peaking factors that reach the minimum are highlighted in yellow. Also a Google Maps link is added to each sheet. The user can choose between adding RAWN calculations as values or Excel formulas. Excel formulas are recommended for full transparency of the calculations to those using the sheets. However, python calculations are maintained in the code to enable future additional outputs (Power BI, HTML etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, numbers\n",
    "from openpyxl.formatting.rule import FormulaRule\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "import subprocess\n",
    "import gc\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "\n",
    "#Create functions for to use SQL with the model sqlite database.\n",
    "\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "\n",
    "#User Input\n",
    "try:\n",
    "\n",
    "    model = 'NSSA'\n",
    "\n",
    "    max_steps = 1005 #\n",
    "    use_formula = True\n",
    "    transfer_mixed_area_to_commercial = True\n",
    "\n",
    "    gdb_name = 'RAWN.gdb'\n",
    "    gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "    #Options to skip time consuming steps during debug (by setting to False), must be True during production runs.\n",
    "    run_dissolve = True\n",
    "    run_jpg = True\n",
    "\n",
    "    check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "    #                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "    #                      A more robust check may be developed in the future.\n",
    "\n",
    "\n",
    "    induced_values = [] #Only to be appended to under specific models where required\n",
    "\n",
    "    if model == 'NSSA':   \n",
    "        model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\NSSA\\01_MASTER_MODEL\\MODEL\\NSSA_Base_2023pop.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\NSSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'MPF Update 4'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '22602'\n",
    "        line_exclusions = []\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 93\n",
    "        excluded_acronyms_csv = ''\n",
    "\n",
    "    if model == 'FSA':   \n",
    "        model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\01_MASTER_MODEL\\MODEL\\FSA_Base_2021pop.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'MPF Update 17'\n",
    "        scenario = '2030_Network'\n",
    "        wwtp_muid = '1162'\n",
    "        line_exclusions = ['GoldenEar_Dummy_Pump','GoldenEarSSOLink','GoldenEar_SSO_Tank_Cell_Dummy_Link3','MH35_Orifice','44473']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 157\n",
    "        excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model\\FSA_Excluded_Acronyms.csv\"\n",
    "        induced_values.append(['84674','Area_Ind',257.3]) #Vancouver landfill, equivalent area to get 520 L/s PWWF.\n",
    "        \n",
    "    if model == 'VSA':   \n",
    "        model_output_folder = r\"J:\\SEWER_AREA_MODELS\\VSA\\04_ANALYSIS_WORK\\79. RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\VSA\\03_SIMULATION_WORK\\Key_Flow_HGL_GIS_Sim_VSA\\MIKE+_Import\\VSA_V304.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\VSA\\02_MODEL_COMPONENTS\\03_NETWORKS\\11. NETWORK_UPDATE\\O. V304 Updates\\VSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'Export'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '7478'\n",
    "        line_exclusions = ['51833','51834','51835','FS-241','FS-253','SADR082 SALIB DIV TO VICT','Victoria_Drive_Weir']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 304\n",
    "        excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\VSA\\04_ANALYSIS_WORK\\79. RAWN_From_Model\\VSA_Excluded_Acronyms.csv\"\n",
    "        \n",
    "    if model == 'LISA':   \n",
    "        model_output_folder = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\\Lisa_Base.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\\LISA_Master_Population.xlsx\"\n",
    "        pop_sheet = 'Export'\n",
    "        scenario = '2021_Network'\n",
    "        wwtp_muid = 'WWTP_Outlet'\n",
    "        line_exclusions = []\n",
    "        years = [2060,2070]\n",
    "        model_version = 22\n",
    "        excluded_acronyms_csv = ''   \n",
    "        \n",
    "\n",
    "    #Do not change the lines below\n",
    "    sewer_area = model \n",
    "    global_output_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder \n",
    "    \n",
    "    #Warn user if run_dissolve or run_jpg is False\n",
    "    if run_dissolve == False or run_jpg == False:\n",
    "        if MessageBox(None, b'run_dissolve and/or run_jpg is set to False.\\n\\nThis is only allowed in special cases!\\n\\nContinue?', b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please set both run_dissolve and run_jpg to True.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 3', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "    \n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    #Items: [Keyword (upper Excel header),Type ('lower Excel header'),Average(lower Excel header),Unit flow cell address, quantifyer column]\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)','$D$3','H']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$4','K']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$5','N']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)','$D$6','Q']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)','$D$7','T']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)','$D$7','T']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid == 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + str(altid)\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "    \n",
    "    sql = \"SELECT msm_Catchcon.catchid AS Catchment, msm_Catchcon.nodeid AS Connected_Node FROM msm_Catchcon INNER JOIN msm_catchment \"\n",
    "    sql += \"ON msm_Catchcon.catchid = msm_catchment.muid WHERE msm_Catchment.nettypeno <> 2 AND msm_Catchcon.Active = 1 AND msm_catchment.Active = 1 \"\n",
    "    sql += \"AND msm_catchment.enabled = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1 AND enabled = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1 AND enabled = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1 AND enabled = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1 AND enabled = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "    \n",
    "    lines = lines[~lines['MUID'].isin(line_exclusions)]\n",
    "    \n",
    "    excluded_acronyms = []\n",
    "    if excluded_acronyms_csv != '':\n",
    "        excluded_acronyms = pd.read_csv(excluded_acronyms_csv)\n",
    "        excluded_acronyms = list(excluded_acronyms.iloc[:, 0])\n",
    "\n",
    "    sql = \"SELECT muid, UPPER(acronym) AS acronym, UPPER(owner) AS owner, UPPER(assetname) AS assetname, to_outfall FROM msm_Node WHERE active = 1 AND enabled = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(((node_id_df.assetname.str[:2]=='MH') & (node_id_df.acronym.notna()) & (node_id_df.acronym!='') & \n",
    "        (node_id_df.to_outfall!=1) & (node_id_df.owner=='GV') & (~node_id_df.acronym.isin(excluded_acronyms))) | (node_id_df.muid==wwtp_muid))]\n",
    "    \n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df.loc[node_id_df['Node'] == wwtp_muid, 'ID'] = 'WWTP'\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "    duplicate_ids = node_id_df[node_id_df.duplicated('ID', keep=False)]\n",
    "    node_id_df.loc[duplicate_ids.index, 'ID'] = node_id_df['ID'] + '_(' + node_id_df['Node'] + ')'\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "\n",
    "#Import population\n",
    "\n",
    "try:\n",
    "    def warning_message(message):\n",
    "        if MessageBox(None, message.encode('utf-8'), b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please report the issue to the Master Population File admin.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df.fillna(0, inplace=True) #Fill NA with 0 or the sum of all will be NA\n",
    "    \n",
    "    if transfer_mixed_area_to_commercial:\n",
    "        pop_df.Area_Com = pop_df.Area_Com + pop_df.Area_Mixed\n",
    "        pop_df.Area_Mixed = 0\n",
    "    \n",
    "    for induced_value in induced_values:\n",
    "        catchment = induced_value[0]\n",
    "        column = induced_value[1] \n",
    "        value = induced_value[2]\n",
    "        pop_df.loc[pop_df.Catchment == catchment,column] += value\n",
    "        \n",
    "        \n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "        \n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    #Check if Pop_Total = 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (sum of all rows)\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "        message = f\"Warning. The sum of 'Population' in 'Pop_Total' ({pop_sum_total_col:,}) is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' ({pop_sum_sub_cols:,}).\"\n",
    "        message += \"\\n\\n'Pop_Total' will be used.\\n\\nContinue?\"\n",
    "        warning_message(message)\n",
    "     \n",
    "    #Check if some rows have population but 0 area\n",
    "    res_area_check_df = pop_df[(pop_df.Population > 0) & (pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed == 0)]\n",
    "    if len(res_area_check_df) > 0:\n",
    "        warning_message(f\"{len(res_area_check_df):,} out of {len(pop_df):,} rows have population but 0 residential area.\\n\\nPrint dataframe 'res_area_check_df' to see all.\\n\\nContinue?\")\n",
    "    \n",
    "    #Check is some catchment/year combinations are not found.\n",
    "    catchment_years = []\n",
    "    for muid in list(catchments.Catchment.unique()):\n",
    "        for year in years:\n",
    "            catchment_years.append([muid,year])\n",
    "    catchment_year_df = pd.DataFrame(catchment_years,columns=(['Catchment','Year']))\n",
    "    merged = catchment_year_df.merge(pop_df[['Catchment', 'Year']], on=['Catchment', 'Year'], how='left', indicator=True)\n",
    "    not_founds = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    if len(not_founds) > 0:\n",
    "        message = \"The following catchment/year combinations are not found:\\n\\n\"\n",
    "        for index, row in not_founds[:20].iterrows():\n",
    "            message += row[0] + ', ' + str(row[1]) + '.\\n'\n",
    "        if len(not_founds) > 20:\n",
    "             message += \"and more, print variable 'not_founds' to see the rest.\"                                                 \n",
    "        message += '\\n\\nContinue?'\n",
    "        warning_message(message)\n",
    "                                                                      \n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "        \n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]  \n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                \n",
    "#                 catchment_mus.append(catchment)\n",
    "#                 print('skipped for ' + catchment)\n",
    "#                 break\n",
    "                \n",
    "                \n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Last node traced is '\" + node + \"'\")\n",
    "                \n",
    "                \n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl']:\n",
    "                    adwf += avg_flow    \n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)),1.5) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*0.8,1.5)*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)),1.5) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    \n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = np.maximum(\n",
    "        0.8 * (\n",
    "            1 + 14 / (\n",
    "                4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5\n",
    "            )\n",
    "        ) * np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ), \n",
    "        1.5\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    \n",
    "    node_df[('FLOWS','PWWF (L/s)')] = node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] + node_df[('COMMERCIAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] + node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] + node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending msm_CatchCon\n",
      "Appending msm_Catchment\n",
      "Appending msm_Link\n",
      "Appending msm_Node\n",
      "Appending msm_Pump\n",
      "Appending msm_Weir\n",
      "Appending msm_Orifice\n",
      "Appending msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "\n",
    "#Import GIS from the model\n",
    "#Note that if the RAWN database does not already exist with the layers inside, then it will be created \n",
    "#but in this casethe map symbology will be reset and Map Series reset as well, creating extra work.\n",
    "#An extra word of caution: The first import of e.g. msm_Catchment sets the maximum extend, meaning if you import NSSA in a fresh database,\n",
    "#then later import FSA, then many catchments will not be imported due to being outside the extend of the old import.\n",
    "#In the current database, an initial model with added elements outside the extend of all MV models was used to prime the layer. \n",
    "#If the database is not maintained, this will need to be done again.\n",
    "#Efforts to programmatically increase the layer extend were unsuccessful but may later get resolved.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(out_path + '\\\\' + layer):\n",
    "            print('Appending ' + layer)\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:  \n",
    "            print('Creating ' + layer)\n",
    "\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer + '_Test')\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        arcpy.Project_management('msm_Node', 'msm_Node_Google',arcpy.SpatialReference(4326))\n",
    "        centroids = arcpy.da.FeatureClassToNumPyArray('msm_Node_Google', (\"MUID\",\"SHAPE@X\",\"SHAPE@Y\"))\n",
    "        centroids = centroids.tolist()\n",
    "        centroids_df = pd.DataFrame(centroids, columns =['MUID','X','Y'])\n",
    "        centroids_df.set_index('MUID',inplace=True)\n",
    "\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "\n",
    "#Create merge_df (realizing the terms 'merge' and 'dissolve' are used here but they mean the same), \n",
    "#minimizing the number of computation heavy dissolves that need to be done.\n",
    "#It prevents the duplicate effort of merging the same catchments together multiple times,\n",
    "#by moving downstream and reusing upstream merges for downstream merges.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_57, 0 of 101 at time 2024-10-23 09:22:03.635395\n",
      "Dissolving for Merge_ID_12, 1 of 101 at time 2024-10-23 09:22:51.939259\n",
      "Dissolving for Merge_ID_84, 2 of 101 at time 2024-10-23 09:23:22.095268\n",
      "Dissolving for Merge_ID_51, 3 of 101 at time 2024-10-23 09:23:59.325825\n",
      "Dissolving for Merge_ID_39, 4 of 101 at time 2024-10-23 09:24:26.589138\n",
      "Dissolving for Merge_ID_3, 5 of 101 at time 2024-10-23 09:24:50.811626\n",
      "Dissolving for Merge_ID_7, 6 of 101 at time 2024-10-23 09:25:16.601853\n",
      "Dissolving for Merge_ID_67, 7 of 101 at time 2024-10-23 09:26:56.926123\n",
      "Dissolving for Merge_ID_99, 8 of 101 at time 2024-10-23 09:27:19.454038\n",
      "Dissolving for Merge_ID_8, 9 of 101 at time 2024-10-23 09:27:40.771829\n",
      "Dissolving for Merge_ID_29, 10 of 101 at time 2024-10-23 09:28:01.412897\n",
      "Dissolving for Merge_ID_15, 11 of 101 at time 2024-10-23 09:28:48.069213\n",
      "Dissolving for Merge_ID_38, 12 of 101 at time 2024-10-23 09:29:14.680915\n",
      "Dissolving for Merge_ID_75, 13 of 101 at time 2024-10-23 09:29:49.416149\n",
      "Dissolving for Merge_ID_45, 14 of 101 at time 2024-10-23 09:30:16.236964\n",
      "Dissolving for Merge_ID_59, 15 of 101 at time 2024-10-23 09:30:48.309727\n",
      "Dissolving for Merge_ID_28, 16 of 101 at time 2024-10-23 09:31:18.809030\n",
      "Dissolving for Merge_ID_40, 17 of 101 at time 2024-10-23 09:31:48.306404\n",
      "Dissolving for Merge_ID_55, 18 of 101 at time 2024-10-23 09:32:06.380133\n",
      "Dissolving for Merge_ID_22, 19 of 101 at time 2024-10-23 09:32:38.208670\n",
      "Dissolving for Merge_ID_96, 20 of 101 at time 2024-10-23 09:33:38.481603\n",
      "Dissolving for Merge_ID_10, 21 of 101 at time 2024-10-23 09:34:04.672406\n",
      "Dissolving for Merge_ID_27, 22 of 101 at time 2024-10-23 09:34:29.660585\n",
      "Dissolving for Merge_ID_101, 23 of 101 at time 2024-10-23 09:34:50.630036\n",
      "Dissolving for Merge_ID_83, 24 of 101 at time 2024-10-23 09:35:55.752915\n",
      "Dissolving for Merge_ID_94, 25 of 101 at time 2024-10-23 09:36:22.116370\n",
      "Dissolving for Merge_ID_85, 26 of 101 at time 2024-10-23 09:36:43.286007\n",
      "Dissolving for Merge_ID_54, 27 of 101 at time 2024-10-23 09:37:57.749609\n",
      "Dissolving for Merge_ID_41, 28 of 101 at time 2024-10-23 09:39:28.399227\n",
      "Dissolving for Merge_ID_87, 29 of 101 at time 2024-10-23 09:40:14.966414\n",
      "Dissolving for Merge_ID_16, 30 of 101 at time 2024-10-23 09:41:13.339543\n",
      "Dissolving for Merge_ID_82, 31 of 101 at time 2024-10-23 09:41:51.975370\n",
      "Dissolving for Merge_ID_60, 32 of 101 at time 2024-10-23 09:42:35.462663\n",
      "Dissolving for Merge_ID_86, 33 of 101 at time 2024-10-23 09:43:31.663778\n",
      "Dissolving for Merge_ID_47, 34 of 101 at time 2024-10-23 09:44:08.941950\n",
      "Dissolving for Merge_ID_100, 35 of 101 at time 2024-10-23 09:44:42.429990\n",
      "Dissolving for Merge_ID_93, 36 of 101 at time 2024-10-23 09:45:06.210032\n",
      "Dissolving for Merge_ID_98, 37 of 101 at time 2024-10-23 09:45:24.899355\n",
      "Dissolving for Merge_ID_4, 38 of 101 at time 2024-10-23 09:45:54.178493\n",
      "Dissolving for Merge_ID_46, 39 of 101 at time 2024-10-23 09:46:36.140886\n",
      "Dissolving for Merge_ID_9, 40 of 101 at time 2024-10-23 09:47:18.003689\n",
      "Dissolving for Merge_ID_31, 41 of 101 at time 2024-10-23 09:47:55.461408\n",
      "Dissolving for Merge_ID_65, 42 of 101 at time 2024-10-23 09:48:26.884022\n",
      "Dissolving for Merge_ID_79, 43 of 101 at time 2024-10-23 09:48:43.740646\n",
      "Dissolving for Merge_ID_77, 44 of 101 at time 2024-10-23 09:49:09.920911\n",
      "Dissolving for Merge_ID_0, 45 of 101 at time 2024-10-23 09:49:40.336094\n",
      "Dissolving for Merge_ID_32, 46 of 101 at time 2024-10-23 09:50:01.031270\n",
      "Dissolving for Merge_ID_14, 47 of 101 at time 2024-10-23 09:50:21.799012\n",
      "Dissolving for Merge_ID_37, 48 of 101 at time 2024-10-23 09:50:36.803916\n",
      "Dissolving for Merge_ID_33, 49 of 101 at time 2024-10-23 09:52:11.749404\n",
      "Dissolving for Merge_ID_68, 50 of 101 at time 2024-10-23 09:52:33.528585\n",
      "Dissolving for Merge_ID_48, 51 of 101 at time 2024-10-23 09:53:06.741359\n",
      "Dissolving for Merge_ID_78, 52 of 101 at time 2024-10-23 09:53:23.667043\n",
      "Dissolving for Merge_ID_35, 53 of 101 at time 2024-10-23 09:53:45.229022\n",
      "Dissolving for Merge_ID_92, 54 of 101 at time 2024-10-23 09:54:19.610530\n",
      "Dissolving for Merge_ID_69, 55 of 101 at time 2024-10-23 09:54:33.813685\n",
      "Dissolving for Merge_ID_20, 56 of 101 at time 2024-10-23 09:55:02.303071\n",
      "Dissolving for Merge_ID_80, 57 of 101 at time 2024-10-23 09:55:44.519172\n",
      "Dissolving for Merge_ID_25, 58 of 101 at time 2024-10-23 09:56:00.945386\n",
      "Dissolving for Merge_ID_1, 59 of 101 at time 2024-10-23 09:56:23.919289\n",
      "Dissolving for Merge_ID_21, 60 of 101 at time 2024-10-23 09:57:31.963312\n",
      "Dissolving for Merge_ID_90, 61 of 101 at time 2024-10-23 09:58:05.315202\n",
      "Dissolving for Merge_ID_36, 62 of 101 at time 2024-10-23 09:58:27.328175\n",
      "Dissolving for Merge_ID_89, 63 of 101 at time 2024-10-23 09:59:40.557567\n",
      "Dissolving for Merge_ID_2, 64 of 101 at time 2024-10-23 10:00:06.630708\n",
      "Dissolving for Merge_ID_81, 65 of 101 at time 2024-10-23 10:00:26.819526\n",
      "Dissolving for Merge_ID_44, 66 of 101 at time 2024-10-23 10:00:50.591536\n",
      "Dissolving for Merge_ID_64, 67 of 101 at time 2024-10-23 10:01:25.545900\n",
      "Dissolving for Merge_ID_72, 68 of 101 at time 2024-10-23 10:01:56.977002\n",
      "Dissolving for Merge_ID_95, 69 of 101 at time 2024-10-23 10:02:17.855836\n",
      "Dissolving for Merge_ID_58, 70 of 101 at time 2024-10-23 10:02:38.119599\n",
      "Dissolving for Merge_ID_30, 71 of 101 at time 2024-10-23 10:02:57.769793\n",
      "Dissolving for Merge_ID_61, 72 of 101 at time 2024-10-23 10:03:19.786178\n",
      "Dissolving for Merge_ID_66, 73 of 101 at time 2024-10-23 10:03:38.728716\n",
      "Dissolving for Merge_ID_19, 74 of 101 at time 2024-10-23 10:03:58.883378\n",
      "Dissolving for Merge_ID_50, 75 of 101 at time 2024-10-23 10:04:23.362141\n",
      "Dissolving for Merge_ID_70, 76 of 101 at time 2024-10-23 10:05:40.481522\n",
      "Dissolving for Merge_ID_26, 77 of 101 at time 2024-10-23 10:06:11.688556\n",
      "Dissolving for Merge_ID_6, 78 of 101 at time 2024-10-23 10:06:35.545638\n",
      "Dissolving for Merge_ID_91, 79 of 101 at time 2024-10-23 10:07:45.915773\n",
      "Dissolving for Merge_ID_88, 80 of 101 at time 2024-10-23 10:09:58.151815\n",
      "Dissolving for Merge_ID_43, 81 of 101 at time 2024-10-23 10:10:34.010006\n",
      "Dissolving for Merge_ID_73, 82 of 101 at time 2024-10-23 10:11:15.229146\n",
      "Dissolving for Merge_ID_24, 83 of 101 at time 2024-10-23 10:11:48.724139\n",
      "Dissolving for Merge_ID_5, 84 of 101 at time 2024-10-23 10:13:24.551315\n",
      "Dissolving for Merge_ID_52, 85 of 101 at time 2024-10-23 10:13:59.224398\n",
      "Dissolving for Merge_ID_34, 86 of 101 at time 2024-10-23 10:14:31.711371\n",
      "Dissolving for Merge_ID_17, 87 of 101 at time 2024-10-23 10:15:09.567404\n",
      "Dissolving for Merge_ID_62, 88 of 101 at time 2024-10-23 10:16:56.892895\n",
      "Dissolving for Merge_ID_56, 89 of 101 at time 2024-10-23 10:17:57.752218\n",
      "Dissolving for Merge_ID_74, 90 of 101 at time 2024-10-23 10:18:56.963519\n",
      "Dissolving for Merge_ID_49, 91 of 101 at time 2024-10-23 10:20:30.390998\n",
      "Dissolving for Merge_ID_97, 92 of 101 at time 2024-10-23 10:20:52.934354\n",
      "Dissolving for Merge_ID_13, 93 of 101 at time 2024-10-23 10:21:16.823459\n",
      "Dissolving for Merge_ID_63, 94 of 101 at time 2024-10-23 10:21:50.282932\n",
      "Dissolving for Merge_ID_11, 95 of 101 at time 2024-10-23 10:22:09.543251\n",
      "Dissolving for Merge_ID_18, 96 of 101 at time 2024-10-23 10:22:55.111104\n",
      "Dissolving for Merge_ID_76, 97 of 101 at time 2024-10-23 10:23:26.371023\n",
      "Dissolving for Merge_ID_42, 98 of 101 at time 2024-10-23 10:23:56.110040\n",
      "Dissolving for Merge_ID_23, 99 of 101 at time 2024-10-23 10:24:32.226481\n",
      "Dissolving for Merge_ID_71, 100 of 101 at time 2024-10-23 10:25:09.891320\n",
      "Dissolving for Merge_ID_53, 101 of 101 at time 2024-10-23 10:25:34.478081\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "\n",
    "#Run dissolve (also referred to as merge).\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_name_dissolve)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df.iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "\n",
    "#Export jpgs. \n",
    "#This process was too heavy to run inside this notebook, causing failed exports and freeze of the program.\n",
    "#It was therefore moved to an external script, called from here by writing a batch file and executing it.\n",
    "\n",
    "try:\n",
    "    \n",
    "    if run_jpg:\n",
    "       \n",
    "        aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "        project_path = aprx.filePath\n",
    "        project_folder = os.path.dirname(project_path)\n",
    "\n",
    "        jpg_folder = model_output_folder + r'\\jpg'\n",
    "        if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "        jpg_script = project_folder + '\\\\JPG_Subprocess.py'\n",
    "        bat_file_path = project_folder + '\\\\Execute_JPG.bat'\n",
    "        bat_file = open(bat_file_path, \"w\")\n",
    "        python_installation = sys.executable\n",
    "        python_installation = os.path.dirname(sys.executable) + r'\\Python\\envs\\arcgispro-py3\\python.exe'\n",
    "\n",
    "        bat_file_text = '@echo off\\n'\n",
    "        bat_file_text += 'set PYTHON_PATH=\"' + python_installation + '\"\\n'\n",
    "        bat_file_text += 'set SCRIPT_PATH=\"JPG_Subprocess.py\"\\n'\n",
    "        bat_file_text += 'set ARG1=\"' + project_path + '\"\\n'\n",
    "        bat_file_text += 'set ARG2=\"' + jpg_folder + '\"\\n'\n",
    "        bat_file_text += '%PYTHON_PATH% %SCRIPT_PATH% %ARG1% %ARG2%\\n'\n",
    "\n",
    "        bat_file.write(bat_file_text)\n",
    "        bat_file.close()\n",
    "        result = subprocess.call([bat_file_path]) \n",
    "\n",
    "        if result == 1: #Error\n",
    "            raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "            then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "\n",
    "        print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "    \n",
    "    username = os.getlogin()\n",
    "    date_created = str(datetime.datetime.today()).split(' ')[0]\n",
    "    \n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    backup_folder = f'{excel_folder}\\\\Backup_{date_created}_{model}_V{model_version}_{pop_sheet}'\n",
    "    \n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    if not os.path.isdir(backup_folder): os.makedirs(backup_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "        if use_formula: #Replace spreadsheet values with formulas\n",
    "            value_startrow = 17\n",
    "            for i in node_single_df.index:\n",
    "                currentrow = i + value_startrow -1\n",
    "                for avg_calc_dict_key in avg_calc_dict:\n",
    "#                     currentrow = i + value_startrow\n",
    "                    inputs = avg_calc_dict[avg_calc_dict_key]\n",
    "                    header = inputs[0]\n",
    "                    subheader = inputs[2]\n",
    "                    unitflow_ref = inputs[3]\n",
    "                    col_ref = inputs[4]\n",
    "                    formula = f'{r\"=\"}{unitflow_ref}*{col_ref}{currentrow}/86400'\n",
    "                    node_single_df.loc[i,(header,subheader)] = formula                \n",
    "                \n",
    "                node_single_df.loc[i,('RESIDENTIAL','PEAK FLOW (L/s)')] = f'{r\"=\"}MAX(1.5,(1+(14/(4+((H{currentrow}/1000)^0.5)))))*I{currentrow}'\n",
    "                node_single_df.loc[i,('COMMERCIAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"com\"][3]}*K{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5))))*0.8)*L{currentrow}'\n",
    "                \n",
    "                ind_formula = f'{r\"=\"}MAX(1.5,0.8*(1+((14)/(4+(((N{currentrow}*{avg_calc_dict[\"ind\"][3]}/{avg_calc_dict[\"res\"][3]}'\n",
    "                ind_formula += f')/1000)^0.5))))*IF(N{currentrow}<121,1.7,(2.505-0.1673*LN(N{currentrow}))))*O{currentrow}'\n",
    "                node_single_df.loc[i,('INDUSTRIAL','PEAK FLOW (L/s)')] = ind_formula\n",
    "                \n",
    "                node_single_df.loc[i,('INSTITUTIONAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"inst\"][3]}*Q{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5)))))*R{currentrow}'\n",
    "\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','AREA (Ha)')] = f'{r\"=\"}G{currentrow}+K{currentrow}+N{currentrow}+Q{currentrow}'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFLOW (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infl\"][3]}/86400'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFILTRATION (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infi\"][3]}/86400'\n",
    "                \n",
    "                node_single_df.loc[i,('FLOWS','AVG. SAN. FLOW (L/s)')] = f'{r\"=\"}I{currentrow}+L{currentrow}+O{currentrow}+R{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','ADWF (L/s)')] = f'{r\"=\"}W{currentrow}+V{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','PWWF (L/s)')] = \\\n",
    "                    f'{r\"=\"}J{currentrow}+M{currentrow}+P{currentrow}+S{currentrow}+U{currentrow}+V{currentrow}'\n",
    "                \n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        id = id.replace('\\\\','_')\n",
    "        id = id.upper()\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "            workbook.create_sheet(\"Disclaimer\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 23 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13\n",
    "        \n",
    "        #Delete empty row between header and data\n",
    "        sheet1.delete_rows(16)\n",
    "        \n",
    "        #Find the minimum factor in the sheet. Array formulas do not work with openpyxl, do divisions one by one instead\n",
    "        cols = [['I','J'],['L','M'],['O','P'],['R','S']]\n",
    "        rows = list(range(16,currentrow + 1))      \n",
    "        formula = r'=ROUND(MIN('\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                formula += f'IFERROR({col[1]}{row}/{col[0]}{row},999),'\n",
    "        formula = formula[:-1] + '),2)'        \n",
    "        sheet1['J9'] = formula\n",
    "        sheet1['H9'] = 'Lowest peaking factor:'\n",
    "        sheet1['H10'] = 'Lowest peaking factors at lower limit 1.5 highlighted in yellow.'\n",
    "                      \n",
    "        #Color code cells where peaking facor is at minimum.\n",
    "        format_formula = 'J16/I16<1.501'\n",
    "        format_range = ''\n",
    "        for col in cols:\n",
    "            format_range += f'{col[1]}16:{col[1]}{currentrow} '\n",
    "        format_range = format_range[:-1] #Remove last space.               \n",
    "        fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "        rule = FormulaRule(formula=[format_formula], fill=fill)\n",
    "        sheet1.conditional_formatting.add(format_range, rule)\n",
    "        \n",
    "        decimals = [['#,##0','H'],['#,##0.0','IJLMOPRSUVWXY'],['#,##0.00','GKNQT']]\n",
    "        for decimal in decimals:\n",
    "            for col in decimal[1]:\n",
    "                for row in range(16,currentrow+1):\n",
    "                    sheet1[f'{col}{row}'].number_format = decimal[0]\n",
    "        \n",
    "        display_name = \"Open in Google Maps\"\n",
    "        google_map_string = 'https://maps.google.com/?q='\n",
    "        google_map_string += str(centroids_df.loc[muid,'Y']) + ', '\n",
    "        google_map_string += str(centroids_df.loc[muid,'X']) \n",
    "\n",
    "        # Adding a hyperlink with a display name\n",
    "        cell = sheet1.cell(row=10, column=3, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "                      \n",
    "        sheet = workbook[\"Map\"]\n",
    "        \n",
    "        #Add source info\n",
    "        source_infos = []\n",
    "        source_infos.append(['Date:',date_created])\n",
    "        source_infos.append(['Model area:',model])\n",
    "        source_infos.append(['Model version:',model_version])\n",
    "        source_infos.append(['Population file:',os.path.basename(pop_book)])\n",
    "        source_infos.append(['Population sheet:',pop_sheet ])\n",
    "        \n",
    "        for i, source_info in enumerate(source_infos):\n",
    "            sheet1[f'H{i+2}']  = source_info[0]\n",
    "            sheet1[f'J{i+2}']  = source_info[1]\n",
    "        \n",
    "        for row in sheet1['H2:H9']:\n",
    "            for cell in row:\n",
    "                cell.font = Font(bold=True)\n",
    "        \n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        sheet.add_image(img, 'B3')\n",
    "        \n",
    "        cell = sheet.cell(row=1, column=2, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "        \n",
    "        sheet_disclaimer = workbook[\"Disclaimer\"]\n",
    "        sheet_disclaimer.merge_cells('A1:Z1')\n",
    "        sheet_disclaimer.merge_cells('A2:Z28')\n",
    "        sheet_disclaimer['A1'] = 'Disclaimer'\n",
    "        sheet_disclaimer['A1'].font = Font(bold=True, size=16)\n",
    "        disclaimer = 'Liquid Waste Services – Policy, Planning, & Analysis (LWS-PPA) has prepared the Flow & HGL forecasts for use by LWS-PPA for planning purposes. No warranty, expressed or implied is made regarding how accurate the forecasts will be to actual conditions observed at the site.'\n",
    "        disclaimer += \"\\n\\nLWS-PPA assumes no liability concerning your use of this information or any errors or omissions therein. Any reliance on the information's accuracy or completeness is at your own risk.\"\n",
    "        disclaimer += '\\n\\nProceeding beyond this Disclaimer will constitute your acceptance of the terms and conditions outlined above.'\n",
    "        sheet_disclaimer['A2'] = disclaimer\n",
    "        sheet_disclaimer['A2'].alignment = Alignment(wrap_text=True, vertical='top')\n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "        shutil.copy(sheetpath,backup_folder + \"\\\\\" + id + \".xlsx\")\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'All cells ran successfully.\\n\\nPlease check the following folder (not its backup subfolders) for obsolete sheets (from previous rounds not overwritten) and delete them. Sort by date to see which ones.\\n\\n' + model_output_folder + '\\\\Excel'\n",
    "MessageBox(None,message.encode('utf-8'), b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
