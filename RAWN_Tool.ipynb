{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "# User Input, to move to separate sheet so no permanent cell\n",
    "#stop trace if more pipes than max_steps traced from catchment, must be an endless loop. \n",
    "max_steps = 1000 \n",
    "\n",
    "update_field_in_model = True\n",
    "update_field = 'Description'\n",
    "\n",
    "output_folder = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output\"\n",
    "model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "sewer_area = 'NSSA'\n",
    "pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "pop_sheet = 'MPF Update 4'\n",
    "model = 'NSSA'\n",
    "gdb_name = 'RAWN.gdb'\n",
    "gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "#Options to skip time consuming steps during debug, must be True during production runs\n",
    "run_dissolve = False\n",
    "run_dissolve_append = False\n",
    "run_jpg = True\n",
    "run_import = False\n",
    "run_html = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "#Set up column names\n",
    "\n",
    "years = [2060,2070,2080,2090,2100]\n",
    "categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "mpf_col_dict = {}\n",
    "\n",
    "area_col_dict = {}\n",
    "area_col_dict['res'] = 'Area_Res'\n",
    "area_col_dict['com'] = 'Area_Com'\n",
    "area_col_dict['ind'] = 'Area_Ind'\n",
    "area_col_dict['inst'] = 'Area_Inst'\n",
    "area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "per_unit_dict = {}\n",
    "per_unit_dict['res'] = 320\n",
    "per_unit_dict['com'] = 33700 \n",
    "per_unit_dict['ind'] = 56200\n",
    "per_unit_dict['inst'] = 33700\n",
    "per_unit_dict['infl'] = 5600\n",
    "per_unit_dict['infi'] = 5600\n",
    "\n",
    "header_dict = {}\n",
    "# header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "avg_calc_dict = {}\n",
    "avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)']\n",
    "avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)']\n",
    "\n",
    "header_tuples = []\n",
    "for header in header_dict:\n",
    "    for sub_header in (header_dict[header][1]):\n",
    "        header_tuples.append((header_dict[header][0],sub_header))\n",
    "header_tuples\n",
    "\n",
    "# columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "#Import population\n",
    "pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "pop_sum_total_col = int(pop_df.Population.sum())\n",
    "pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "if pop_sum_total_col != pop_sum_sub_cols:\n",
    "      raise ValueError(\"Error. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "#Import model data\n",
    "\n",
    "node_types = {}\n",
    "node_types[1] = 'Manhole'\n",
    "node_types[2] = 'Basin'\n",
    "node_types[3] = 'Outlet'\n",
    "node_types[4] = 'Junction'\n",
    "node_types[5] = 'Soakaway'\n",
    "node_types[6] = 'River Junction'\n",
    "\n",
    "sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "lines = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "orifices = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,orifices])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "valves = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,valves])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "weirs = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,weirs])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "pumps = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,pumps])\n",
    "\n",
    "lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "\n",
    "sql = \"SELECT muid, acronym, assetname FROM msm_Node WHERE active = 1\"\n",
    "node_id_df = sql_to_df(sql,model_path)\n",
    "node_id_df = node_id_df[(node_id_df.assetname.str[:2]=='MH') & (node_id_df.assetname.str.len() > 2) & (node_id_df.acronym.notna())]\n",
    "node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "node_id_df = node_id_df[['Node','ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "accumulated_catchment_set = set()\n",
    "accumulated_node_set = set()\n",
    "\n",
    "for index1, row1 in catchments.iterrows():\n",
    "    catchment = row1['Catchment']\n",
    "    nodes = [row1['Connected_Node']]\n",
    "    start_node = row1['Connected_Node']\n",
    "    steps = 0\n",
    "    \n",
    "    accumulated_catchment_set.add((start_node,catchment))\n",
    "        \n",
    "    while steps <= max_steps:\n",
    "        steps += 1\n",
    "        downstream_df = lines[lines['From'].isin(nodes)]\n",
    "\n",
    "        if len(downstream_df) > 0:\n",
    "            nodes = list(downstream_df.To.unique())\n",
    "\n",
    "            nodes = [node for node in nodes if len(node)>0]\n",
    "            for node in nodes:\n",
    "                accumulated_catchment_set.add((node,catchment))       \n",
    "        else:\n",
    "            break\n",
    "        if steps == max_steps:\n",
    "            raise ValueError(\"Maximum steps were reached, indicating a loop. Start catchment is '\" + catchment + \"'\")\n",
    "           \n",
    "        accumulated_catchment_set.add((node,catchment))\n",
    "        \n",
    "accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "data = {\n",
    "    ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "    ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "    ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "}\n",
    "\n",
    "# Create a DataFrame with MultiIndex columns\n",
    "accumulation_df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "catchment_df = df_template.copy()\n",
    "for catchment in catchments:\n",
    "    for year in years:\n",
    "        key = model + '@' + catchment + '@' + str(year)\n",
    "        catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'UNKNOWN'\n",
    "        catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "        catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "        catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "        for area_col_dict_key in area_col_dict:\n",
    "            catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "        catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "        san_flow = 0\n",
    "        adwf = 0\n",
    "        for avg_calc_dict_key in avg_calc_dict:\n",
    "            input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "            input2 = per_unit_dict[avg_calc_dict_key]\n",
    "            avg_flow = input1 * input2 / 86400\n",
    "            adwf += avg_flow\n",
    "            if avg_calc_dict_key not in ['infl','infi']:\n",
    "                san_flow += avg_flow\n",
    "            catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "        catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "        catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "        \n",
    "catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "node_df = catchment_node_df.copy()\n",
    "node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "node_df.reset_index(inplace=True)\n",
    "node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]*0.8\n",
    "node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = (\n",
    "    0.8 * (1 + 14 / (4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5)) *\n",
    "    np.where(\n",
    "        node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "        1.7,\n",
    "        2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    ")\n",
    "\n",
    "node_df[('FLOWS','PWWF (L/s)')] = (\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    ")\n",
    "\n",
    "excel_folder = output_folder + '\\\\Excel'\n",
    "if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "    node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id]\n",
    "    id = id.replace('/','-') if '/' in id else id\n",
    "    node_single_df.to_excel(excel_folder + '\\\\' + id + '.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test cell\n",
    "# aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "# m = aprx.listMaps('Map')[0]\n",
    "# for lyr in m.listLayers():\n",
    "#   if lyr.isFeatureLayer:\n",
    "#     print (lyr.name)\n",
    "#     sym = lyr.symbology\n",
    "#     if lyr.name == 'msm_CatchCon':\n",
    "#         sym.renderer.symbol.color = {'RGB': [255, 0, 0]}  # Red color\n",
    "#         lyr.symbology = sym\n",
    "# aprx.save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "# out_path = r'J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output' + '\\\\' + gdb_name\n",
    "# arcpy.env.workspace = out_path\n",
    "# sr = arcpy.SpatialReference(26910)\n",
    "# layers = ['msm_CatchCon']\n",
    "# arcpy.env.overwriteOutput = True\n",
    "# for layer in layers:\n",
    "#     arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "#     arcpy.DefineProjection_management(layer, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path = r'J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output' + '\\\\' + gdb_name\n",
    "# arcpy.env.workspace = out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 9\n",
    "#Import GIS from the model\n",
    "if run_import:\n",
    "\n",
    "    out_path = output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "    #     arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(layer):\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:    \n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer)\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm to speed things up\n",
    "#Add a level to the trace, 1 being most upstream\n",
    "#Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "#Dissolve catchments\n",
    "arcpy.env.addOutputsToMap = False\n",
    "if run_dissolve:\n",
    "    arcpy.management.CreateFileGDB(output_folder, gdb_name_dissolve)\n",
    "    dissolve_path = output_folder + '\\\\' + gdb_name_dissolve\n",
    "    arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'msm_Catchment')\n",
    "    nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "    for i, node in enumerate(nodes):\n",
    "        print('Dissolving for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "        catchment_df = accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node]\n",
    "        catchments = list(catchment_df[('GENERAL INFO','CATCHMENT')].unique())\n",
    "        arcpy.management.CalculateField(dissolve_path + '\\\\msm_Catchment', \"Drains_To\", \"''\", \"PYTHON3\")\n",
    "        with arcpy.da.UpdateCursor(dissolve_path + '\\\\msm_catchment', ['muid', 'Drains_To']) as cursor:\n",
    "            for row in cursor:\n",
    "                if row[0] in catchments:\n",
    "                    row[1] = node\n",
    "                    cursor.updateRow(row)\n",
    "\n",
    "        query = \"Drains_To = 'Test'\"\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\msm_catchment', \"temp_layer\", \"Drains_To = '\" + node + \"'\")\n",
    "        dissolve_output = dissolve_path + '\\\\msm_Catchment_Dissolve_Single'\n",
    "        arcpy.management.Dissolve(\"temp_layer\", dissolve_output, \"Drains_To\", \"\", \"MULTI_PART\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\msm_Catchment_Dissolve_Single', dissolve_path, 'Node_Catchment_' + node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# def get_memory_usage(var):\n",
    "#     \"\"\"Returns the memory usage of a variable in bytes.\"\"\"\n",
    "#     return sys.getsizeof(var)\n",
    "\n",
    "# def list_variables_memory():\n",
    "#     # Get the local and global variables\n",
    "#     variables = {**globals(), **locals()}\n",
    "#     total_memory = 0\n",
    "    \n",
    "#     # Print the variables and their memory consumption\n",
    "#     for var_name, var_value in variables.items():\n",
    "#         # Filter out the built-in variables and functions\n",
    "#         if not var_name.startswith('__') and not callable(var_value):\n",
    "#             memory = get_memory_usage(var_value)\n",
    "#             total_memory += memory\n",
    "#             print(f\"Variable: {var_name}, Type: {type(var_value)}, Memory: {memory} bytes\")\n",
    "\n",
    "#     total_memory_mb = total_memory / (1024 * 1024)  # Convert bytes to megabytes\n",
    "#     print(f\"Total Memory Usage: {total_memory_mb:.2f} MB\")\n",
    "#     return total_memory_mb\n",
    "\n",
    "\n",
    "# # Call the function to list variables and their memory consumption\n",
    "# total_memory_usage_mb = list_variables_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 11\n",
    "#Append individual dissolved catchments to one layer.\n",
    "\n",
    "if run_dissolve_append:\n",
    "    nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "    for i, node in enumerate(nodes):    \n",
    "        print('Appending for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "        if i == 0:\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\Node_Catchment_' + node, out_path, 'Node_Catchment')\n",
    "        else:\n",
    "            arcpy.management.Append(dissolve_path + '\\\\Node_Catchment_' + node, \"Node_Catchment\", \"NO_TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 1 of 469 at time 2024-05-27 09:26:01.093705\n",
      "Printing jpg 2 of 469 at time 2024-05-27 09:26:02.720209\n",
      "Printing jpg 3 of 469 at time 2024-05-27 09:26:04.314186\n",
      "Printing jpg 4 of 469 at time 2024-05-27 09:26:06.033776\n",
      "Printing jpg 5 of 469 at time 2024-05-27 09:26:07.877481\n",
      "Printing jpg 6 of 469 at time 2024-05-27 09:26:09.813775\n",
      "Printing jpg 7 of 469 at time 2024-05-27 09:26:11.690510\n",
      "Printing jpg 8 of 469 at time 2024-05-27 09:26:13.316014\n",
      "Printing jpg 9 of 469 at time 2024-05-27 09:26:15.095665\n",
      "Printing jpg 10 of 469 at time 2024-05-27 09:26:17.001428\n",
      "Printing jpg 11 of 469 at time 2024-05-27 09:26:18.690990\n",
      "Printing jpg 12 of 469 at time 2024-05-27 09:26:20.533694\n",
      "Printing jpg 13 of 469 at time 2024-05-27 09:26:22.346874\n",
      "Printing jpg 14 of 469 at time 2024-05-27 09:26:24.095491\n",
      "Printing jpg 15 of 469 at time 2024-05-27 09:26:25.878140\n",
      "Printing jpg 16 of 469 at time 2024-05-27 09:26:27.846464\n",
      "Printing jpg 17 of 469 at time 2024-05-27 09:26:29.598084\n",
      "Printing jpg 18 of 469 at time 2024-05-27 09:26:31.533874\n",
      "Printing jpg 19 of 469 at time 2024-05-27 09:26:33.036263\n",
      "Printing jpg 20 of 469 at time 2024-05-27 09:26:34.567182\n",
      "Printing jpg 21 of 469 at time 2024-05-27 09:26:36.345827\n",
      "Printing jpg 22 of 469 at time 2024-05-27 09:26:38.191534\n",
      "Printing jpg 23 of 469 at time 2024-05-27 09:26:39.785008\n",
      "Printing jpg 24 of 469 at time 2024-05-27 09:26:41.502099\n",
      "Printing jpg 25 of 469 at time 2024-05-27 09:26:43.315776\n",
      "Printing jpg 26 of 469 at time 2024-05-27 09:26:44.939278\n",
      "Printing jpg 27 of 469 at time 2024-05-27 09:26:46.568792\n",
      "Printing jpg 28 of 469 at time 2024-05-27 09:26:48.222321\n",
      "Printing jpg 29 of 469 at time 2024-05-27 09:26:49.817300\n",
      "Printing jpg 30 of 469 at time 2024-05-27 09:26:52.095910\n",
      "Printing jpg 31 of 469 at time 2024-05-27 09:26:54.221876\n",
      "Printing jpg 32 of 469 at time 2024-05-27 09:26:56.440928\n",
      "Printing jpg 33 of 469 at time 2024-05-27 09:26:58.564453\n",
      "Printing jpg 34 of 469 at time 2024-05-27 09:27:00.784506\n",
      "Printing jpg 35 of 469 at time 2024-05-27 09:27:02.534124\n",
      "Printing jpg 36 of 469 at time 2024-05-27 09:27:04.130103\n",
      "Printing jpg 37 of 469 at time 2024-05-27 09:27:05.753604\n",
      "Printing jpg 38 of 469 at time 2024-05-27 09:27:07.692397\n",
      "Printing jpg 39 of 469 at time 2024-05-27 09:27:09.536606\n",
      "Printing jpg 40 of 469 at time 2024-05-27 09:27:11.440366\n",
      "Printing jpg 41 of 469 at time 2024-05-27 09:27:13.440215\n",
      "Printing jpg 42 of 469 at time 2024-05-27 09:27:15.347979\n",
      "Printing jpg 43 of 469 at time 2024-05-27 09:27:17.098101\n",
      "Printing jpg 44 of 469 at time 2024-05-27 09:27:18.878746\n",
      "Printing jpg 45 of 469 at time 2024-05-27 09:27:20.473220\n",
      "Printing jpg 46 of 469 at time 2024-05-27 09:27:22.035670\n",
      "Printing jpg 47 of 469 at time 2024-05-27 09:27:23.628142\n",
      "Printing jpg 48 of 469 at time 2024-05-27 09:27:25.441819\n",
      "Printing jpg 49 of 469 at time 2024-05-27 09:27:27.127880\n",
      "Printing jpg 50 of 469 at time 2024-05-27 09:27:29.003117\n",
      "Printing jpg 51 of 469 at time 2024-05-27 09:27:30.877850\n",
      "Printing jpg 52 of 469 at time 2024-05-27 09:27:32.598440\n",
      "Printing jpg 53 of 469 at time 2024-05-27 09:27:34.347056\n",
      "Printing jpg 54 of 469 at time 2024-05-27 09:27:36.067149\n",
      "Printing jpg 55 of 469 at time 2024-05-27 09:27:37.753211\n",
      "Printing jpg 56 of 469 at time 2024-05-27 09:27:39.284627\n",
      "Printing jpg 57 of 469 at time 2024-05-27 09:27:40.720457\n",
      "Printing jpg 58 of 469 at time 2024-05-27 09:27:42.096729\n",
      "Printing jpg 59 of 469 at time 2024-05-27 09:27:43.596630\n",
      "Printing jpg 60 of 469 at time 2024-05-27 09:27:45.004932\n",
      "Printing jpg 61 of 469 at time 2024-05-27 09:27:46.440762\n",
      "Printing jpg 62 of 469 at time 2024-05-27 09:27:47.941149\n",
      "Printing jpg 63 of 469 at time 2024-05-27 09:27:49.319423\n",
      "Printing jpg 64 of 469 at time 2024-05-27 09:27:50.785778\n",
      "Printing jpg 65 of 469 at time 2024-05-27 09:27:52.288167\n",
      "Printing jpg 66 of 469 at time 2024-05-27 09:27:53.692968\n",
      "Printing jpg 67 of 469 at time 2024-05-27 09:27:55.193355\n",
      "Printing jpg 68 of 469 at time 2024-05-27 09:27:56.815855\n",
      "Printing jpg 69 of 469 at time 2024-05-27 09:27:58.253184\n",
      "Printing jpg 70 of 469 at time 2024-05-27 09:27:59.691016\n",
      "Printing jpg 71 of 469 at time 2024-05-27 09:28:01.098317\n",
      "Printing jpg 72 of 469 at time 2024-05-27 09:28:02.504623\n",
      "Printing jpg 73 of 469 at time 2024-05-27 09:28:03.911924\n",
      "Printing jpg 74 of 469 at time 2024-05-27 09:28:05.473870\n",
      "Printing jpg 75 of 469 at time 2024-05-27 09:28:06.941227\n",
      "Printing jpg 76 of 469 at time 2024-05-27 09:28:08.567238\n",
      "Printing jpg 77 of 469 at time 2024-05-27 09:28:10.066630\n",
      "Printing jpg 78 of 469 at time 2024-05-27 09:28:11.503461\n",
      "Printing jpg 79 of 469 at time 2024-05-27 09:28:13.067907\n",
      "Printing jpg 80 of 469 at time 2024-05-27 09:28:14.537265\n",
      "Printing jpg 81 of 469 at time 2024-05-27 09:28:16.036651\n",
      "Printing jpg 82 of 469 at time 2024-05-27 09:28:17.692685\n",
      "Printing jpg 83 of 469 at time 2024-05-27 09:28:19.161042\n",
      "Printing jpg 84 of 469 at time 2024-05-27 09:28:21.315033\n",
      "Printing jpg 85 of 469 at time 2024-05-27 09:28:22.911509\n",
      "Printing jpg 86 of 469 at time 2024-05-27 09:28:24.317312\n",
      "Printing jpg 87 of 469 at time 2024-05-27 09:28:25.755641\n",
      "Printing jpg 88 of 469 at time 2024-05-27 09:28:27.192970\n",
      "Printing jpg 89 of 469 at time 2024-05-27 09:28:28.815974\n",
      "Printing jpg 90 of 469 at time 2024-05-27 09:28:30.287838\n",
      "Printing jpg 91 of 469 at time 2024-05-27 09:28:31.849281\n",
      "Printing jpg 92 of 469 at time 2024-05-27 09:28:33.316637\n",
      "Printing jpg 93 of 469 at time 2024-05-27 09:28:34.971670\n",
      "Printing jpg 94 of 469 at time 2024-05-27 09:28:36.411047\n",
      "Printing jpg 95 of 469 at time 2024-05-27 09:28:37.847374\n",
      "Printing jpg 96 of 469 at time 2024-05-27 09:28:39.441848\n",
      "Printing jpg 97 of 469 at time 2024-05-27 09:28:41.036826\n",
      "Printing jpg 98 of 469 at time 2024-05-27 09:28:42.536715\n",
      "Printing jpg 99 of 469 at time 2024-05-27 09:28:44.162218\n",
      "Printing jpg 100 of 469 at time 2024-05-27 09:28:45.691632\n",
      "Printing jpg 101 of 469 at time 2024-05-27 09:28:47.286609\n",
      "Printing jpg 102 of 469 at time 2024-05-27 09:28:48.756968\n",
      "Printing jpg 103 of 469 at time 2024-05-27 09:28:50.724787\n",
      "Printing jpg 104 of 469 at time 2024-05-27 09:28:52.724636\n",
      "Printing jpg 105 of 469 at time 2024-05-27 09:28:54.410697\n",
      "Printing jpg 106 of 469 at time 2024-05-27 09:28:56.099258\n",
      "Printing jpg 107 of 469 at time 2024-05-27 09:28:57.754788\n",
      "Printing jpg 108 of 469 at time 2024-05-27 09:28:59.536939\n",
      "Printing jpg 109 of 469 at time 2024-05-27 09:29:01.350615\n",
      "Printing jpg 110 of 469 at time 2024-05-27 09:29:02.911057\n",
      "Printing jpg 111 of 469 at time 2024-05-27 09:29:04.660675\n",
      "Printing jpg 112 of 469 at time 2024-05-27 09:29:06.316708\n",
      "Printing jpg 113 of 469 at time 2024-05-27 09:29:07.913184\n",
      "Printing jpg 114 of 469 at time 2024-05-27 09:29:09.536187\n",
      "Printing jpg 115 of 469 at time 2024-05-27 09:29:11.160191\n",
      "Printing jpg 116 of 469 at time 2024-05-27 09:29:13.097982\n",
      "Printing jpg 117 of 469 at time 2024-05-27 09:29:14.757019\n",
      "Printing jpg 118 of 469 at time 2024-05-27 09:29:16.286433\n",
      "Printing jpg 119 of 469 at time 2024-05-27 09:29:17.975497\n",
      "Printing jpg 120 of 469 at time 2024-05-27 09:29:19.662056\n",
      "Printing jpg 121 of 469 at time 2024-05-27 09:29:21.566817\n",
      "Printing jpg 122 of 469 at time 2024-05-27 09:29:23.067204\n",
      "Printing jpg 123 of 469 at time 2024-05-27 09:29:24.693211\n",
      "Printing jpg 124 of 469 at time 2024-05-27 09:29:26.475858\n",
      "Printing jpg 125 of 469 at time 2024-05-27 09:29:28.036301\n",
      "Printing jpg 126 of 469 at time 2024-05-27 09:29:29.660802\n",
      "Printing jpg 127 of 469 at time 2024-05-27 09:29:31.285808\n",
      "Printing jpg 128 of 469 at time 2024-05-27 09:29:32.881283\n",
      "Printing jpg 129 of 469 at time 2024-05-27 09:29:34.599871\n",
      "Printing jpg 130 of 469 at time 2024-05-27 09:29:36.256906\n",
      "Printing jpg 131 of 469 at time 2024-05-27 09:29:37.941463\n",
      "Printing jpg 132 of 469 at time 2024-05-27 09:29:39.504908\n",
      "Printing jpg 133 of 469 at time 2024-05-27 09:29:41.473728\n",
      "Printing jpg 134 of 469 at time 2024-05-27 09:29:43.285906\n",
      "Printing jpg 135 of 469 at time 2024-05-27 09:29:45.317785\n",
      "Printing jpg 136 of 469 at time 2024-05-27 09:29:48.848551\n",
      "Printing jpg 137 of 469 at time 2024-05-27 09:29:50.755314\n",
      "Printing jpg 138 of 469 at time 2024-05-27 09:29:52.663077\n",
      "Printing jpg 139 of 469 at time 2024-05-27 09:29:54.974717\n",
      "Printing jpg 140 of 469 at time 2024-05-27 09:29:57.005594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 141 of 469 at time 2024-05-27 09:29:59.007444\n",
      "Printing jpg 142 of 469 at time 2024-05-27 09:30:00.975263\n",
      "Printing jpg 143 of 469 at time 2024-05-27 09:30:02.755414\n",
      "Printing jpg 144 of 469 at time 2024-05-27 09:30:04.599621\n",
      "Printing jpg 145 of 469 at time 2024-05-27 09:30:06.539414\n",
      "Printing jpg 146 of 469 at time 2024-05-27 09:30:08.317561\n",
      "Printing jpg 147 of 469 at time 2024-05-27 09:30:10.224323\n",
      "Printing jpg 148 of 469 at time 2024-05-27 09:30:12.130085\n",
      "Printing jpg 149 of 469 at time 2024-05-27 09:30:13.913236\n",
      "Printing jpg 150 of 469 at time 2024-05-27 09:30:15.692881\n",
      "Printing jpg 151 of 469 at time 2024-05-27 09:30:17.539588\n",
      "Printing jpg 152 of 469 at time 2024-05-27 09:30:19.412325\n",
      "Printing jpg 153 of 469 at time 2024-05-27 09:30:21.227003\n",
      "Printing jpg 154 of 469 at time 2024-05-27 09:30:23.194822\n",
      "Printing jpg 155 of 469 at time 2024-05-27 09:30:25.224698\n",
      "Printing jpg 156 of 469 at time 2024-05-27 09:30:27.166996\n",
      "Printing jpg 157 of 469 at time 2024-05-27 09:30:29.351519\n",
      "Printing jpg 158 of 469 at time 2024-05-27 09:30:31.320338\n",
      "Printing jpg 159 of 469 at time 2024-05-27 09:30:33.224602\n",
      "Printing jpg 160 of 469 at time 2024-05-27 09:30:35.102337\n",
      "Printing jpg 161 of 469 at time 2024-05-27 09:30:36.913011\n",
      "Printing jpg 162 of 469 at time 2024-05-27 09:30:38.727193\n",
      "Printing jpg 163 of 469 at time 2024-05-27 09:30:40.603430\n",
      "Printing jpg 164 of 469 at time 2024-05-27 09:30:42.599782\n",
      "Printing jpg 165 of 469 at time 2024-05-27 09:30:44.441988\n",
      "Printing jpg 166 of 469 at time 2024-05-27 09:30:46.255664\n",
      "Printing jpg 167 of 469 at time 2024-05-27 09:30:48.068340\n",
      "Printing jpg 168 of 469 at time 2024-05-27 09:30:49.913549\n",
      "Printing jpg 169 of 469 at time 2024-05-27 09:30:51.820311\n",
      "Printing jpg 170 of 469 at time 2024-05-27 09:30:53.664015\n",
      "Printing jpg 171 of 469 at time 2024-05-27 09:30:55.506719\n",
      "Printing jpg 172 of 469 at time 2024-05-27 09:30:57.350927\n",
      "Printing jpg 173 of 469 at time 2024-05-27 09:30:59.162601\n",
      "Printing jpg 174 of 469 at time 2024-05-27 09:31:01.162450\n",
      "Printing jpg 175 of 469 at time 2024-05-27 09:31:02.975630\n",
      "Printing jpg 176 of 469 at time 2024-05-27 09:31:04.789306\n",
      "Printing jpg 177 of 469 at time 2024-05-27 09:31:06.696069\n",
      "Printing jpg 178 of 469 at time 2024-05-27 09:31:08.507246\n",
      "Printing jpg 179 of 469 at time 2024-05-27 09:31:10.381979\n",
      "Printing jpg 180 of 469 at time 2024-05-27 09:31:12.319770\n",
      "Printing jpg 181 of 469 at time 2024-05-27 09:31:13.850185\n",
      "Printing jpg 182 of 469 at time 2024-05-27 09:31:15.475190\n",
      "Printing jpg 183 of 469 at time 2024-05-27 09:31:17.193779\n",
      "Printing jpg 184 of 469 at time 2024-05-27 09:31:18.849316\n",
      "Printing jpg 185 of 469 at time 2024-05-27 09:31:20.445791\n",
      "Printing jpg 186 of 469 at time 2024-05-27 09:31:22.101825\n",
      "Printing jpg 187 of 469 at time 2024-05-27 09:31:23.631239\n",
      "Printing jpg 188 of 469 at time 2024-05-27 09:31:25.256741\n",
      "Printing jpg 189 of 469 at time 2024-05-27 09:31:27.101949\n",
      "Printing jpg 190 of 469 at time 2024-05-27 09:31:28.850566\n",
      "Printing jpg 191 of 469 at time 2024-05-27 09:31:30.475067\n",
      "Printing jpg 192 of 469 at time 2024-05-27 09:31:32.132600\n",
      "Printing jpg 193 of 469 at time 2024-05-27 09:31:33.787632\n",
      "Printing jpg 194 of 469 at time 2024-05-27 09:31:35.413135\n",
      "Printing jpg 195 of 469 at time 2024-05-27 09:31:37.162752\n",
      "Printing jpg 196 of 469 at time 2024-05-27 09:31:38.850312\n",
      "Printing jpg 197 of 469 at time 2024-05-27 09:31:40.507347\n",
      "Printing jpg 198 of 469 at time 2024-05-27 09:31:42.164385\n",
      "Printing jpg 199 of 469 at time 2024-05-27 09:31:43.850944\n",
      "Printing jpg 200 of 469 at time 2024-05-27 09:31:45.633094\n",
      "Printing jpg 201 of 469 at time 2024-05-27 09:31:47.444769\n",
      "Printing jpg 202 of 469 at time 2024-05-27 09:31:49.257445\n",
      "Printing jpg 203 of 469 at time 2024-05-27 09:31:51.069119\n",
      "Printing jpg 204 of 469 at time 2024-05-27 09:31:52.852271\n",
      "Printing jpg 205 of 469 at time 2024-05-27 09:31:54.820090\n",
      "Printing jpg 206 of 469 at time 2024-05-27 09:31:56.599735\n",
      "Printing jpg 207 of 469 at time 2024-05-27 09:31:58.383887\n",
      "Printing jpg 208 of 469 at time 2024-05-27 09:32:00.257619\n",
      "Printing jpg 209 of 469 at time 2024-05-27 09:32:01.976207\n",
      "Printing jpg 210 of 469 at time 2024-05-27 09:32:03.725825\n",
      "Printing jpg 211 of 469 at time 2024-05-27 09:32:05.726177\n",
      "Printing jpg 212 of 469 at time 2024-05-27 09:32:07.569881\n",
      "Printing jpg 213 of 469 at time 2024-05-27 09:32:09.413585\n",
      "Printing jpg 214 of 469 at time 2024-05-27 09:32:11.288821\n",
      "Printing jpg 215 of 469 at time 2024-05-27 09:32:13.132526\n",
      "Printing jpg 216 of 469 at time 2024-05-27 09:32:14.883659\n",
      "Printing jpg 217 of 469 at time 2024-05-27 09:32:16.727865\n",
      "Printing jpg 218 of 469 at time 2024-05-27 09:32:18.538538\n",
      "Printing jpg 219 of 469 at time 2024-05-27 09:32:20.195069\n",
      "Printing jpg 220 of 469 at time 2024-05-27 09:32:22.038773\n",
      "Printing jpg 221 of 469 at time 2024-05-27 09:32:23.850950\n",
      "Printing jpg 222 of 469 at time 2024-05-27 09:32:25.538510\n",
      "Printing jpg 223 of 469 at time 2024-05-27 09:32:27.445272\n",
      "Printing jpg 224 of 469 at time 2024-05-27 09:32:29.196393\n",
      "Printing jpg 225 of 469 at time 2024-05-27 09:32:30.914981\n",
      "Printing jpg 226 of 469 at time 2024-05-27 09:32:32.601539\n",
      "Printing jpg 227 of 469 at time 2024-05-27 09:32:34.290100\n",
      "Printing jpg 228 of 469 at time 2024-05-27 09:32:36.132306\n",
      "Printing jpg 229 of 469 at time 2024-05-27 09:32:37.945981\n",
      "Printing jpg 230 of 469 at time 2024-05-27 09:32:39.632540\n",
      "Printing jpg 231 of 469 at time 2024-05-27 09:32:41.289071\n",
      "Printing jpg 232 of 469 at time 2024-05-27 09:32:42.945104\n",
      "Printing jpg 233 of 469 at time 2024-05-27 09:32:44.663692\n",
      "Printing jpg 234 of 469 at time 2024-05-27 09:32:46.413813\n",
      "Printing jpg 235 of 469 at time 2024-05-27 09:32:48.257522\n",
      "Printing jpg 236 of 469 at time 2024-05-27 09:32:50.008140\n",
      "Printing jpg 237 of 469 at time 2024-05-27 09:32:51.757756\n",
      "Printing jpg 238 of 469 at time 2024-05-27 09:32:53.538906\n",
      "Printing jpg 239 of 469 at time 2024-05-27 09:32:55.195436\n",
      "Printing jpg 240 of 469 at time 2024-05-27 09:32:56.882995\n",
      "Printing jpg 241 of 469 at time 2024-05-27 09:32:58.508497\n",
      "Printing jpg 242 of 469 at time 2024-05-27 09:33:00.101472\n",
      "Printing jpg 243 of 469 at time 2024-05-27 09:33:01.945176\n",
      "Printing jpg 244 of 469 at time 2024-05-27 09:33:03.632736\n",
      "Printing jpg 245 of 469 at time 2024-05-27 09:33:05.320295\n",
      "Printing jpg 246 of 469 at time 2024-05-27 09:33:07.133474\n",
      "Printing jpg 247 of 469 at time 2024-05-27 09:33:08.820556\n",
      "Printing jpg 248 of 469 at time 2024-05-27 09:33:10.541146\n",
      "Printing jpg 249 of 469 at time 2024-05-27 09:33:12.228711\n",
      "Printing jpg 250 of 469 at time 2024-05-27 09:33:13.853213\n",
      "Printing jpg 251 of 469 at time 2024-05-27 09:33:15.446685\n",
      "Printing jpg 252 of 469 at time 2024-05-27 09:33:17.228331\n",
      "Printing jpg 253 of 469 at time 2024-05-27 09:33:18.853336\n",
      "Printing jpg 254 of 469 at time 2024-05-27 09:33:20.540896\n",
      "Printing jpg 255 of 469 at time 2024-05-27 09:33:22.946623\n",
      "Printing jpg 256 of 469 at time 2024-05-27 09:33:24.603657\n",
      "Printing jpg 257 of 469 at time 2024-05-27 09:33:26.444358\n",
      "Printing jpg 258 of 469 at time 2024-05-27 09:33:28.227005\n",
      "Printing jpg 259 of 469 at time 2024-05-27 09:33:29.946096\n",
      "Printing jpg 260 of 469 at time 2024-05-27 09:33:31.758772\n",
      "Printing jpg 261 of 469 at time 2024-05-27 09:33:33.726094\n",
      "Printing jpg 262 of 469 at time 2024-05-27 09:33:35.416159\n",
      "Printing jpg 263 of 469 at time 2024-05-27 09:33:37.514098\n",
      "Printing jpg 264 of 469 at time 2024-05-27 09:33:39.696114\n",
      "Printing jpg 265 of 469 at time 2024-05-27 09:33:41.790553\n",
      "Printing jpg 266 of 469 at time 2024-05-27 09:33:43.884488\n",
      "Printing jpg 267 of 469 at time 2024-05-27 09:33:46.135568\n",
      "Printing jpg 268 of 469 at time 2024-05-27 09:33:47.885185\n",
      "Printing jpg 269 of 469 at time 2024-05-27 09:33:49.634809\n",
      "Printing jpg 270 of 469 at time 2024-05-27 09:33:51.478512\n",
      "Printing jpg 271 of 469 at time 2024-05-27 09:33:53.321215\n",
      "Printing jpg 272 of 469 at time 2024-05-27 09:33:54.948221\n",
      "Printing jpg 273 of 469 at time 2024-05-27 09:33:56.758895\n",
      "Printing jpg 274 of 469 at time 2024-05-27 09:33:58.635629\n",
      "Printing jpg 275 of 469 at time 2024-05-27 09:34:00.603951\n",
      "Printing jpg 276 of 469 at time 2024-05-27 09:34:02.477185\n",
      "Printing jpg 277 of 469 at time 2024-05-27 09:34:04.603150\n",
      "Printing jpg 278 of 469 at time 2024-05-27 09:34:06.449359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 279 of 469 at time 2024-05-27 09:34:08.479235\n",
      "Printing jpg 280 of 469 at time 2024-05-27 09:34:10.447053\n",
      "Printing jpg 281 of 469 at time 2024-05-27 09:34:12.259230\n",
      "Printing jpg 282 of 469 at time 2024-05-27 09:34:14.292109\n",
      "Printing jpg 283 of 469 at time 2024-05-27 09:34:15.760466\n",
      "Printing jpg 284 of 469 at time 2024-05-27 09:34:17.259354\n",
      "Printing jpg 285 of 469 at time 2024-05-27 09:34:18.853330\n",
      "Printing jpg 286 of 469 at time 2024-05-27 09:34:20.509861\n",
      "Printing jpg 287 of 469 at time 2024-05-27 09:34:22.167393\n",
      "Printing jpg 288 of 469 at time 2024-05-27 09:34:23.916008\n",
      "Printing jpg 289 of 469 at time 2024-05-27 09:34:25.603073\n",
      "Printing jpg 290 of 469 at time 2024-05-27 09:34:27.259604\n",
      "Printing jpg 291 of 469 at time 2024-05-27 09:34:29.011223\n",
      "Printing jpg 292 of 469 at time 2024-05-27 09:34:30.696780\n",
      "Printing jpg 293 of 469 at time 2024-05-27 09:34:32.292758\n",
      "Printing jpg 294 of 469 at time 2024-05-27 09:34:33.884229\n",
      "Printing jpg 295 of 469 at time 2024-05-27 09:34:35.479703\n",
      "Printing jpg 296 of 469 at time 2024-05-27 09:34:37.104707\n",
      "Printing jpg 297 of 469 at time 2024-05-27 09:34:38.728208\n",
      "Printing jpg 298 of 469 at time 2024-05-27 09:34:40.354711\n",
      "Printing jpg 299 of 469 at time 2024-05-27 09:34:41.978211\n",
      "Printing jpg 300 of 469 at time 2024-05-27 09:34:43.572187\n",
      "Printing jpg 301 of 469 at time 2024-05-27 09:34:45.228717\n",
      "Printing jpg 302 of 469 at time 2024-05-27 09:34:46.853219\n",
      "Printing jpg 303 of 469 at time 2024-05-27 09:34:48.417664\n",
      "Printing jpg 304 of 469 at time 2024-05-27 09:34:50.103726\n",
      "Printing jpg 305 of 469 at time 2024-05-27 09:34:51.697199\n",
      "Printing jpg 306 of 469 at time 2024-05-27 09:34:53.290671\n",
      "Printing jpg 307 of 469 at time 2024-05-27 09:34:54.916173\n",
      "Printing jpg 308 of 469 at time 2024-05-27 09:34:56.603235\n",
      "Printing jpg 309 of 469 at time 2024-05-27 09:34:58.228737\n",
      "Printing jpg 310 of 469 at time 2024-05-27 09:35:00.010383\n",
      "Printing jpg 311 of 469 at time 2024-05-27 09:35:01.728971\n",
      "Printing jpg 312 of 469 at time 2024-05-27 09:35:03.385005\n",
      "Printing jpg 313 of 469 at time 2024-05-27 09:35:04.980479\n",
      "Printing jpg 314 of 469 at time 2024-05-27 09:35:06.760124\n",
      "Printing jpg 315 of 469 at time 2024-05-27 09:35:08.352595\n",
      "Printing jpg 316 of 469 at time 2024-05-27 09:35:09.948574\n",
      "Printing jpg 317 of 469 at time 2024-05-27 09:35:11.636133\n",
      "Printing jpg 318 of 469 at time 2024-05-27 09:35:13.322691\n",
      "Printing jpg 319 of 469 at time 2024-05-27 09:35:14.947696\n",
      "Printing jpg 320 of 469 at time 2024-05-27 09:35:16.450085\n",
      "Printing jpg 321 of 469 at time 2024-05-27 09:35:17.977496\n",
      "Printing jpg 322 of 469 at time 2024-05-27 09:35:19.478883\n",
      "Printing jpg 323 of 469 at time 2024-05-27 09:35:20.980271\n",
      "Printing jpg 324 of 469 at time 2024-05-27 09:35:22.635303\n",
      "Printing jpg 325 of 469 at time 2024-05-27 09:35:24.197747\n",
      "Printing jpg 326 of 469 at time 2024-05-27 09:35:25.916335\n",
      "Printing jpg 327 of 469 at time 2024-05-27 09:35:27.573369\n",
      "Printing jpg 328 of 469 at time 2024-05-27 09:35:29.197871\n",
      "Printing jpg 329 of 469 at time 2024-05-27 09:35:30.947487\n",
      "Printing jpg 330 of 469 at time 2024-05-27 09:35:32.541961\n",
      "Printing jpg 331 of 469 at time 2024-05-27 09:35:34.198995\n",
      "Printing jpg 332 of 469 at time 2024-05-27 09:35:36.010669\n",
      "Printing jpg 333 of 469 at time 2024-05-27 09:35:37.605142\n",
      "Printing jpg 334 of 469 at time 2024-05-27 09:35:39.197614\n",
      "Printing jpg 335 of 469 at time 2024-05-27 09:35:40.949736\n",
      "Printing jpg 336 of 469 at time 2024-05-27 09:35:42.543208\n",
      "Printing jpg 337 of 469 at time 2024-05-27 09:35:44.135680\n",
      "Printing jpg 338 of 469 at time 2024-05-27 09:35:45.885297\n",
      "Printing jpg 339 of 469 at time 2024-05-27 09:35:47.448244\n",
      "Printing jpg 340 of 469 at time 2024-05-27 09:35:49.104781\n",
      "Printing jpg 341 of 469 at time 2024-05-27 09:35:50.729282\n",
      "Printing jpg 342 of 469 at time 2024-05-27 09:35:52.354286\n",
      "Printing jpg 343 of 469 at time 2024-05-27 09:35:53.979788\n",
      "Printing jpg 344 of 469 at time 2024-05-27 09:35:55.604290\n",
      "Printing jpg 345 of 469 at time 2024-05-27 09:35:57.293851\n",
      "Printing jpg 346 of 469 at time 2024-05-27 09:35:58.916854\n",
      "Printing jpg 347 of 469 at time 2024-05-27 09:36:00.481300\n",
      "Printing jpg 348 of 469 at time 2024-05-27 09:36:02.104800\n",
      "Printing jpg 349 of 469 at time 2024-05-27 09:36:03.729301\n",
      "Printing jpg 350 of 469 at time 2024-05-27 09:36:05.386840\n",
      "Printing jpg 351 of 469 at time 2024-05-27 09:36:06.982314\n",
      "Printing jpg 352 of 469 at time 2024-05-27 09:36:08.573785\n",
      "Printing jpg 353 of 469 at time 2024-05-27 09:36:10.386963\n",
      "Printing jpg 354 of 469 at time 2024-05-27 09:36:11.979435\n",
      "Printing jpg 355 of 469 at time 2024-05-27 09:36:13.606939\n",
      "Printing jpg 356 of 469 at time 2024-05-27 09:36:15.358557\n",
      "Printing jpg 357 of 469 at time 2024-05-27 09:36:16.948530\n",
      "Printing jpg 358 of 469 at time 2024-05-27 09:36:18.605060\n",
      "Printing jpg 359 of 469 at time 2024-05-27 09:36:20.354677\n",
      "Printing jpg 360 of 469 at time 2024-05-27 09:36:21.949151\n",
      "Printing jpg 361 of 469 at time 2024-05-27 09:36:23.575156\n",
      "Printing jpg 362 of 469 at time 2024-05-27 09:36:25.167627\n",
      "Printing jpg 363 of 469 at time 2024-05-27 09:36:26.792129\n",
      "Printing jpg 364 of 469 at time 2024-05-27 09:36:28.387603\n",
      "Printing jpg 365 of 469 at time 2024-05-27 09:36:30.197778\n",
      "Printing jpg 366 of 469 at time 2024-05-27 09:36:31.730194\n",
      "Printing jpg 367 of 469 at time 2024-05-27 09:36:33.323667\n",
      "Printing jpg 368 of 469 at time 2024-05-27 09:36:35.044257\n",
      "Printing jpg 369 of 469 at time 2024-05-27 09:36:36.792375\n",
      "Printing jpg 370 of 469 at time 2024-05-27 09:36:38.354819\n",
      "Printing jpg 371 of 469 at time 2024-05-27 09:36:40.072406\n",
      "Printing jpg 372 of 469 at time 2024-05-27 09:36:41.731946\n",
      "Printing jpg 373 of 469 at time 2024-05-27 09:36:43.355446\n",
      "Printing jpg 374 of 469 at time 2024-05-27 09:36:44.981949\n",
      "Printing jpg 375 of 469 at time 2024-05-27 09:36:46.544393\n",
      "Printing jpg 376 of 469 at time 2024-05-27 09:36:48.325045\n",
      "Printing jpg 377 of 469 at time 2024-05-27 09:36:50.199777\n",
      "Printing jpg 378 of 469 at time 2024-05-27 09:36:52.105538\n",
      "Printing jpg 379 of 469 at time 2024-05-27 09:36:53.792600\n",
      "Printing jpg 380 of 469 at time 2024-05-27 09:36:55.574246\n",
      "Printing jpg 381 of 469 at time 2024-05-27 09:36:57.387922\n",
      "Printing jpg 382 of 469 at time 2024-05-27 09:36:59.075482\n",
      "Printing jpg 383 of 469 at time 2024-05-27 09:37:00.824601\n",
      "Printing jpg 384 of 469 at time 2024-05-27 09:37:02.574218\n",
      "Printing jpg 385 of 469 at time 2024-05-27 09:37:04.322834\n",
      "Printing jpg 386 of 469 at time 2024-05-27 09:37:05.979868\n",
      "Printing jpg 387 of 469 at time 2024-05-27 09:37:07.793544\n",
      "Printing jpg 388 of 469 at time 2024-05-27 09:37:09.481103\n",
      "Printing jpg 389 of 469 at time 2024-05-27 09:37:11.199691\n",
      "Printing jpg 390 of 469 at time 2024-05-27 09:37:12.980840\n",
      "Printing jpg 391 of 469 at time 2024-05-27 09:37:14.855572\n",
      "Printing jpg 392 of 469 at time 2024-05-27 09:37:16.576161\n",
      "Printing jpg 393 of 469 at time 2024-05-27 09:37:18.261718\n",
      "Printing jpg 394 of 469 at time 2024-05-27 09:37:19.980809\n",
      "Printing jpg 395 of 469 at time 2024-05-27 09:37:21.638340\n",
      "Printing jpg 396 of 469 at time 2024-05-27 09:37:23.512078\n",
      "Printing jpg 397 of 469 at time 2024-05-27 09:37:25.199642\n",
      "Printing jpg 398 of 469 at time 2024-05-27 09:37:26.949258\n",
      "Printing jpg 399 of 469 at time 2024-05-27 09:37:28.763935\n",
      "Printing jpg 400 of 469 at time 2024-05-27 09:37:30.513551\n",
      "Printing jpg 401 of 469 at time 2024-05-27 09:37:32.262670\n",
      "Printing jpg 402 of 469 at time 2024-05-27 09:37:34.076345\n",
      "Printing jpg 403 of 469 at time 2024-05-27 09:37:35.919047\n",
      "Printing jpg 404 of 469 at time 2024-05-27 09:37:37.731224\n",
      "Printing jpg 405 of 469 at time 2024-05-27 09:37:39.638986\n",
      "Printing jpg 406 of 469 at time 2024-05-27 09:37:41.512717\n",
      "Printing jpg 407 of 469 at time 2024-05-27 09:37:43.293368\n",
      "Printing jpg 408 of 469 at time 2024-05-27 09:37:45.264189\n",
      "Printing jpg 409 of 469 at time 2024-05-27 09:37:47.107891\n",
      "Printing jpg 410 of 469 at time 2024-05-27 09:37:49.012651\n",
      "Printing jpg 411 of 469 at time 2024-05-27 09:37:50.979971\n",
      "Printing jpg 412 of 469 at time 2024-05-27 09:37:52.824675\n",
      "Printing jpg 413 of 469 at time 2024-05-27 09:37:54.730435\n",
      "Printing jpg 414 of 469 at time 2024-05-27 09:37:56.325412\n",
      "Printing jpg 415 of 469 at time 2024-05-27 09:37:57.918884\n",
      "Printing jpg 416 of 469 at time 2024-05-27 09:37:59.512355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 417 of 469 at time 2024-05-27 09:38:01.294001\n",
      "Printing jpg 418 of 469 at time 2024-05-27 09:38:02.951034\n",
      "Printing jpg 419 of 469 at time 2024-05-27 09:38:04.575535\n",
      "Printing jpg 420 of 469 at time 2024-05-27 09:38:06.199034\n",
      "Printing jpg 421 of 469 at time 2024-05-27 09:38:07.795509\n",
      "Printing jpg 422 of 469 at time 2024-05-27 09:38:09.450553\n",
      "Printing jpg 423 of 469 at time 2024-05-27 09:38:11.200169\n",
      "Printing jpg 424 of 469 at time 2024-05-27 09:38:12.920758\n",
      "Printing jpg 425 of 469 at time 2024-05-27 09:38:14.670878\n",
      "Printing jpg 426 of 469 at time 2024-05-27 09:38:16.231319\n",
      "Printing jpg 427 of 469 at time 2024-05-27 09:38:17.795764\n",
      "Printing jpg 428 of 469 at time 2024-05-27 09:38:19.981295\n",
      "Printing jpg 429 of 469 at time 2024-05-27 09:38:21.888057\n",
      "Printing jpg 430 of 469 at time 2024-05-27 09:38:23.545091\n",
      "Printing jpg 431 of 469 at time 2024-05-27 09:38:25.106533\n",
      "Printing jpg 432 of 469 at time 2024-05-27 09:38:26.637948\n",
      "Printing jpg 433 of 469 at time 2024-05-27 09:38:28.170363\n",
      "Printing jpg 434 of 469 at time 2024-05-27 09:38:29.857424\n",
      "Printing jpg 435 of 469 at time 2024-05-27 09:38:31.419868\n",
      "Printing jpg 436 of 469 at time 2024-05-27 09:38:32.952283\n",
      "Printing jpg 437 of 469 at time 2024-05-27 09:38:34.481696\n",
      "Printing jpg 438 of 469 at time 2024-05-27 09:38:35.949555\n",
      "Printing jpg 439 of 469 at time 2024-05-27 09:38:37.515001\n",
      "Printing jpg 440 of 469 at time 2024-05-27 09:38:39.169529\n",
      "Printing jpg 441 of 469 at time 2024-05-27 09:38:40.765002\n",
      "Printing jpg 442 of 469 at time 2024-05-27 09:38:42.355975\n",
      "Printing jpg 443 of 469 at time 2024-05-27 09:38:43.982477\n",
      "Printing jpg 444 of 469 at time 2024-05-27 09:38:45.606978\n",
      "Printing jpg 445 of 469 at time 2024-05-27 09:38:47.325565\n",
      "Printing jpg 446 of 469 at time 2024-05-27 09:38:49.045657\n",
      "Printing jpg 447 of 469 at time 2024-05-27 09:38:50.763243\n",
      "Printing jpg 448 of 469 at time 2024-05-27 09:38:52.482831\n",
      "Printing jpg 449 of 469 at time 2024-05-27 09:38:54.107332\n",
      "Printing jpg 450 of 469 at time 2024-05-27 09:38:55.891483\n",
      "Printing jpg 451 of 469 at time 2024-05-27 09:38:57.671127\n",
      "Printing jpg 452 of 469 at time 2024-05-27 09:38:59.327162\n",
      "Printing jpg 453 of 469 at time 2024-05-27 09:39:00.921138\n",
      "Printing jpg 454 of 469 at time 2024-05-27 09:39:02.545141\n",
      "Printing jpg 455 of 469 at time 2024-05-27 09:39:04.169642\n",
      "Printing jpg 456 of 469 at time 2024-05-27 09:39:05.764115\n",
      "Printing jpg 457 of 469 at time 2024-05-27 09:39:07.326060\n",
      "Printing jpg 458 of 469 at time 2024-05-27 09:39:09.013628\n",
      "Printing jpg 459 of 469 at time 2024-05-27 09:39:10.857841\n",
      "Printing jpg 460 of 469 at time 2024-05-27 09:39:12.575936\n",
      "Printing jpg 461 of 469 at time 2024-05-27 09:39:14.420640\n",
      "Printing jpg 462 of 469 at time 2024-05-27 09:39:16.201284\n",
      "Printing jpg 463 of 469 at time 2024-05-27 09:39:18.044987\n",
      "Printing jpg 464 of 469 at time 2024-05-27 09:39:19.858165\n",
      "Printing jpg 465 of 469 at time 2024-05-27 09:39:21.609783\n",
      "Printing jpg 466 of 469 at time 2024-05-27 09:39:23.296341\n",
      "Printing jpg 467 of 469 at time 2024-05-27 09:39:24.890316\n",
      "Printing jpg 468 of 469 at time 2024-05-27 09:39:26.702991\n",
      "Printing jpg 469 of 469 at time 2024-05-27 09:39:28.733866\n",
      "the following pages failed: []\n",
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "#Export jpgs\n",
    "if run_jpg:\n",
    "    aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "    project_path = aprx.filePath\n",
    "\n",
    "    jpg_folder = output_folder + r'\\HTML\\Maps_And_CSS'\n",
    "    if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "\n",
    "    # project_directory = os.path.dirname(project_path)\n",
    "\n",
    "    layouts = aprx.listLayouts()\n",
    "    export_fails = []\n",
    "\n",
    "    for layout in layouts:\n",
    "\n",
    "        if layout.mapSeries is not None:\n",
    "            map_series = layout.mapSeries\n",
    "            # Loop through all pages in the map series\n",
    "            for page_number in range(1, map_series.pageCount + 1):\n",
    "                map_series.currentPageNumber = page_number\n",
    "                output_filename = os.path.join(jpg_folder, f\"{map_series.pageRow.Drains_To}.jpg\")\n",
    "                try:\n",
    "                    layout.exportToJPEG(output_filename, resolution=300)\n",
    "                except:\n",
    "                    print(f'WARNING! {map_series.pageRow.Drains_To} could not be made')\n",
    "                    export_fails.append(map_series.pageRow.Drains_To)\n",
    "                print (f'Printing jpg {page_number} of {map_series.pageCount} at time {datetime.datetime.now()}')\n",
    "\n",
    "    print(f'the following pages failed: {export_fails}')            \n",
    "    print(\"Export complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "#Create HTMLs\n",
    "\n",
    "if run_html:\n",
    "    shutil.copy2('style.css', html_folder + '\\\\style.css')\n",
    "    shutil.copy2('script.js', html_folder + '\\\\script.js')\n",
    "\n",
    "    for category in categories:\n",
    "        area_type = category[0]\n",
    "        area_names = category[1]\n",
    "        header_start = category[2]\n",
    "\n",
    "        f = open(html_folder + '\\\\Population_By_' + area_type + '_' + model_area + '.html', \"w\")\n",
    "        f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "        f.write('<script src=\"script.js\"></script>\\n')\n",
    "        f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "        f.write('<!DOCTYPE html>\\n')\n",
    "        f.write('<html>\\n')\n",
    "        f.write('<head>\\n')\n",
    "        f.write('<meta charset=\"utf-8\">\\n')\n",
    "        f.write('</head>\\n')\n",
    "        f.write('<body>\\n\\n')\n",
    "\n",
    "        f.write('<div class=\"tab\">\\n')\n",
    "        for area_name in area_names:\n",
    "            tab = area_name\n",
    "\n",
    "        #     color = ps_dict[first_year]\n",
    "        #     bg_color = color_dict[color][0]\n",
    "        #     text_color = color_dict[color][1]\n",
    "\n",
    "            f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "        f.write('</div>\\n')\n",
    "\n",
    "        pop_df = pop_dfss[0][2]\n",
    "\n",
    "        for area_name in area_names:\n",
    "\n",
    "            area_df = pop_df[pop_df[area_type]==area_name]\n",
    "            area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "\n",
    "            f.write('<div id=\"' + area_name + '\" class=\"tabcontent\">\\n') \n",
    "            f.write('<h1>' + area_name + '</h1>\\n')\n",
    "\n",
    "            f.write('<div class=\"sidenav\">\\n')\n",
    "\n",
    "            f.write('<table style=\\'width: 90%;\\'>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th>Year</th>\\n')\n",
    "            f.write('<th>Population</th>\\n')\n",
    "            f.write('</tr>\\n')\n",
    "\n",
    "            for index, row in area_df.iterrows():\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ str(index) + '</td>\\n')\n",
    "                population_with_separator = f\"{int(row['Population']):,}\"\n",
    "                f.write('<td>'+ population_with_separator + '</td>\\n')\n",
    "\n",
    "                f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "\n",
    "            for i in range(4):\n",
    "                f.write('<h1 style=\"color: white\">End of tables</h1>\\n')#Invisible, just to enable scroll to table bottoms\n",
    "\n",
    "            f.write('</div>\\n') #end sidenav\n",
    "\n",
    "\n",
    "            f.write('<div class=\"main\">\\n')\n",
    "\n",
    "            fig = go.Figure()\n",
    "\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                         y = area_df.Population, \n",
    "                                         mode='lines',name=pop_dfss[0][0],line=dict(width=5)))\n",
    "\n",
    "            for pop_dfs in pop_dfss[1:]:\n",
    "                pop_df_past = pop_dfs[2]\n",
    "                area_df = pop_df_past[pop_df_past[area_type]==area_name]\n",
    "                area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "                fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                         y = area_df.Population, \n",
    "                                         mode='lines',name=pop_dfs[0],line=dict(width=2)))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=header_start + area_name,\n",
    "                autosize=False,\n",
    "                width = 1500,\n",
    "                height=850,\n",
    "                margin=dict(\n",
    "                    l=50,\n",
    "                    r=50,\n",
    "                    b=50,\n",
    "                    t=50,\n",
    "                    pad=4\n",
    "                    ),\n",
    "                    yaxis_title = 'Population'\n",
    "                )\n",
    "\n",
    "            f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "            f.write('</div>\\n') #end div main  \n",
    "\n",
    "            f.write('</div>\\n')  #end div tab   \n",
    "\n",
    "            f.write('</body>\\n')\n",
    "        f.write('</html>\\n')\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample DataFrame\n",
    "# data = {\n",
    "#     'Catchment': [1, 2, 3],\n",
    "#     'Node': [4, 5, 6]\n",
    "\n",
    "# }\n",
    "# accumulation_df = pd.DataFrame(data)\n",
    "\n",
    "# # Create a MultiIndex with the existing columns\n",
    "# existing_columns_multiindex = pd.MultiIndex.from_tuples([\n",
    "#     ('GENERAL INFO', 'Catchment'),  # Header with no subheaders\n",
    "#     ('GENERAL INFO', 'Node'),  # Header with no subheaders\n",
    "# ])\n",
    "\n",
    "# # Create a MultiIndex with the upper level 'GENERAL INFO'\n",
    "# upper_level = [('GENERAL INFO', '')] * len(existing_columns_multiindex)\n",
    "\n",
    "# # Concatenate the upper level and the existing columns MultiIndex\n",
    "# new_columns_multiindex = pd.MultiIndex.from_tuples(list(zip(upper_level, existing_columns_multiindex)))\n",
    "\n",
    "# # Assign the new MultiIndex to the DataFrame columns\n",
    "# accumulation_df.columns = new_columns_multiindex\n",
    "\n",
    "# accumulation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<string>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;34mC:\\Users\\hloecke\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-mike\\Lib\\ast.py\u001b[0m, in \u001b[0;32mparse\u001b[0m:\nLine \u001b[0;34m50\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m \u001b[36mcompile\u001b[39;49;00m(source, filename, mode, flags,\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mSyntaxError\u001b[0m: EOL while scanning string literal (<string>, line 8)\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "accumulation_df\n",
    "\n",
    "''''''''''\n",
    "rank manholes as per how many catchments they have\n",
    "first 'merge' those ith just 1\n",
    "then merge those with just 2. Register which ones they replace. Do not repeat. If the merge happened already, copy instead.\n",
    "then merge those with just 3. For each, check if they are in previous merges. \n",
    "''''''''''\n",
    "\n",
    "accumulation_df['Node']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">GENERAL INFO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CATCHMENT</th>\n",
       "      <th>NODE</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2047</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2054</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14088</th>\n",
       "      <td>2035</td>\n",
       "      <td>7140</td>\n",
       "      <td>GNK_MH41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14089</th>\n",
       "      <td>2033</td>\n",
       "      <td>7140</td>\n",
       "      <td>GNK_MH41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14090</th>\n",
       "      <td>2122</td>\n",
       "      <td>6790</td>\n",
       "      <td>SEBD_MH6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14091</th>\n",
       "      <td>2052</td>\n",
       "      <td>9670</td>\n",
       "      <td>HLY_MH61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14092</th>\n",
       "      <td>2169</td>\n",
       "      <td>7112</td>\n",
       "      <td>NVD_MH40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14093 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GENERAL INFO                 \n",
       "         CATCHMENT  NODE         ID\n",
       "0             2016  7494  HLR_MH10A\n",
       "1             2011  7494  HLR_MH10A\n",
       "2             2020  7494  HLR_MH10A\n",
       "3             2047  7494  HLR_MH10A\n",
       "4             2054  7494  HLR_MH10A\n",
       "...            ...   ...        ...\n",
       "14088         2035  7140   GNK_MH41\n",
       "14089         2033  7140   GNK_MH41\n",
       "14090         2122  6790   SEBD_MH6\n",
       "14091         2052  9670   HLY_MH61\n",
       "14092         2169  7112   NVD_MH40\n",
       "\n",
       "[14093 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[2,3] in [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sub_list = [2, 3]\n",
    "main_list = [2, 3, 4]\n",
    "\n",
    "# Check if all elements in sub_list are present in main_list\n",
    "result = all(item in main_list for item in sub_list)\n",
    "\n",
    "print(result)  # This will print True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "In  \u001b[0;34m[63]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     sort([\u001b[34m2\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sort' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "merge_set = set()\n",
    "for node in accumulation_df[('GENERAL INFO','NODE')].unique():\n",
    "    catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node][('GENERAL INFO','CATCHMENT')].unique())\n",
    "    catchments = tuple(sorted(catchments))\n",
    "    merge_set.add(catchments)\n",
    "print(len(merge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accumulation_df[('GENERAL INFO','NODE')].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(['a','c','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">GENERAL INFO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CATCHMENT</th>\n",
       "      <th>NODE</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2047</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2054</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14088</th>\n",
       "      <td>2035</td>\n",
       "      <td>7140</td>\n",
       "      <td>GNK_MH41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14089</th>\n",
       "      <td>2033</td>\n",
       "      <td>7140</td>\n",
       "      <td>GNK_MH41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14090</th>\n",
       "      <td>2122</td>\n",
       "      <td>6790</td>\n",
       "      <td>SEBD_MH6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14091</th>\n",
       "      <td>2052</td>\n",
       "      <td>9670</td>\n",
       "      <td>HLY_MH61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14092</th>\n",
       "      <td>2169</td>\n",
       "      <td>7112</td>\n",
       "      <td>NVD_MH40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14093 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GENERAL INFO                 \n",
       "         CATCHMENT  NODE         ID\n",
       "0             2016  7494  HLR_MH10A\n",
       "1             2011  7494  HLR_MH10A\n",
       "2             2020  7494  HLR_MH10A\n",
       "3             2047  7494  HLR_MH10A\n",
       "4             2054  7494  HLR_MH10A\n",
       "...            ...   ...        ...\n",
       "14088         2035  7140   GNK_MH41\n",
       "14089         2033  7140   GNK_MH41\n",
       "14090         2122  6790   SEBD_MH6\n",
       "14091         2052  9670   HLY_MH61\n",
       "14092         2169  7112   NVD_MH40\n",
       "\n",
       "[14093 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">GENERAL INFO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>CATCHMENT</th>\n",
       "      <th>NODE</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2047</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2054</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2053</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2039</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2044</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2032</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2028</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2005</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2031</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2034</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2241</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2049</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10198</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2006</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2045</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2001</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2042</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2035</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2050</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2038</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2024</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2027</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2033</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2013</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2007</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2037</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2009</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2003</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2015</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2030</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2017</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2026</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2048</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2240</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2025</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2055</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2056</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2052</td>\n",
       "      <td>7494</td>\n",
       "      <td>HLR_MH10A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENERAL INFO                 \n",
       "      CATCHMENT  NODE         ID\n",
       "0          2016  7494  HLR_MH10A\n",
       "1          2011  7494  HLR_MH10A\n",
       "2          2020  7494  HLR_MH10A\n",
       "3          2047  7494  HLR_MH10A\n",
       "4          2054  7494  HLR_MH10A\n",
       "5          2053  7494  HLR_MH10A\n",
       "6          2023  7494  HLR_MH10A\n",
       "7          2039  7494  HLR_MH10A\n",
       "8          2044  7494  HLR_MH10A\n",
       "9          2032  7494  HLR_MH10A\n",
       "10         2018  7494  HLR_MH10A\n",
       "11         2028  7494  HLR_MH10A\n",
       "12         2010  7494  HLR_MH10A\n",
       "13         2005  7494  HLR_MH10A\n",
       "14         2031  7494  HLR_MH10A\n",
       "15         2034  7494  HLR_MH10A\n",
       "16         2241  7494  HLR_MH10A\n",
       "17         2014  7494  HLR_MH10A\n",
       "18         2049  7494  HLR_MH10A\n",
       "19        10198  7494  HLR_MH10A\n",
       "20         2019  7494  HLR_MH10A\n",
       "21         2006  7494  HLR_MH10A\n",
       "22         2045  7494  HLR_MH10A\n",
       "23         2012  7494  HLR_MH10A\n",
       "24         2001  7494  HLR_MH10A\n",
       "25         2042  7494  HLR_MH10A\n",
       "26         2035  7494  HLR_MH10A\n",
       "27         2050  7494  HLR_MH10A\n",
       "28         2038  7494  HLR_MH10A\n",
       "29         2024  7494  HLR_MH10A\n",
       "30         2027  7494  HLR_MH10A\n",
       "31         2033  7494  HLR_MH10A\n",
       "32         2013  7494  HLR_MH10A\n",
       "33         2007  7494  HLR_MH10A\n",
       "34         2037  7494  HLR_MH10A\n",
       "35         2004  7494  HLR_MH10A\n",
       "36         2009  7494  HLR_MH10A\n",
       "37         2021  7494  HLR_MH10A\n",
       "38         2003  7494  HLR_MH10A\n",
       "39         2015  7494  HLR_MH10A\n",
       "40         2030  7494  HLR_MH10A\n",
       "41         2017  7494  HLR_MH10A\n",
       "42         2022  7494  HLR_MH10A\n",
       "43         2026  7494  HLR_MH10A\n",
       "44         2048  7494  HLR_MH10A\n",
       "45         2240  7494  HLR_MH10A\n",
       "46         2025  7494  HLR_MH10A\n",
       "47         2055  7494  HLR_MH10A\n",
       "48         2008  7494  HLR_MH10A\n",
       "49         2056  7494  HLR_MH10A\n",
       "50         2052  7494  HLR_MH10A"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulation_df[accumulation_df[('GENERAL INFO','NODE')]=='7494']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2122', '2119']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catchment_Count</th>\n",
       "      <th>Catchments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(GENERAL INFO, NODE)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>1</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>1</td>\n",
       "      <td>(2122,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6787</th>\n",
       "      <td>1</td>\n",
       "      <td>(2122,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>1</td>\n",
       "      <td>(2122,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>1</td>\n",
       "      <td>(2122,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9766</th>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9764</th>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9763</th>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9765</th>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9632</th>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Catchment_Count Catchments\n",
       "(GENERAL INFO, NODE)                            \n",
       "7311                                1       2169\n",
       "6786                                1    (2122,)\n",
       "6787                                1    (2122,)\n",
       "6788                                1    (2122,)\n",
       "6789                                1    (2122,)\n",
       "...                               ...        ...\n",
       "9766                              104        NaN\n",
       "9764                              104        NaN\n",
       "9763                              104        NaN\n",
       "9765                              104        NaN\n",
       "9632                              105        NaN\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Merge_ID</th>\n",
       "      <th>Catchments</th>\n",
       "      <th>Catchment_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merge_ID_82</td>\n",
       "      <td>(10108, 10109, 10110, 2059, 2060, 2061, 2062, ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merge_ID_96</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Merge_ID_57</td>\n",
       "      <td>(2059, 2060, 2061, 2062, 2063, 2064, 2065, 206...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merge_ID_52</td>\n",
       "      <td>(2059, 2061, 2062, 2063, 2064, 2065, 2066, 206...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merge_ID_16</td>\n",
       "      <td>(2059, 2062, 2063, 2064, 2066, 2068, 2069, 207...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Merge_ID_43</td>\n",
       "      <td>(2050, 2052)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Merge_ID_38</td>\n",
       "      <td>(2128, 2136)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Merge_ID_79</td>\n",
       "      <td>(2052,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Merge_ID_55</td>\n",
       "      <td>(2122,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Merge_ID_75</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Merge_ID  ... Catchment_Count\n",
       "0    Merge_ID_82  ...             105\n",
       "1    Merge_ID_96  ...             104\n",
       "2    Merge_ID_57  ...              99\n",
       "3    Merge_ID_52  ...              98\n",
       "4    Merge_ID_16  ...              95\n",
       "..           ...  ...             ...\n",
       "98   Merge_ID_43  ...               2\n",
       "99   Merge_ID_38  ...               2\n",
       "100  Merge_ID_79  ...               1\n",
       "101  Merge_ID_55  ...               1\n",
       "102  Merge_ID_75  ...               1\n",
       "\n",
       "[103 rows x 3 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_set = set()\n",
    "\n",
    "rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "# rank_df.reset_index(inplace=True)\n",
    "rank_df.columns = ['Catchment_Count']\n",
    "max_catchments = max(rank_df.Catchment_Count)\n",
    "rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "# rank_df.reset_index(inplace=True)\n",
    "\n",
    "catchment_list = []\n",
    "merge_set = set()\n",
    "for index, row in rank_df.iterrows():\n",
    "\n",
    "    catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "    catchments = tuple(sorted(catchments))\n",
    "#     rank_df.loc[index,'Catchments'] = catchments\n",
    "    catchment_list.append(catchments)\n",
    "    merge_set.add(catchments)\n",
    "    \n",
    "    \n",
    "rank_df['Catchments'] = catchment_list\n",
    "rank_df['Node'] = rank_df.index\n",
    "print(len(merge_set))\n",
    "\n",
    "merge_list = []\n",
    "for i, catchments in enumerate(merge_set):\n",
    "    merge_id = 'Merge_ID_' + str(i)\n",
    "    merge_list.append([merge_id,catchments])\n",
    "    \n",
    "merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# rank_df = pd.merge(rank_df,merge_df, on=['Catchments'],how='inner')\n",
    "# rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "print(max_catchments)\n",
    "\n",
    "# for index1, row1 in merge_df.iterrows():\n",
    "#     print\n",
    "\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index1: 0\n",
      "Index1: 1\n",
      "Index1: 2\n",
      "Index1: 3\n",
      "Index1: 4\n",
      "Index1: 5\n",
      "Index1: 6\n",
      "Index1: 7\n",
      "Index1: 8\n",
      "Index1: 9\n",
      "Index1: 10\n",
      "Index1: 11\n",
      "Index1: 12\n",
      "Index1: 13\n",
      "Index1: 14\n",
      "Index1: 15\n",
      "Index1: 16\n",
      "Index1: 17\n",
      "Index1: 18\n",
      "Index1: 19\n",
      "Index1: 20\n",
      "Index1: 21\n",
      "Index1: 22\n",
      "Index1: 23\n",
      "Index1: 24\n",
      "Index1: 25\n",
      "Index1: 26\n",
      "Index1: 27\n",
      "Index1: 28\n",
      "Index1: 29\n",
      "Index1: 30\n",
      "Index1: 31\n",
      "Index1: 32\n",
      "Index1: 33\n",
      "Index1: 34\n",
      "Index1: 35\n",
      "Index1: 36\n",
      "Index1: 37\n",
      "Index1: 38\n",
      "Index1: 39\n",
      "Index1: 40\n",
      "Index1: 41\n",
      "Index1: 42\n",
      "Index1: 43\n",
      "Index1: 44\n",
      "Index1: 45\n",
      "Index1: 46\n",
      "Index1: 47\n",
      "Index1: 48\n",
      "Index1: 49\n",
      "Index1: 50\n",
      "Index1: 51\n",
      "Index1: 52\n",
      "Index1: 53\n",
      "Index1: 54\n",
      "Index1: 55\n",
      "Index1: 56\n",
      "Index1: 57\n",
      "Index1: 58\n",
      "Index1: 59\n",
      "Index1: 60\n",
      "Index1: 61\n",
      "Index1: 62\n",
      "Index1: 63\n",
      "Index1: 64\n",
      "Index1: 65\n",
      "Index1: 66\n",
      "Index1: 67\n",
      "Index1: 68\n",
      "Index1: 69\n",
      "Index1: 70\n",
      "Index1: 71\n",
      "Index1: 72\n",
      "Index1: 73\n",
      "Index1: 74\n",
      "Index1: 75\n",
      "Index1: 76\n",
      "Index1: 77\n",
      "Index1: 78\n",
      "Index1: 79\n",
      "Index1: 80\n",
      "Index1: 81\n",
      "Index1: 82\n",
      "Index1: 83\n",
      "Index1: 84\n",
      "Index1: 85\n",
      "Index1: 86\n",
      "Index1: 87\n",
      "Index1: 88\n",
      "Index1: 89\n",
      "Index1: 90\n",
      "Index1: 91\n",
      "Index1: 92\n",
      "Index1: 93\n",
      "Index1: 94\n",
      "Index1: 95\n",
      "Index1: 96\n",
      "Index1: 97\n",
      "Index1: 98\n",
      "Index1: 99\n",
      "Index1: 100\n",
      "Index1: 101\n",
      "Index1: 102\n",
      "5253\n"
     ]
    }
   ],
   "source": [
    "cou = 0\n",
    "for index1, row1 in merge_df.iterrows():\n",
    "    catchments1 = list(row1['Catchments']_\n",
    "    print(f'Index1: {index1}')\n",
    "    for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "        catchments2 = row2['Catchments']\n",
    "        if len(catchments1) >= len(catchments2):\n",
    "            if all(item in catchments1 for item in catchments1):\n",
    "                       \n",
    "            \n",
    "\n",
    "print(cou)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort_values in module pandas.core.frame:\n",
      "\n",
      "sort_values(by, axis: 'Axis' = 0, ascending=True, inplace: 'bool' = False, kind: 'str' = 'quicksort', na_position: 'str' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc' = None) method of pandas.core.frame.DataFrame instance\n",
      "    Sort by the values along either axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "            by : str or list of str\n",
      "                Name or list of names to sort by.\n",
      "    \n",
      "                - if `axis` is 0 or `'index'` then `by` may contain index\n",
      "                  levels and/or column labels.\n",
      "                - if `axis` is 1 or `'columns'` then `by` may contain column\n",
      "                  levels and/or index labels.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "         Axis to be sorted.\n",
      "    ascending : bool or list of bool, default True\n",
      "         Sort ascending vs. descending. Specify list for multiple sort\n",
      "         orders.  If this is a list of bools, must match the length of\n",
      "         the by.\n",
      "    inplace : bool, default False\n",
      "         If True, perform operation in-place.\n",
      "    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n",
      "         Choice of sorting algorithm. See also :func:`numpy.sort` for more\n",
      "         information. `mergesort` and `stable` are the only stable algorithms. For\n",
      "         DataFrames, this option is only applied when sorting on a single\n",
      "         column or label.\n",
      "    na_position : {'first', 'last'}, default 'last'\n",
      "         Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
      "         end.\n",
      "    ignore_index : bool, default False\n",
      "         If True, the resulting axis will be labeled 0, 1, , n - 1.\n",
      "    \n",
      "         .. versionadded:: 1.0.0\n",
      "    \n",
      "    key : callable, optional\n",
      "        Apply the key function to the values\n",
      "        before sorting. This is similar to the `key` argument in the\n",
      "        builtin :meth:`sorted` function, with the notable difference that\n",
      "        this `key` function should be *vectorized*. It should expect a\n",
      "        ``Series`` and return a Series with the same shape as the input.\n",
      "        It will be applied to each column in `by` independently.\n",
      "    \n",
      "        .. versionadded:: 1.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        DataFrame with sorted values or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.sort_index : Sort a DataFrame by the index.\n",
      "    Series.sort_values : Similar method for a Series.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
      "    ...     'col2': [2, 1, 9, 8, 7, 4],\n",
      "    ...     'col3': [0, 1, 9, 4, 2, 3],\n",
      "    ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
      "    ... })\n",
      "    >>> df\n",
      "      col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    \n",
      "    Sort by col1\n",
      "    \n",
      "    >>> df.sort_values(by=['col1'])\n",
      "      col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    5    C     4     3    F\n",
      "    4    D     7     2    e\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Sort by multiple columns\n",
      "    \n",
      "    >>> df.sort_values(by=['col1', 'col2'])\n",
      "      col1  col2  col3 col4\n",
      "    1    A     1     1    B\n",
      "    0    A     2     0    a\n",
      "    2    B     9     9    c\n",
      "    5    C     4     3    F\n",
      "    4    D     7     2    e\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Sort Descending\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False)\n",
      "      col1  col2  col3 col4\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    2    B     9     9    c\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Putting NAs first\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
      "      col1  col2  col3 col4\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    2    B     9     9    c\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    \n",
      "    Sorting with a key function\n",
      "    \n",
      "    >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n",
      "       col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    \n",
      "    Natural sort with the key argument,\n",
      "    using the `natsort <https://github.com/SethMMorton/natsort>` package.\n",
      "    \n",
      "    >>> df = pd.DataFrame({\n",
      "    ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n",
      "    ...    \"value\": [10, 20, 30, 40, 50]\n",
      "    ... })\n",
      "    >>> df\n",
      "        time  value\n",
      "    0    0hr     10\n",
      "    1  128hr     20\n",
      "    2   72hr     30\n",
      "    3   48hr     40\n",
      "    4   96hr     50\n",
      "    >>> from natsort import index_natsorted\n",
      "    >>> df.sort_values(\n",
      "    ...    by=\"time\",\n",
      "    ...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n",
      "    ... )\n",
      "        time  value\n",
      "    0    0hr     10\n",
      "    3   48hr     40\n",
      "    2   72hr     30\n",
      "    4   96hr     50\n",
      "    1  128hr     20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rank_df.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample data for the DataFrame\n",
    "# data = {\n",
    "#     ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "#     ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame with MultiIndex columns\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Set names for the levels of the MultiIndex\n",
    "# # df.columns.names = ['Header', 'Subheader']\n",
    "\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
