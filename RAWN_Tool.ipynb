{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-08-08, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model to list the upstream catchments of each manhole (in flow splits, each split direction will both get 100% of the upstream flow). It then uses the MPF sheets to summarise population and residential/ICI areas for each catchment. Aggregated catchment contributions to each manhole are then used as input to RAWN calculations. For each manhole, the upstream catchments are dissolved together and added to a layer 'Node_Catchment'. This is used as input to 'Map Series' (similar to 'Data Driven Pages' in ArcGIS Desktop). This is used to create one jpg image per manhole, highlighting the manhole and all its upstream catchments. The RAWN calculations and images are added to RAWN spreadsheets which are similar in layout to the original RAWN sheets which are derived from Neural Network. One important difference in RAWN calculations to previous is that no peaking factor is allowed to go below 1.5, as per official RAWN guidelines. Peaking factors that reach the minimum are highlighted in yellow. Also a Google Maps link is added to each sheet. The user can choose between adding RAWN calculations as values or Excel formulas. Excel formulas are recommended for full transparency of the calculations to those using the sheets. However, python calculations are maintained in the code to enable future additional outputs (Power BI, HTML etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, numbers\n",
    "from openpyxl.formatting.rule import FormulaRule\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "import subprocess\n",
    "import gc\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "\n",
    "#Create functions for to use SQL with the model sqlite database.\n",
    "\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "\n",
    "#User Input\n",
    "try:\n",
    "\n",
    "    model = 'VSA'\n",
    "\n",
    "    max_steps = 1005 #\n",
    "    use_formula = True\n",
    "    transfer_mixed_area_to_commercial = True\n",
    "\n",
    "    gdb_name = 'RAWN.gdb'\n",
    "    gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "    #Options to skip time consuming steps during debug (by setting to False), must be True during production runs.\n",
    "    run_dissolve = True\n",
    "    run_jpg = True\n",
    "\n",
    "    check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "    #                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "    #                      A more robust check may be developed in the future.\n",
    "\n",
    "\n",
    "    induced_values = [] #Only to be appended to under specific models where required\n",
    "\n",
    "    if model == 'NSSA':   \n",
    "        model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "        pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "        pop_sheet = 'MPF Update 4'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '22602'\n",
    "        line_exclusions = []\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 89\n",
    "        excluded_acronyms_csv = ''\n",
    "\n",
    "    if model == 'FSA':   \n",
    "        model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\01_MASTER_MODEL\\MODEL\\FSA_Base_2021pop.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'MPF Update 17'\n",
    "        scenario = '2030_Network'\n",
    "        wwtp_muid = '1162'\n",
    "        line_exclusions = ['GoldenEar_Dummy_Pump','GoldenEarSSOLink','GoldenEar_SSO_Tank_Cell_Dummy_Link3','MH35_Orifice','44473']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 154\n",
    "        excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model\\FSA_Excluded_Acronyms.csv\"\n",
    "        induced_values.append(['84674','Area_Ind',257.3]) #Vancouver landfill, equivalent area to get 520 L/s PWWF.\n",
    "        \n",
    "    if model == 'VSA':   \n",
    "        model_output_folder = r\"J:\\SEWER_AREA_MODELS\\VSA\\04_ANALYSIS_WORK\\79. RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\VSA\\03_SIMULATION_WORK\\Key_Flow_HGL_GIS_Sim_VSA\\MIKE+_Import\\VSA_V304.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\VSA\\02_MODEL_COMPONENTS\\03_NETWORKS\\11. NETWORK_UPDATE\\O. V304 Updates\\VSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'Export'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '7478'\n",
    "        line_exclusions = ['51833','51834','51835','FS-241','FS-253','SADR082 SALIB DIV TO VICT','Victoria_Drive_Weir']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 304\n",
    "        excluded_acronyms_csv = ''\n",
    "\n",
    "    #Do not change the lines below\n",
    "    sewer_area = model \n",
    "    global_output_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder \n",
    "    \n",
    "    #Warn user if run_dissolve or run_jpg is False\n",
    "    if run_dissolve == False or run_jpg == False:\n",
    "        if MessageBox(None, b'run_dissolve and/or run_jpg is set to False.\\n\\nThis is only allowed in special cases!\\n\\nContinue?', b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please set both run_dissolve and run_jpg to True.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 3', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "    \n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    #Items: [Keyword (upper Excel header),Type ('lower Excel header'),Average(lower Excel header),Unit flow cell address, quantifyer column]\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)','$D$3','H']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$4','K']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$5','N']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)','$D$6','Q']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)','$D$7','T']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)','$D$7','T']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid == 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + str(altid)\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "    \n",
    "    sql = \"SELECT msm_Catchcon.catchid AS Catchment, msm_Catchcon.nodeid AS Connected_Node FROM msm_Catchcon INNER JOIN msm_catchment \"\n",
    "    sql += \"ON msm_Catchcon.catchid = msm_catchment.muid WHERE msm_Catchment.nettypeno <> 2 AND msm_Catchcon.Active = 1 AND msm_catchment.Active = 1 \"\n",
    "    sql += \"AND msm_Catchcon.enabled = 1 AND msm_catchment.enabled = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1 AND enabled = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1 AND enabled = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1 AND enabled = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1 AND enabled = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "    \n",
    "    lines = lines[~lines['MUID'].isin(line_exclusions)]\n",
    "    \n",
    "    excluded_acronyms = []\n",
    "    if excluded_acronyms_csv != '':\n",
    "        excluded_acronyms = pd.read_csv(excluded_acronyms_csv)\n",
    "        excluded_acronyms = list(excluded_acronyms.iloc[:, 0])\n",
    "\n",
    "    sql = \"SELECT muid, UPPER(acronym) AS acronym, UPPER(owner) AS owner, UPPER(assetname) AS assetname FROM msm_Node WHERE active = 1 AND enabled = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(((node_id_df.assetname.str[:2]=='MH') & (node_id_df.acronym.notna()) & (node_id_df.acronym!='') & \n",
    "        (node_id_df.owner=='GV') & (~node_id_df.acronym.isin(excluded_acronyms))) | (node_id_df.muid==wwtp_muid))]\n",
    "    \n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df.loc[node_id_df['Node'] == wwtp_muid, 'ID'] = 'WWTP'\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "    duplicate_ids = node_id_df[node_id_df.duplicated('ID', keep=False)]\n",
    "    node_id_df.loc[duplicate_ids.index, 'ID'] = node_id_df['ID'] + '_(' + node_id_df['Node'] + ')'\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "\n",
    "#Import population\n",
    "\n",
    "try:\n",
    "    def warning_message(message):\n",
    "        if MessageBox(None, message.encode('utf-8'), b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please report the issue to the Master Population File admin.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df.fillna(0, inplace=True) #Fill NA with 0 or the sum of all will be NA\n",
    "    \n",
    "    if transfer_mixed_area_to_commercial:\n",
    "        pop_df.Area_Com = pop_df.Area_Com + pop_df.Area_Mixed\n",
    "        pop_df.Area_Mixed = 0\n",
    "    \n",
    "    for induced_value in induced_values:\n",
    "        catchment = induced_value[0]\n",
    "        column = induced_value[1] \n",
    "        value = induced_value[2]\n",
    "        pop_df.loc[pop_df.Catchment == catchment,column] += value\n",
    "        \n",
    "        \n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "        \n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    #Check if Pop_Total = 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (sum of all rows)\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "        message = f\"Warning. The sum of 'Population' in 'Pop_Total' ({pop_sum_total_col:,}) is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' ({pop_sum_sub_cols:,}).\"\n",
    "        message += \"\\n\\n'Pop_Total' will be used.\\n\\nContinue?\"\n",
    "        warning_message(message)\n",
    "     \n",
    "    #Check if some rows have population but 0 area\n",
    "    res_area_check_df = pop_df[(pop_df.Population > 0) & (pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed == 0)]\n",
    "    if len(res_area_check_df) > 0:\n",
    "        warning_message(f\"{len(res_area_check_df):,} out of {len(pop_df):,} rows have population but 0 residential area.\\n\\nPrint dataframe 'res_area_check_df' to see all.\\n\\nContinue?\")\n",
    "    \n",
    "    #Check is some catchment/year combinations are not found.\n",
    "    catchment_years = []\n",
    "    for muid in list(catchments.Catchment.unique()):\n",
    "        for year in years:\n",
    "            catchment_years.append([muid,year])\n",
    "    catchment_year_df = pd.DataFrame(catchment_years,columns=(['Catchment','Year']))\n",
    "    merged = catchment_year_df.merge(pop_df[['Catchment', 'Year']], on=['Catchment', 'Year'], how='left', indicator=True)\n",
    "    not_founds = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    if len(not_founds) > 0:\n",
    "        message = \"The following catchment/year combinations are not found:\\n\\n\"\n",
    "        for index, row in not_founds[:20].iterrows():\n",
    "            message += row[0] + ', ' + str(row[1]) + '.\\n'\n",
    "        if len(not_founds) > 20:\n",
    "             message += \"and more, print variable 'not_founds' to see the rest.\"                                                 \n",
    "        message += '\\n\\nContinue?'\n",
    "        warning_message(message)\n",
    "                                                                      \n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "        \n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]  \n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                \n",
    "#                 catchment_mus.append(catchment)\n",
    "#                 print('skipped for ' + catchment)\n",
    "#                 break\n",
    "                \n",
    "                \n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Last node traced is '\" + node + \"'\")\n",
    "                \n",
    "                \n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl']:\n",
    "                    adwf += avg_flow    \n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)),1.5) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*0.8,1.5)*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)),1.5) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    \n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = np.maximum(\n",
    "        0.8 * (\n",
    "            1 + 14 / (\n",
    "                4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5\n",
    "            )\n",
    "        ) * np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ), \n",
    "        1.5\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    \n",
    "    node_df[('FLOWS','PWWF (L/s)')] = node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] + node_df[('COMMERCIAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] + node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] + node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending msm_CatchCon\n",
      "Appending msm_Catchment\n",
      "Appending msm_Link\n",
      "Appending msm_Node\n",
      "Appending msm_Pump\n",
      "Appending msm_Weir\n",
      "Appending msm_Orifice\n",
      "Appending msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "\n",
    "#Import GIS from the model\n",
    "#Note that if the RAWN database does not already exist with the layers inside, then it will be created \n",
    "#but in this casethe map symbology will be reset and Map Series reset as well, creating extra work.\n",
    "#An extra word of caution: The first import of e.g. msm_Catchment sets the maximum extend, meaning if you import NSSA in a fresh database,\n",
    "#then later import FSA, then many catchments will not be imported due to being outside the extend of the old import.\n",
    "#In the current database, an initial model with added elements outside the extend of all MV models was used to prime the layer. \n",
    "#If the database is not maintained, this will need to be done again.\n",
    "#Efforts to programmatically increase the layer extend were unsuccessful but may later get resolved.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(out_path + '\\\\' + layer):\n",
    "            print('Appending ' + layer)\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:  \n",
    "            print('Creating ' + layer)\n",
    "\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer + '_Test')\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        arcpy.Project_management('msm_Node', 'msm_Node_Google',arcpy.SpatialReference(4326))\n",
    "        centroids = arcpy.da.FeatureClassToNumPyArray('msm_Node_Google', (\"MUID\",\"SHAPE@X\",\"SHAPE@Y\"))\n",
    "        centroids = centroids.tolist()\n",
    "        centroids_df = pd.DataFrame(centroids, columns =['MUID','X','Y'])\n",
    "        centroids_df.set_index('MUID',inplace=True)\n",
    "\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "\n",
    "#Create merge_df (realizing the terms 'merge' and 'dissolve' are used here but they mean the same), \n",
    "#minimizing the number of computation heavy dissolves that need to be done.\n",
    "#It prevents the duplicate effort of merging the same catchments together multiple times,\n",
    "#by moving downstream and reusing upstream merges for downstream merges.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_217, 0 of 282 at time 2024-08-08 08:14:21.646128\n",
      "Dissolving for Merge_ID_9, 1 of 282 at time 2024-08-08 08:15:00.225425\n",
      "Dissolving for Merge_ID_272, 2 of 282 at time 2024-08-08 08:15:32.431891\n",
      "Dissolving for Merge_ID_178, 3 of 282 at time 2024-08-08 08:15:55.106636\n",
      "Dissolving for Merge_ID_226, 4 of 282 at time 2024-08-08 08:16:15.284097\n",
      "Dissolving for Merge_ID_190, 5 of 282 at time 2024-08-08 08:16:53.903948\n",
      "Dissolving for Merge_ID_269, 6 of 282 at time 2024-08-08 08:17:13.637002\n",
      "Dissolving for Merge_ID_222, 7 of 282 at time 2024-08-08 08:17:32.336121\n",
      "Dissolving for Merge_ID_220, 8 of 282 at time 2024-08-08 08:17:49.013380\n",
      "Dissolving for Merge_ID_270, 9 of 282 at time 2024-08-08 08:18:10.716340\n",
      "Dissolving for Merge_ID_267, 10 of 282 at time 2024-08-08 08:18:25.629985\n",
      "Dissolving for Merge_ID_80, 11 of 282 at time 2024-08-08 08:18:55.143987\n",
      "Dissolving for Merge_ID_115, 12 of 282 at time 2024-08-08 08:19:09.020683\n",
      "Dissolving for Merge_ID_51, 13 of 282 at time 2024-08-08 08:20:43.956111\n",
      "Dissolving for Merge_ID_245, 14 of 282 at time 2024-08-08 08:20:55.525696\n",
      "Dissolving for Merge_ID_113, 15 of 282 at time 2024-08-08 08:21:07.257430\n",
      "Dissolving for Merge_ID_257, 16 of 282 at time 2024-08-08 08:21:18.483701\n",
      "Dissolving for Merge_ID_37, 17 of 282 at time 2024-08-08 08:21:29.575859\n",
      "Dissolving for Merge_ID_252, 18 of 282 at time 2024-08-08 08:21:56.976928\n",
      "Dissolving for Merge_ID_108, 19 of 282 at time 2024-08-08 08:22:07.566617\n",
      "Dissolving for Merge_ID_104, 20 of 282 at time 2024-08-08 08:22:29.984161\n",
      "Dissolving for Merge_ID_240, 21 of 282 at time 2024-08-08 08:22:48.350169\n",
      "Dissolving for Merge_ID_237, 22 of 282 at time 2024-08-08 08:23:33.659130\n",
      "Dissolving for Merge_ID_242, 23 of 282 at time 2024-08-08 08:23:44.422097\n",
      "Dissolving for Merge_ID_274, 24 of 282 at time 2024-08-08 08:24:02.719042\n",
      "Dissolving for Merge_ID_88, 25 of 282 at time 2024-08-08 08:24:26.027628\n",
      "Dissolving for Merge_ID_65, 26 of 282 at time 2024-08-08 08:24:49.930765\n",
      "Dissolving for Merge_ID_111, 27 of 282 at time 2024-08-08 08:25:02.327285\n",
      "Dissolving for Merge_ID_114, 28 of 282 at time 2024-08-08 08:25:15.876832\n",
      "Dissolving for Merge_ID_125, 29 of 282 at time 2024-08-08 08:25:47.183328\n",
      "Dissolving for Merge_ID_132, 30 of 282 at time 2024-08-08 08:26:03.164128\n",
      "Dissolving for Merge_ID_64, 31 of 282 at time 2024-08-08 08:26:30.331300\n",
      "Dissolving for Merge_ID_140, 32 of 282 at time 2024-08-08 08:26:53.481740\n",
      "Dissolving for Merge_ID_234, 33 of 282 at time 2024-08-08 08:27:20.794554\n",
      "Dissolving for Merge_ID_209, 34 of 282 at time 2024-08-08 08:27:33.872600\n",
      "Dissolving for Merge_ID_203, 35 of 282 at time 2024-08-08 08:27:54.192694\n",
      "Dissolving for Merge_ID_24, 36 of 282 at time 2024-08-08 08:28:14.625993\n",
      "Dissolving for Merge_ID_36, 37 of 282 at time 2024-08-08 08:28:27.325114\n",
      "Dissolving for Merge_ID_180, 38 of 282 at time 2024-08-08 08:28:54.599068\n",
      "Dissolving for Merge_ID_146, 39 of 282 at time 2024-08-08 08:30:40.678637\n",
      "Dissolving for Merge_ID_97, 40 of 282 at time 2024-08-08 08:31:08.592182\n",
      "Dissolving for Merge_ID_5, 41 of 282 at time 2024-08-08 08:32:08.255805\n",
      "Dissolving for Merge_ID_35, 42 of 282 at time 2024-08-08 08:32:25.053184\n",
      "Dissolving for Merge_ID_130, 43 of 282 at time 2024-08-08 08:32:40.226073\n",
      "Dissolving for Merge_ID_136, 44 of 282 at time 2024-08-08 08:33:24.587694\n",
      "Dissolving for Merge_ID_50, 45 of 282 at time 2024-08-08 08:34:31.397862\n",
      "Dissolving for Merge_ID_0, 46 of 282 at time 2024-08-08 08:34:56.025904\n",
      "Dissolving for Merge_ID_25, 47 of 282 at time 2024-08-08 08:35:20.660619\n",
      "Dissolving for Merge_ID_13, 48 of 282 at time 2024-08-08 08:35:37.877874\n",
      "Dissolving for Merge_ID_66, 49 of 282 at time 2024-08-08 08:35:54.913466\n",
      "Dissolving for Merge_ID_23, 50 of 282 at time 2024-08-08 08:36:12.711762\n",
      "Dissolving for Merge_ID_59, 51 of 282 at time 2024-08-08 08:37:18.877323\n",
      "Dissolving for Merge_ID_133, 52 of 282 at time 2024-08-08 08:37:35.489534\n",
      "Dissolving for Merge_ID_75, 53 of 282 at time 2024-08-08 08:37:48.607541\n",
      "Dissolving for Merge_ID_224, 54 of 282 at time 2024-08-08 08:38:25.966236\n",
      "Dissolving for Merge_ID_275, 55 of 282 at time 2024-08-08 08:38:47.233706\n",
      "Dissolving for Merge_ID_6, 56 of 282 at time 2024-08-08 08:39:04.441463\n",
      "Dissolving for Merge_ID_251, 57 of 282 at time 2024-08-08 08:39:36.782121\n",
      "Dissolving for Merge_ID_238, 58 of 282 at time 2024-08-08 08:39:50.836630\n",
      "Dissolving for Merge_ID_167, 59 of 282 at time 2024-08-08 08:40:12.314527\n",
      "Dissolving for Merge_ID_68, 60 of 282 at time 2024-08-08 08:40:52.775507\n",
      "Dissolving for Merge_ID_84, 61 of 282 at time 2024-08-08 08:41:10.604016\n",
      "Dissolving for Merge_ID_171, 62 of 282 at time 2024-08-08 08:41:41.569175\n",
      "Dissolving for Merge_ID_162, 63 of 282 at time 2024-08-08 08:41:54.967574\n",
      "Dissolving for Merge_ID_86, 64 of 282 at time 2024-08-08 08:42:28.500108\n",
      "Dissolving for Merge_ID_74, 65 of 282 at time 2024-08-08 08:42:45.558894\n",
      "Dissolving for Merge_ID_216, 66 of 282 at time 2024-08-08 08:43:10.653116\n",
      "Dissolving for Merge_ID_110, 67 of 282 at time 2024-08-08 08:43:40.840051\n",
      "Dissolving for Merge_ID_144, 68 of 282 at time 2024-08-08 08:43:54.665846\n",
      "Dissolving for Merge_ID_60, 69 of 282 at time 2024-08-08 08:44:17.261756\n",
      "Dissolving for Merge_ID_27, 70 of 282 at time 2024-08-08 08:44:49.334253\n",
      "Dissolving for Merge_ID_202, 71 of 282 at time 2024-08-08 08:45:12.249218\n",
      "Dissolving for Merge_ID_201, 72 of 282 at time 2024-08-08 08:45:41.316812\n",
      "Dissolving for Merge_ID_105, 73 of 282 at time 2024-08-08 08:46:07.338620\n",
      "Dissolving for Merge_ID_198, 74 of 282 at time 2024-08-08 08:46:21.157766\n",
      "Dissolving for Merge_ID_48, 75 of 282 at time 2024-08-08 08:46:35.123052\n",
      "Dissolving for Merge_ID_98, 76 of 282 at time 2024-08-08 08:46:55.712889\n",
      "Dissolving for Merge_ID_181, 77 of 282 at time 2024-08-08 08:47:09.820797\n",
      "Dissolving for Merge_ID_179, 78 of 282 at time 2024-08-08 08:47:53.090395\n",
      "Dissolving for Merge_ID_261, 79 of 282 at time 2024-08-08 08:48:20.641602\n",
      "Dissolving for Merge_ID_137, 80 of 282 at time 2024-08-08 08:48:58.761478\n",
      "Dissolving for Merge_ID_134, 81 of 282 at time 2024-08-08 08:49:16.151388\n",
      "Dissolving for Merge_ID_169, 82 of 282 at time 2024-08-08 08:49:29.931996\n",
      "Dissolving for Merge_ID_236, 83 of 282 at time 2024-08-08 08:49:43.778665\n",
      "Dissolving for Merge_ID_176, 84 of 282 at time 2024-08-08 08:49:57.695397\n",
      "Dissolving for Merge_ID_18, 85 of 282 at time 2024-08-08 08:50:11.548071\n",
      "Dissolving for Merge_ID_271, 86 of 282 at time 2024-08-08 08:50:28.420514\n",
      "Dissolving for Merge_ID_239, 87 of 282 at time 2024-08-08 08:50:49.812086\n",
      "Dissolving for Merge_ID_265, 88 of 282 at time 2024-08-08 08:51:23.296721\n",
      "Dissolving for Merge_ID_57, 89 of 282 at time 2024-08-08 08:51:45.623147\n",
      "Dissolving for Merge_ID_89, 90 of 282 at time 2024-08-08 08:51:59.907216\n",
      "Dissolving for Merge_ID_8, 91 of 282 at time 2024-08-08 08:52:17.881668\n",
      "Dissolving for Merge_ID_148, 92 of 282 at time 2024-08-08 08:53:05.086923\n",
      "Dissolving for Merge_ID_30, 93 of 282 at time 2024-08-08 08:53:18.893555\n",
      "Dissolving for Merge_ID_211, 94 of 282 at time 2024-08-08 08:53:37.352946\n",
      "Dissolving for Merge_ID_168, 95 of 282 at time 2024-08-08 08:53:52.641934\n",
      "Dissolving for Merge_ID_44, 96 of 282 at time 2024-08-08 08:54:08.265742\n",
      "Dissolving for Merge_ID_21, 97 of 282 at time 2024-08-08 08:55:34.282457\n",
      "Dissolving for Merge_ID_95, 98 of 282 at time 2024-08-08 08:55:59.314359\n",
      "Dissolving for Merge_ID_277, 99 of 282 at time 2024-08-08 08:56:31.936204\n",
      "Dissolving for Merge_ID_71, 100 of 282 at time 2024-08-08 08:57:01.563117\n",
      "Dissolving for Merge_ID_182, 101 of 282 at time 2024-08-08 08:57:29.891374\n",
      "Dissolving for Merge_ID_276, 102 of 282 at time 2024-08-08 08:58:10.051592\n",
      "Dissolving for Merge_ID_77, 103 of 282 at time 2024-08-08 08:58:31.073086\n",
      "Dissolving for Merge_ID_177, 104 of 282 at time 2024-08-08 08:58:56.572722\n",
      "Dissolving for Merge_ID_118, 105 of 282 at time 2024-08-08 08:59:26.858303\n",
      "Dissolving for Merge_ID_10, 106 of 282 at time 2024-08-08 09:00:10.627448\n",
      "Dissolving for Merge_ID_196, 107 of 282 at time 2024-08-08 09:00:41.865900\n",
      "Dissolving for Merge_ID_22, 108 of 282 at time 2024-08-08 09:00:58.331155\n",
      "Dissolving for Merge_ID_250, 109 of 282 at time 2024-08-08 09:01:28.479087\n",
      "Dissolving for Merge_ID_156, 110 of 282 at time 2024-08-08 09:01:49.934803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_188, 111 of 282 at time 2024-08-08 09:02:06.681125\n",
      "Dissolving for Merge_ID_62, 112 of 282 at time 2024-08-08 09:02:23.139182\n",
      "Dissolving for Merge_ID_128, 113 of 282 at time 2024-08-08 09:02:44.507733\n",
      "Dissolving for Merge_ID_143, 114 of 282 at time 2024-08-08 09:03:00.610465\n",
      "Dissolving for Merge_ID_26, 115 of 282 at time 2024-08-08 09:03:16.294814\n",
      "Dissolving for Merge_ID_45, 116 of 282 at time 2024-08-08 09:03:36.895662\n",
      "Dissolving for Merge_ID_233, 117 of 282 at time 2024-08-08 09:03:52.832243\n",
      "Dissolving for Merge_ID_243, 118 of 282 at time 2024-08-08 09:04:08.129238\n",
      "Dissolving for Merge_ID_72, 119 of 282 at time 2024-08-08 09:04:23.249072\n",
      "Dissolving for Merge_ID_215, 120 of 282 at time 2024-08-08 09:05:05.588808\n",
      "Dissolving for Merge_ID_219, 121 of 282 at time 2024-08-08 09:05:20.926842\n",
      "Dissolving for Merge_ID_149, 122 of 282 at time 2024-08-08 09:05:36.142763\n",
      "Dissolving for Merge_ID_248, 123 of 282 at time 2024-08-08 09:07:18.710106\n",
      "Dissolving for Merge_ID_4, 124 of 282 at time 2024-08-08 09:07:56.038258\n",
      "Dissolving for Merge_ID_73, 125 of 282 at time 2024-08-08 09:08:17.997348\n",
      "Dissolving for Merge_ID_87, 126 of 282 at time 2024-08-08 09:09:06.597813\n",
      "Dissolving for Merge_ID_63, 127 of 282 at time 2024-08-08 09:09:26.553070\n",
      "Dissolving for Merge_ID_126, 128 of 282 at time 2024-08-08 09:09:42.006208\n",
      "Dissolving for Merge_ID_61, 129 of 282 at time 2024-08-08 09:10:13.046608\n",
      "Dissolving for Merge_ID_46, 130 of 282 at time 2024-08-08 09:10:38.098528\n",
      "Dissolving for Merge_ID_54, 131 of 282 at time 2024-08-08 09:11:09.694435\n",
      "Dissolving for Merge_ID_145, 132 of 282 at time 2024-08-08 09:11:26.121464\n",
      "Dissolving for Merge_ID_258, 133 of 282 at time 2024-08-08 09:11:51.085303\n",
      "Dissolving for Merge_ID_218, 134 of 282 at time 2024-08-08 09:12:11.329826\n",
      "Dissolving for Merge_ID_207, 135 of 282 at time 2024-08-08 09:12:26.482734\n",
      "Dissolving for Merge_ID_52, 136 of 282 at time 2024-08-08 09:12:44.855543\n",
      "Dissolving for Merge_ID_85, 137 of 282 at time 2024-08-08 09:13:08.643307\n",
      "Dissolving for Merge_ID_195, 138 of 282 at time 2024-08-08 09:13:23.299716\n",
      "Dissolving for Merge_ID_2, 139 of 282 at time 2024-08-08 09:13:37.556760\n",
      "Dissolving for Merge_ID_231, 140 of 282 at time 2024-08-08 09:13:52.463620\n",
      "Dissolving for Merge_ID_69, 141 of 282 at time 2024-08-08 09:14:27.744523\n",
      "Dissolving for Merge_ID_213, 142 of 282 at time 2024-08-08 09:14:45.938490\n",
      "Dissolving for Merge_ID_139, 143 of 282 at time 2024-08-08 09:15:09.297275\n",
      "Dissolving for Merge_ID_67, 144 of 282 at time 2024-08-08 09:15:28.747414\n",
      "Dissolving for Merge_ID_227, 145 of 282 at time 2024-08-08 09:15:42.948658\n",
      "Dissolving for Merge_ID_124, 146 of 282 at time 2024-08-08 09:16:09.734638\n",
      "Dissolving for Merge_ID_254, 147 of 282 at time 2024-08-08 09:16:50.383547\n",
      "Dissolving for Merge_ID_17, 148 of 282 at time 2024-08-08 09:17:13.538141\n",
      "Dissolving for Merge_ID_47, 149 of 282 at time 2024-08-08 09:17:32.175533\n",
      "Dissolving for Merge_ID_212, 150 of 282 at time 2024-08-08 09:17:56.129955\n",
      "Dissolving for Merge_ID_281, 151 of 282 at time 2024-08-08 09:18:11.100917\n",
      "Dissolving for Merge_ID_81, 152 of 282 at time 2024-08-08 09:18:30.418933\n",
      "Dissolving for Merge_ID_96, 153 of 282 at time 2024-08-08 09:18:54.246901\n",
      "Dissolving for Merge_ID_249, 154 of 282 at time 2024-08-08 09:19:09.546899\n",
      "Dissolving for Merge_ID_154, 155 of 282 at time 2024-08-08 09:19:33.802615\n",
      "Dissolving for Merge_ID_55, 156 of 282 at time 2024-08-08 09:19:54.509560\n",
      "Dissolving for Merge_ID_83, 157 of 282 at time 2024-08-08 09:20:25.164606\n",
      "Dissolving for Merge_ID_280, 158 of 282 at time 2024-08-08 09:20:42.842780\n",
      "Dissolving for Merge_ID_121, 159 of 282 at time 2024-08-08 09:21:41.148644\n",
      "Dissolving for Merge_ID_112, 160 of 282 at time 2024-08-08 09:21:57.548193\n",
      "Dissolving for Merge_ID_70, 161 of 282 at time 2024-08-08 09:22:29.093053\n",
      "Dissolving for Merge_ID_11, 162 of 282 at time 2024-08-08 09:23:15.341870\n",
      "Dissolving for Merge_ID_40, 163 of 282 at time 2024-08-08 09:23:53.831091\n",
      "Dissolving for Merge_ID_192, 164 of 282 at time 2024-08-08 09:24:23.876586\n",
      "Dissolving for Merge_ID_29, 165 of 282 at time 2024-08-08 09:24:39.534912\n",
      "Dissolving for Merge_ID_191, 166 of 282 at time 2024-08-08 09:24:55.171722\n",
      "Dissolving for Merge_ID_135, 167 of 282 at time 2024-08-08 09:25:10.846062\n",
      "Dissolving for Merge_ID_127, 168 of 282 at time 2024-08-08 09:25:30.543083\n",
      "Dissolving for Merge_ID_16, 169 of 282 at time 2024-08-08 09:26:05.605666\n",
      "Dissolving for Merge_ID_189, 170 of 282 at time 2024-08-08 09:26:39.054277\n",
      "Dissolving for Merge_ID_58, 171 of 282 at time 2024-08-08 09:26:59.457944\n",
      "Dissolving for Merge_ID_99, 172 of 282 at time 2024-08-08 09:27:55.699418\n",
      "Dissolving for Merge_ID_262, 173 of 282 at time 2024-08-08 09:28:17.566425\n",
      "Dissolving for Merge_ID_119, 174 of 282 at time 2024-08-08 09:29:08.459496\n",
      "Dissolving for Merge_ID_161, 175 of 282 at time 2024-08-08 09:29:30.048758\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "\n",
    "#Run dissolve (also referred to as merge).\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_name_dissolve)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df.iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Permanent cell 12\n",
    "\n",
    "#Export jpgs. \n",
    "#This process was too heavy to run inside this notebook, causing failed exports and freeze of the program.\n",
    "#It was therefore moved to an external script, called from here by writing a batch file and executing it.\n",
    "\n",
    "try:\n",
    "    \n",
    "    if run_jpg:\n",
    "       \n",
    "        aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "        project_path = aprx.filePath\n",
    "        project_folder = os.path.dirname(project_path)\n",
    "\n",
    "        jpg_folder = model_output_folder + r'\\jpg'\n",
    "        if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "        jpg_script = project_folder + '\\\\JPG_Subprocess.py'\n",
    "        bat_file_path = project_folder + '\\\\Execute_JPG.bat'\n",
    "        bat_file = open(bat_file_path, \"w\")\n",
    "        python_installation = sys.executable\n",
    "        python_installation = os.path.dirname(sys.executable) + r'\\Python\\envs\\arcgispro-py3\\python.exe'\n",
    "\n",
    "        bat_file_text = '@echo off\\n'\n",
    "        bat_file_text += 'set PYTHON_PATH=\"' + python_installation + '\"\\n'\n",
    "        bat_file_text += 'set SCRIPT_PATH=\"JPG_Subprocess.py\"\\n'\n",
    "        bat_file_text += 'set ARG1=\"' + project_path + '\"\\n'\n",
    "        bat_file_text += 'set ARG2=\"' + jpg_folder + '\"\\n'\n",
    "        bat_file_text += '%PYTHON_PATH% %SCRIPT_PATH% %ARG1% %ARG2%\\n'\n",
    "\n",
    "        bat_file.write(bat_file_text)\n",
    "        bat_file.close()\n",
    "        result = subprocess.call([bat_file_path]) \n",
    "\n",
    "        if result == 1: #Error\n",
    "            raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "            then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "\n",
    "        print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "    \n",
    "    username = os.getlogin()\n",
    "    date_created = str(datetime.datetime.today()).split(' ')[0]\n",
    "    \n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    backup_folder = f'{excel_folder}\\\\Backup_{date_created}_{model}_V{model_version}_{pop_sheet}'\n",
    "    \n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    if not os.path.isdir(backup_folder): os.makedirs(backup_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "        if use_formula: #Replace spreadsheet values with formulas\n",
    "            value_startrow = 17\n",
    "            for i in node_single_df.index:\n",
    "                currentrow = i + value_startrow -1\n",
    "                for avg_calc_dict_key in avg_calc_dict:\n",
    "#                     currentrow = i + value_startrow\n",
    "                    inputs = avg_calc_dict[avg_calc_dict_key]\n",
    "                    header = inputs[0]\n",
    "                    subheader = inputs[2]\n",
    "                    unitflow_ref = inputs[3]\n",
    "                    col_ref = inputs[4]\n",
    "                    formula = f'{r\"=\"}{unitflow_ref}*{col_ref}{currentrow}/86400'\n",
    "                    node_single_df.loc[i,(header,subheader)] = formula                \n",
    "                \n",
    "                node_single_df.loc[i,('RESIDENTIAL','PEAK FLOW (L/s)')] = f'{r\"=\"}MAX(1.5,(1+(14/(4+((H{currentrow}/1000)^0.5)))))*I{currentrow}'\n",
    "                node_single_df.loc[i,('COMMERCIAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"com\"][3]}*K{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5))))*0.8)*L{currentrow}'\n",
    "                \n",
    "                ind_formula = f'{r\"=\"}MAX(1.5,0.8*(1+((14)/(4+(((N{currentrow}*{avg_calc_dict[\"ind\"][3]}/{avg_calc_dict[\"res\"][3]}'\n",
    "                ind_formula += f')/1000)^0.5))))*IF(N{currentrow}<121,1.7,(2.505-0.1673*LN(N{currentrow}))))*O{currentrow}'\n",
    "                node_single_df.loc[i,('INDUSTRIAL','PEAK FLOW (L/s)')] = ind_formula\n",
    "                \n",
    "                node_single_df.loc[i,('INSTITUTIONAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"inst\"][3]}*Q{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5)))))*R{currentrow}'\n",
    "\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','AREA (Ha)')] = f'{r\"=\"}G{currentrow}+K{currentrow}+N{currentrow}+Q{currentrow}'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFLOW (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infl\"][3]}/86400'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFILTRATION (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infi\"][3]}/86400'\n",
    "                \n",
    "                node_single_df.loc[i,('FLOWS','AVG. SAN. FLOW (L/s)')] = f'{r\"=\"}I{currentrow}+L{currentrow}+O{currentrow}+R{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','ADWF (L/s)')] = f'{r\"=\"}W{currentrow}+V{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','PWWF (L/s)')] = \\\n",
    "                    f'{r\"=\"}J{currentrow}+M{currentrow}+P{currentrow}+S{currentrow}+U{currentrow}+V{currentrow}'\n",
    "                \n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        id = id.replace('\\\\','_')\n",
    "        id = id.upper()\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 23 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13\n",
    "        \n",
    "        #Delete empty row between header and data\n",
    "        sheet1.delete_rows(16)\n",
    "        \n",
    "        #Find the minimum factor in the sheet. Array formulas do not work with openpyxl, do divisions one by one instead\n",
    "        cols = [['I','J'],['L','M'],['O','P'],['R','S']]\n",
    "        rows = list(range(16,currentrow + 1))      \n",
    "        formula = r'=ROUND(MIN('\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                formula += f'IFERROR({col[1]}{row}/{col[0]}{row},999),'\n",
    "        formula = formula[:-1] + '),2)'        \n",
    "        sheet1['J9'] = formula\n",
    "        sheet1['H9'] = 'Lowest peaking factor:'\n",
    "        sheet1['H10'] = 'Lowest peaking factors at lower limit 1.5 highlighted in yellow.'\n",
    "                      \n",
    "        #Color code cells where peaking facor is at minimum.\n",
    "        format_formula = 'J16/I16<1.501'\n",
    "        format_range = ''\n",
    "        for col in cols:\n",
    "            format_range += f'{col[1]}16:{col[1]}{currentrow} '\n",
    "        format_range = format_range[:-1] #Remove last space.               \n",
    "        fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "        rule = FormulaRule(formula=[format_formula], fill=fill)\n",
    "        sheet1.conditional_formatting.add(format_range, rule)\n",
    "        \n",
    "        decimals = [['#,##0','H'],['#,##0.0','IJLMOPRSUVWXY'],['#,##0.00','GKNQT']]\n",
    "        for decimal in decimals:\n",
    "            for col in decimal[1]:\n",
    "                for row in range(16,currentrow+1):\n",
    "                    sheet1[f'{col}{row}'].number_format = decimal[0]\n",
    "        \n",
    "        display_name = \"Open in Google Maps\"\n",
    "        google_map_string = 'https://maps.google.com/?q='\n",
    "        google_map_string += str(centroids_df.loc[muid,'Y']) + ', '\n",
    "        google_map_string += str(centroids_df.loc[muid,'X']) \n",
    "\n",
    "        # Adding a hyperlink with a display name\n",
    "        cell = sheet1.cell(row=10, column=3, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "                      \n",
    "        sheet = workbook[\"Map\"]\n",
    "        \n",
    "        #Add source info\n",
    "        source_infos = []\n",
    "        source_infos.append(['Created by:',username])\n",
    "        source_infos.append(['Date:',date_created])\n",
    "        source_infos.append(['Model area:',model])\n",
    "        source_infos.append(['Model version:',model_version])\n",
    "        source_infos.append(['Population file:',os.path.basename(pop_book)])\n",
    "        source_infos.append(['Population sheet:',pop_sheet ])\n",
    "        \n",
    "        for i, source_info in enumerate(source_infos):\n",
    "            sheet1[f'H{i+2}']  = source_info[0]\n",
    "            sheet1[f'J{i+2}']  = source_info[1]\n",
    "        \n",
    "        for row in sheet1['H2:H9']:\n",
    "            for cell in row:\n",
    "                cell.font = Font(bold=True)\n",
    "        \n",
    "\n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        sheet.add_image(img, 'B3')\n",
    "        \n",
    "        cell = sheet.cell(row=1, column=2, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "        shutil.copy(sheetpath,backup_folder + \"\\\\\" + id + \".xlsx\")\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MessageBox(None,b'All cells ran successfully.', b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
