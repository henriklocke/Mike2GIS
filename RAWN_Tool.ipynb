{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "# User Input, to move to separate sheet so no permanent cell\n",
    "#stop trace if more pipes than max_steps traced from catchment, must be an endless loop. \n",
    "max_steps = 1000 \n",
    "\n",
    "update_field_in_model = True\n",
    "update_field = 'Description'\n",
    "\n",
    "output_folder = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output\"\n",
    "model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "sewer_area = 'NSSA'\n",
    "pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "pop_sheet = 'MPF Update 4'\n",
    "model = 'NSSA'\n",
    "gdb_name = 'RAWN.gdb'\n",
    "gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "#Options to skip time consuming steps during debug, must be True during production runs\n",
    "run_dissolve = False\n",
    "run_dissolve_append = False\n",
    "run_jpg = True\n",
    "run_import = False\n",
    "run_html = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "#Set up column names\n",
    "\n",
    "years = [2060,2070,2080,2090,2100]\n",
    "categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "mpf_col_dict = {}\n",
    "\n",
    "area_col_dict = {}\n",
    "area_col_dict['res'] = 'Area_Res'\n",
    "area_col_dict['com'] = 'Area_Com'\n",
    "area_col_dict['ind'] = 'Area_Ind'\n",
    "area_col_dict['inst'] = 'Area_Inst'\n",
    "area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "per_unit_dict = {}\n",
    "per_unit_dict['res'] = 320\n",
    "per_unit_dict['com'] = 33700 \n",
    "per_unit_dict['ind'] = 56200\n",
    "per_unit_dict['inst'] = 33700\n",
    "per_unit_dict['infl'] = 5600\n",
    "per_unit_dict['infi'] = 5600\n",
    "\n",
    "unit_dict = {}\n",
    "unit_dict['res'] = 'L/c/d'\n",
    "unit_dict['com'] = 'L/ha/d'\n",
    "unit_dict['ind'] = 'L/ha/d'\n",
    "unit_dict['inst'] = 'L/ha/d'\n",
    "unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "header_dict = {}\n",
    "# header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "avg_calc_dict = {}\n",
    "avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)']\n",
    "avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)']\n",
    "\n",
    "header_tuples = []\n",
    "for header in header_dict:\n",
    "    for sub_header in (header_dict[header][1]):\n",
    "        header_tuples.append((header_dict[header][0],sub_header))\n",
    "header_tuples\n",
    "\n",
    "# columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "info_list = []\n",
    "for item in unit_dict:\n",
    "    info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "info_df.set_index('DESCRIPTION',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "#Import population\n",
    "pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "pop_sum_total_col = int(pop_df.Population.sum())\n",
    "pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "if pop_sum_total_col != pop_sum_sub_cols:\n",
    "      raise ValueError(\"Error. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "#Import model data\n",
    "\n",
    "node_types = {}\n",
    "node_types[1] = 'Manhole'\n",
    "node_types[2] = 'Basin'\n",
    "node_types[3] = 'Outlet'\n",
    "node_types[4] = 'Junction'\n",
    "node_types[5] = 'Soakaway'\n",
    "node_types[6] = 'River Junction'\n",
    "\n",
    "sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "lines = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "orifices = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,orifices])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "valves = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,valves])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "weirs = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,weirs])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "pumps = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,pumps])\n",
    "\n",
    "lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "\n",
    "sql = \"SELECT muid, acronym, assetname FROM msm_Node WHERE active = 1\"\n",
    "node_id_df = sql_to_df(sql,model_path)\n",
    "node_id_df = node_id_df[(node_id_df.assetname.str[:2]=='MH') & (node_id_df.assetname.str.len() > 2) & (node_id_df.acronym.notna())]\n",
    "node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "node_id_df = node_id_df[['Node','ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "accumulated_catchment_set = set()\n",
    "accumulated_node_set = set()\n",
    "\n",
    "for index1, row1 in catchments.iterrows():\n",
    "    catchment = row1['Catchment']\n",
    "    nodes = [row1['Connected_Node']]\n",
    "    start_node = row1['Connected_Node']\n",
    "    steps = 0\n",
    "    \n",
    "    accumulated_catchment_set.add((start_node,catchment))\n",
    "        \n",
    "    while steps <= max_steps:\n",
    "        steps += 1\n",
    "        downstream_df = lines[lines['From'].isin(nodes)]\n",
    "\n",
    "        if len(downstream_df) > 0:\n",
    "            nodes = list(downstream_df.To.unique())\n",
    "\n",
    "            nodes = [node for node in nodes if len(node)>0]\n",
    "            for node in nodes:\n",
    "                accumulated_catchment_set.add((node,catchment))       \n",
    "        else:\n",
    "            break\n",
    "        if steps == max_steps:\n",
    "            raise ValueError(\"Maximum steps were reached, indicating a loop. Start catchment is '\" + catchment + \"'\")\n",
    "           \n",
    "        accumulated_catchment_set.add((node,catchment))\n",
    "        \n",
    "accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "data = {\n",
    "    ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "    ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "    ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "}\n",
    "\n",
    "# Create a DataFrame with MultiIndex columns\n",
    "accumulation_df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "catchment_df = df_template.copy()\n",
    "for catchment in catchments:\n",
    "    for year in years:\n",
    "        key = model + '@' + catchment + '@' + str(year)\n",
    "        catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'UNKNOWN'\n",
    "        catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "        catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "        catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "        for area_col_dict_key in area_col_dict:\n",
    "            catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "        catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "        san_flow = 0\n",
    "        adwf = 0\n",
    "        for avg_calc_dict_key in avg_calc_dict:\n",
    "            input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "            input2 = per_unit_dict[avg_calc_dict_key]\n",
    "            avg_flow = input1 * input2 / 86400\n",
    "            adwf += avg_flow\n",
    "            if avg_calc_dict_key not in ['infl','infi']:\n",
    "                san_flow += avg_flow\n",
    "            catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "        catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "        catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "        \n",
    "catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "node_df = catchment_node_df.copy()\n",
    "node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "node_df.reset_index(inplace=True)\n",
    "node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]*0.8\n",
    "node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = (\n",
    "    0.8 * (1 + 14 / (4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5)) *\n",
    "    np.where(\n",
    "        node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "        1.7,\n",
    "        2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    ")\n",
    "\n",
    "node_df[('FLOWS','PWWF (L/s)')] = (\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    ")\n",
    "\n",
    "# excel_folder = output_folder + '\\\\Excel'\n",
    "# if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "# for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "#     node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id]\n",
    "#     id = id.replace('/','-') if '/' in id else id\n",
    "#     node_single_df.to_excel(excel_folder + '\\\\' + id + '.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 9\n",
    "#Import GIS from the model\n",
    "if run_import:\n",
    "\n",
    "    out_path = output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "    #     arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(layer):\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:    \n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer)\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catchment_Count</th>\n",
       "      <th>Catchments</th>\n",
       "      <th>Merge_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Node</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>1</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>Merge_ID_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>1</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>Merge_ID_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7310</th>\n",
       "      <td>1</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>Merge_ID_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>1</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>Merge_ID_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>1</td>\n",
       "      <td>(2122,)</td>\n",
       "      <td>Merge_ID_98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9766</th>\n",
       "      <td>104</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>Merge_ID_87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9764</th>\n",
       "      <td>104</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>Merge_ID_87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9763</th>\n",
       "      <td>104</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>Merge_ID_87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9765</th>\n",
       "      <td>104</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>Merge_ID_87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9632</th>\n",
       "      <td>105</td>\n",
       "      <td>(10108, 10109, 10110, 2059, 2060, 2061, 2062, ...</td>\n",
       "      <td>Merge_ID_88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Catchment_Count  ...     Merge_ID\n",
       "Node                   ...             \n",
       "7311                1  ...  Merge_ID_70\n",
       "7312                1  ...  Merge_ID_70\n",
       "7310                1  ...  Merge_ID_70\n",
       "7112                1  ...  Merge_ID_70\n",
       "6786                1  ...  Merge_ID_98\n",
       "...               ...  ...          ...\n",
       "9766              104  ...  Merge_ID_87\n",
       "9764              104  ...  Merge_ID_87\n",
       "9763              104  ...  Merge_ID_87\n",
       "9765              104  ...  Merge_ID_87\n",
       "9632              105  ...  Merge_ID_88\n",
       "\n",
       "[469 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Permanent cell 10\n",
    "#Create merge_df,minimizing the number of computation heavy dissolves that need to be done.\n",
    "\n",
    "merge_set = set()\n",
    "\n",
    "rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "# rank_df.reset_index(inplace=True)\n",
    "rank_df.columns = ['Catchment_Count']\n",
    "max_catchments = max(rank_df.Catchment_Count)\n",
    "rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "# rank_df.reset_index(inplace=True)\n",
    "\n",
    "catchment_list = []\n",
    "merge_set = set()\n",
    "for index, row in rank_df.iterrows():\n",
    "\n",
    "    catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "    catchments = tuple(sorted(catchments))\n",
    "#     rank_df.loc[index,'Catchments'] = catchments\n",
    "    catchment_list.append(catchments)\n",
    "    merge_set.add(catchments)\n",
    "    \n",
    "    \n",
    "rank_df['Catchments'] = catchment_list\n",
    "rank_df['Node'] = rank_df.index\n",
    "print(len(merge_set))\n",
    "\n",
    "merge_list = []\n",
    "for i, catchments in enumerate(merge_set):\n",
    "    merge_id = 'Merge_ID_' + str(i)\n",
    "    merge_list.append([merge_id,catchments])\n",
    "    \n",
    "merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "simpler_merge = []\n",
    "for index1, row1 in merge_df.iterrows():\n",
    "    catchments1 = list(row1['Catchments'])\n",
    "    for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "        catchments2 = row2['Catchments']\n",
    "        \n",
    "        if len(catchments1) >= len(catchments2):\n",
    "            if all(item in catchments1 for item in catchments2):\n",
    "                catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                catchments1.append(row2['Merge_ID'])\n",
    "    simpler_merge.append(catchments1)\n",
    "               \n",
    "merge_df['To_Dissolve'] = simpler_merge\n",
    "merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "print(max_catchments)\n",
    "\n",
    "# for index1, row1 in merge_df.iterrows():\n",
    "#     print\n",
    "\n",
    "rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_70, 0 of 102 at time 2024-05-30 15:32:32.724306\n",
      "Dissolving for Merge_ID_98, 1 of 102 at time 2024-05-30 15:32:35.875191\n",
      "Dissolving for Merge_ID_38, 2 of 102 at time 2024-05-30 15:32:39.108152\n",
      "Dissolving for Merge_ID_67, 3 of 102 at time 2024-05-30 15:32:42.195980\n",
      "Dissolving for Merge_ID_35, 4 of 102 at time 2024-05-30 15:33:09.240746\n",
      "Dissolving for Merge_ID_83, 5 of 102 at time 2024-05-30 15:33:34.090502\n",
      "Dissolving for Merge_ID_71, 6 of 102 at time 2024-05-30 15:33:50.931924\n",
      "Dissolving for Merge_ID_12, 7 of 102 at time 2024-05-30 15:34:03.182142\n",
      "Dissolving for Merge_ID_76, 8 of 102 at time 2024-05-30 15:34:15.740643\n",
      "Dissolving for Merge_ID_97, 9 of 102 at time 2024-05-30 15:34:28.495837\n",
      "Dissolving for Merge_ID_61, 10 of 102 at time 2024-05-30 15:35:18.761868\n",
      "Dissolving for Merge_ID_6, 11 of 102 at time 2024-05-30 15:35:32.365325\n",
      "Dissolving for Merge_ID_19, 12 of 102 at time 2024-05-30 15:35:59.471147\n",
      "Dissolving for Merge_ID_1, 13 of 102 at time 2024-05-30 15:36:12.142761\n",
      "Dissolving for Merge_ID_89, 14 of 102 at time 2024-05-30 15:36:29.077268\n",
      "Dissolving for Merge_ID_13, 15 of 102 at time 2024-05-30 15:36:46.445173\n",
      "Dissolving for Merge_ID_68, 16 of 102 at time 2024-05-30 15:37:06.621649\n",
      "Dissolving for Merge_ID_55, 17 of 102 at time 2024-05-30 15:37:27.023350\n",
      "Dissolving for Merge_ID_11, 18 of 102 at time 2024-05-30 15:37:47.337951\n",
      "Dissolving for Merge_ID_95, 19 of 102 at time 2024-05-30 15:37:59.982527\n",
      "Dissolving for Merge_ID_64, 20 of 102 at time 2024-05-30 15:38:20.221057\n",
      "Dissolving for Merge_ID_77, 21 of 102 at time 2024-05-30 15:38:44.772043\n",
      "Dissolving for Merge_ID_75, 22 of 102 at time 2024-05-30 15:39:32.096370\n",
      "Dissolving for Merge_ID_74, 23 of 102 at time 2024-05-30 15:40:23.000976\n",
      "Dissolving for Merge_ID_51, 24 of 102 at time 2024-05-30 15:40:40.086618\n",
      "Dissolving for Merge_ID_96, 25 of 102 at time 2024-05-30 15:41:00.808590\n",
      "Dissolving for Merge_ID_27, 26 of 102 at time 2024-05-30 15:41:21.043116\n",
      "Dissolving for Merge_ID_10, 27 of 102 at time 2024-05-30 15:41:38.544139\n",
      "Dissolving for Merge_ID_42, 28 of 102 at time 2024-05-30 15:42:37.748846\n",
      "Dissolving for Merge_ID_91, 29 of 102 at time 2024-05-30 15:42:59.139428\n",
      "Dissolving for Merge_ID_79, 30 of 102 at time 2024-05-30 15:44:10.588837\n",
      "Dissolving for Merge_ID_100, 31 of 102 at time 2024-05-30 15:44:52.408640\n",
      "Dissolving for Merge_ID_14, 32 of 102 at time 2024-05-30 15:45:38.577906\n",
      "Dissolving for Merge_ID_33, 33 of 102 at time 2024-05-30 15:46:07.510392\n",
      "Dissolving for Merge_ID_49, 34 of 102 at time 2024-05-30 15:46:52.647720\n",
      "Dissolving for Merge_ID_73, 35 of 102 at time 2024-05-30 15:47:33.860448\n",
      "Dissolving for Merge_ID_90, 36 of 102 at time 2024-05-30 15:47:58.956417\n",
      "Dissolving for Merge_ID_63, 37 of 102 at time 2024-05-30 15:48:27.860374\n",
      "Dissolving for Merge_ID_52, 38 of 102 at time 2024-05-30 15:48:45.129179\n",
      "Dissolving for Merge_ID_93, 39 of 102 at time 2024-05-30 15:49:06.932134\n",
      "Dissolving for Merge_ID_59, 40 of 102 at time 2024-05-30 15:49:40.194091\n",
      "Dissolving for Merge_ID_84, 41 of 102 at time 2024-05-30 15:49:53.656413\n",
      "Dissolving for Merge_ID_18, 42 of 102 at time 2024-05-30 15:50:29.549767\n",
      "Dissolving for Merge_ID_16, 43 of 102 at time 2024-05-30 15:50:59.107820\n",
      "Dissolving for Merge_ID_56, 44 of 102 at time 2024-05-30 15:51:12.934475\n",
      "Dissolving for Merge_ID_24, 45 of 102 at time 2024-05-30 15:51:35.707318\n",
      "Dissolving for Merge_ID_66, 46 of 102 at time 2024-05-30 15:52:01.002469\n",
      "Dissolving for Merge_ID_86, 47 of 102 at time 2024-05-30 15:52:26.406222\n",
      "Dissolving for Merge_ID_69, 48 of 102 at time 2024-05-30 15:52:44.138451\n",
      "Dissolving for Merge_ID_57, 49 of 102 at time 2024-05-30 15:54:01.076861\n",
      "Dissolving for Merge_ID_41, 50 of 102 at time 2024-05-30 15:54:21.862884\n",
      "Dissolving for Merge_ID_37, 51 of 102 at time 2024-05-30 15:54:39.585605\n",
      "Dissolving for Merge_ID_45, 52 of 102 at time 2024-05-30 15:54:58.423845\n",
      "Dissolving for Merge_ID_72, 53 of 102 at time 2024-05-30 15:55:23.945201\n",
      "Dissolving for Merge_ID_102, 54 of 102 at time 2024-05-30 15:55:41.999723\n",
      "Dissolving for Merge_ID_39, 55 of 102 at time 2024-05-30 15:55:55.863411\n",
      "Dissolving for Merge_ID_81, 56 of 102 at time 2024-05-30 15:56:10.889161\n",
      "Dissolving for Merge_ID_2, 57 of 102 at time 2024-05-30 15:56:40.527788\n",
      "Dissolving for Merge_ID_46, 58 of 102 at time 2024-05-30 15:57:06.449510\n",
      "Dissolving for Merge_ID_58, 59 of 102 at time 2024-05-30 15:57:39.774006\n",
      "Dissolving for Merge_ID_92, 60 of 102 at time 2024-05-30 15:57:53.816854\n",
      "Dissolving for Merge_ID_8, 61 of 102 at time 2024-05-30 15:58:11.773284\n",
      "Dissolving for Merge_ID_4, 62 of 102 at time 2024-05-30 15:59:05.584696\n",
      "Dissolving for Merge_ID_101, 63 of 102 at time 2024-05-30 15:59:24.587083\n",
      "Dissolving for Merge_ID_20, 64 of 102 at time 2024-05-30 15:59:50.524387\n",
      "Dissolving for Merge_ID_26, 65 of 102 at time 2024-05-30 16:00:12.431431\n",
      "Dissolving for Merge_ID_53, 66 of 102 at time 2024-05-30 16:00:30.670118\n",
      "Dissolving for Merge_ID_34, 67 of 102 at time 2024-05-30 16:00:48.900798\n",
      "Dissolving for Merge_ID_23, 68 of 102 at time 2024-05-30 16:01:14.543260\n",
      "Dissolving for Merge_ID_62, 69 of 102 at time 2024-05-30 16:01:28.619138\n",
      "Dissolving for Merge_ID_65, 70 of 102 at time 2024-05-30 16:01:50.858487\n",
      "Dissolving for Merge_ID_22, 71 of 102 at time 2024-05-30 16:02:05.266669\n",
      "Dissolving for Merge_ID_17, 72 of 102 at time 2024-05-30 16:02:19.380583\n",
      "Dissolving for Merge_ID_15, 73 of 102 at time 2024-05-30 16:02:33.342357\n",
      "Dissolving for Merge_ID_31, 74 of 102 at time 2024-05-30 16:02:48.211961\n",
      "Dissolving for Merge_ID_78, 75 of 102 at time 2024-05-30 16:03:04.841174\n",
      "Dissolving for Merge_ID_40, 76 of 102 at time 2024-05-30 16:03:23.522767\n",
      "Dissolving for Merge_ID_30, 77 of 102 at time 2024-05-30 16:03:37.662704\n",
      "Dissolving for Merge_ID_43, 78 of 102 at time 2024-05-30 16:04:11.530688\n",
      "Dissolving for Merge_ID_54, 79 of 102 at time 2024-05-30 16:05:10.157837\n",
      "Dissolving for Merge_ID_50, 80 of 102 at time 2024-05-30 16:05:28.742839\n",
      "Dissolving for Merge_ID_25, 81 of 102 at time 2024-05-30 16:06:28.148186\n",
      "Dissolving for Merge_ID_28, 82 of 102 at time 2024-05-30 16:08:03.913647\n",
      "Dissolving for Merge_ID_0, 83 of 102 at time 2024-05-30 16:08:30.456927\n",
      "Dissolving for Merge_ID_60, 84 of 102 at time 2024-05-30 16:09:01.108966\n",
      "Dissolving for Merge_ID_44, 85 of 102 at time 2024-05-30 16:09:28.207262\n",
      "Dissolving for Merge_ID_82, 86 of 102 at time 2024-05-30 16:10:39.664639\n",
      "Dissolving for Merge_ID_80, 87 of 102 at time 2024-05-30 16:11:06.797459\n",
      "Dissolving for Merge_ID_94, 88 of 102 at time 2024-05-30 16:11:34.113446\n",
      "Dissolving for Merge_ID_99, 89 of 102 at time 2024-05-30 16:12:05.569220\n",
      "Dissolving for Merge_ID_5, 90 of 102 at time 2024-05-30 16:13:26.096272\n",
      "Dissolving for Merge_ID_21, 91 of 102 at time 2024-05-30 16:14:10.291691\n",
      "Dissolving for Merge_ID_47, 92 of 102 at time 2024-05-30 16:14:59.490609\n",
      "Dissolving for Merge_ID_9, 93 of 102 at time 2024-05-30 16:16:09.497634\n",
      "Dissolving for Merge_ID_36, 94 of 102 at time 2024-05-30 16:16:24.487342\n",
      "Dissolving for Merge_ID_85, 95 of 102 at time 2024-05-30 16:16:44.381005\n",
      "Dissolving for Merge_ID_7, 96 of 102 at time 2024-05-30 16:17:08.744286\n",
      "Dissolving for Merge_ID_48, 97 of 102 at time 2024-05-30 16:17:24.505701\n",
      "Dissolving for Merge_ID_32, 98 of 102 at time 2024-05-30 16:18:00.815911\n",
      "Dissolving for Merge_ID_3, 99 of 102 at time 2024-05-30 16:18:28.516242\n",
      "Dissolving for Merge_ID_29, 100 of 102 at time 2024-05-30 16:18:53.439523\n",
      "Dissolving for Merge_ID_87, 101 of 102 at time 2024-05-30 16:19:22.086719\n",
      "Dissolving for Merge_ID_88, 102 of 102 at time 2024-05-30 16:19:50.275011\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "#Run dissolve\n",
    "out_path = output_folder + '\\\\' + gdb_name\n",
    "arcpy.env.workspace = out_path  \n",
    "arcpy.env.addOutputsToMap = False\n",
    "if run_dissolve:\n",
    "    if arcpy.Exists(gdb_name_dissolve):\n",
    "        arcpy.management.Delete(gdb_path)\n",
    "    arcpy.management.CreateFileGDB(output_folder, gdb_name_dissolve)\n",
    "    dissolve_path = output_folder + '\\\\' + gdb_name_dissolve\n",
    "    arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "#     arcpy.env.workspace = dissolve_path\n",
    "    arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "    arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "    arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "    arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "    for index, row in merge_df.iterrows():\n",
    "        arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "        print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "        if row['Catchment_Count'] == 1:\n",
    "            arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "            where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "            arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "            arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "            # Clean up by removing the feature layer\n",
    "            arcpy.management.Delete(\"temp_layer\")\n",
    "        else:\n",
    "            arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "            catchments = row['To_Dissolve']\n",
    "            catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "            where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "            arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "            arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "            arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "            arcpy.management.Delete(\"temp_layer\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "            rank_df.head(1)\n",
    "\n",
    "            nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "            for node in nodes:\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "            \n",
    "            arcpy.management.Delete(\"temp_layer\")\n",
    "            arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "            \n",
    "\n",
    "# # Use CalculateField to update the Drains_To field\n",
    "# arcpy.management.CalculateField(\"catchment_layer\", \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "\n",
    "#Delete the features without a Drains_To\n",
    "arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "where_clause = f\"Drains_To IS NULL\"\n",
    "arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "#Append the features into the official Node_Catchment layer\n",
    "arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "          \n",
    "print('Done')            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7311\n",
      "7312\n",
      "7310\n",
      "7112\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "9670\n",
      "9671\n",
      "6791\n",
      "6792\n",
      "6794\n",
      "6795\n",
      "6793\n",
      "9746\n",
      "9747\n",
      "9744\n",
      "9745\n",
      "9673\n",
      "9672\n",
      "7313\n",
      "9942\n",
      "7314\n",
      "7318\n",
      "7319\n",
      "7479\n",
      "7480\n",
      "7481\n",
      "7317\n",
      "7316\n",
      "7315\n",
      "7114\n",
      "7113\n",
      "7189\n",
      "7190\n",
      "7141\n",
      "7140\n",
      "7139\n",
      "7142\n",
      "7143\n",
      "9783\n",
      "9757\n",
      "7191\n",
      "9756\n",
      "9613\n",
      "9614\n",
      "7144\n",
      "7145\n",
      "7146\n",
      "7147\n",
      "7148\n",
      "9483\n",
      "10124\n",
      "9758\n",
      "9728\n",
      "9727\n",
      "9726\n",
      "9615\n",
      "7149\n",
      "7150\n",
      "7151\n",
      "7152\n",
      "9748\n",
      "9606\n",
      "9605\n",
      "9604\n",
      "9603\n",
      "9602\n",
      "9601\n",
      "9600\n",
      "9599\n",
      "9598\n",
      "9729\n",
      "9730\n",
      "9485\n",
      "9486\n",
      "9484\n",
      "9617\n",
      "9616\n",
      "9984\n",
      "7154\n",
      "7155\n",
      "7156\n",
      "7157\n",
      "7158\n",
      "7159\n",
      "7160\n",
      "7161\n",
      "7162\n",
      "7153\n",
      "9805\n",
      "9739\n",
      "9731\n",
      "9732\n",
      "9733\n",
      "9734\n",
      "9735\n",
      "9736\n",
      "9738\n",
      "9737\n",
      "9789\n",
      "9777\n",
      "9742\n",
      "9741\n",
      "9740\n",
      "9778\n",
      "9954\n",
      "9755\n",
      "9753\n",
      "9754\n",
      "9607\n",
      "9609\n",
      "9610\n",
      "9611\n",
      "9752\n",
      "9608\n",
      "9750\n",
      "9751\n",
      "9749\n",
      "9612\n",
      "9489\n",
      "9487\n",
      "9488\n",
      "7163\n",
      "7164\n",
      "9792\n",
      "9790\n",
      "9804\n",
      "10004\n",
      "10003\n",
      "9787\n",
      "9780\n",
      "9779\n",
      "9781\n",
      "7115\n",
      "9493\n",
      "9492\n",
      "9491\n",
      "9490\n",
      "7116\n",
      "7117\n",
      "7167\n",
      "7165\n",
      "7166\n",
      "7168\n",
      "7169\n",
      "10482\n",
      "10484\n",
      "10497\n",
      "10499\n",
      "10501\n",
      "10504\n",
      "10498\n",
      "9550\n",
      "9760\n",
      "9759\n",
      "9494\n",
      "9495\n",
      "9496\n",
      "9497\n",
      "9498\n",
      "9500\n",
      "9499\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7171\n",
      "7170\n",
      "9551\n",
      "9554\n",
      "9553\n",
      "9552\n",
      "7176\n",
      "7175\n",
      "9556\n",
      "9555\n",
      "9557\n",
      "7119\n",
      "6496\n",
      "9501\n",
      "9502\n",
      "9503\n",
      "10020\n",
      "7177\n",
      "9563\n",
      "9562\n",
      "9561\n",
      "9560\n",
      "9559\n",
      "9558\n",
      "9786\n",
      "9784\n",
      "9785\n",
      "10001\n",
      "10002\n",
      "9504\n",
      "7181\n",
      "7178\n",
      "7179\n",
      "7180\n",
      "9507\n",
      "9506\n",
      "9505\n",
      "5015\n",
      "7485\n",
      "7182\n",
      "5016\n",
      "9521\n",
      "9517\n",
      "9512\n",
      "9513\n",
      "9514\n",
      "9743\n",
      "9515\n",
      "9516\n",
      "9518\n",
      "9519\n",
      "9511\n",
      "7122\n",
      "9520\n",
      "9508\n",
      "9510\n",
      "9509\n",
      "7121\n",
      "9634\n",
      "9633\n",
      "9635\n",
      "9618\n",
      "9619\n",
      "9620\n",
      "9621\n",
      "9638\n",
      "9636\n",
      "9637\n",
      "9639\n",
      "9523\n",
      "9522\n",
      "9640\n",
      "9645\n",
      "9644\n",
      "9642\n",
      "9641\n",
      "9643\n",
      "9761\n",
      "9526\n",
      "9525\n",
      "9524\n",
      "9762\n",
      "9530\n",
      "9529\n",
      "9528\n",
      "9527\n",
      "5007\n",
      "7123\n",
      "9646\n",
      "9648\n",
      "9647\n",
      "7125\n",
      "7126\n",
      "9532\n",
      "9533\n",
      "9531\n",
      "9535\n",
      "9534\n",
      "9537\n",
      "9538\n",
      "9536\n",
      "7124\n",
      "9652\n",
      "9649\n",
      "9650\n",
      "9651\n",
      "9539\n",
      "9540\n",
      "9655\n",
      "9653\n",
      "9654\n",
      "9623\n",
      "9622\n",
      "9541\n",
      "9542\n",
      "9656\n",
      "9543\n",
      "9544\n",
      "7482\n",
      "7483\n",
      "9659\n",
      "9658\n",
      "9657\n",
      "9660\n",
      "7484\n",
      "9661\n",
      "9545\n",
      "9662\n",
      "9663\n",
      "9547\n",
      "9546\n",
      "7135\n",
      "5010\n",
      "9549\n",
      "7136\n",
      "7137\n",
      "9548\n",
      "9668\n",
      "9667\n",
      "9666\n",
      "9665\n",
      "9664\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7488\n",
      "7489\n",
      "7490\n",
      "7491\n",
      "9674\n",
      "9669\n",
      "9908\n",
      "9681\n",
      "9680\n",
      "9679\n",
      "9682\n",
      "9890\n",
      "9678\n",
      "9683\n",
      "9676\n",
      "9675\n",
      "9677\n",
      "9898\n",
      "9897\n",
      "9901\n",
      "9896\n",
      "9895\n",
      "9894\n",
      "9893\n",
      "9892\n",
      "9891\n",
      "10172\n",
      "9902\n",
      "9903\n",
      "9899\n",
      "9904\n",
      "9900\n",
      "9907\n",
      "9906\n",
      "9905\n",
      "9685\n",
      "9686\n",
      "9687\n",
      "7492\n",
      "9770\n"
     ]
    }
   ],
   "source": [
    "# merge_ids = set()\n",
    "# for index, row in rank_df.iterrows():\n",
    "#     print(index)\n",
    "#     merge_id = row['Merge_ID']\n",
    "#     arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "#     where_clause = f\"Merge_ID = '{merge_id}'\"\n",
    "#     arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "    \n",
    "#     if merge_id in merge_ids:\n",
    "#         arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", dissolve_path, 'Dissolve_Temp')\n",
    "#         arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f'\"{index}\"', \"PYTHON3\")\n",
    "#         arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "#         arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "#     else:\n",
    "#         arcpy.management.CalculateField(\"temp_layer\", \"Drains_To\", f'\"{index}\"', \"PYTHON3\")\n",
    "        \n",
    "#     arcpy.management.Delete(\"temp_layer\")\n",
    "#     merge_ids.add(merge_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Friday, May 31, 2024 8:13:00 AM\",\"Succeeded at Friday, May 31, 2024 8:13:13 AM (Elapsed Time: 13.46 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'true'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "In  \u001b[0;34m[2]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     merge_df.head(\u001b[34m5\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merge_df' is not defined\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "merge_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obsolete cell\n",
    "# #Dissolve catchments\n",
    "# arcpy.env.addOutputsToMap = False\n",
    "# if run_dissolve:\n",
    "#     arcpy.management.CreateFileGDB(output_folder, gdb_name_dissolve)\n",
    "#     dissolve_path = output_folder + '\\\\' + gdb_name_dissolve\n",
    "#     arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'msm_Catchment')\n",
    "#     nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "#     for i, node in enumerate(nodes):\n",
    "#         print('Dissolving for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "#         catchment_df = accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node]\n",
    "#         catchments = list(catchment_df[('GENERAL INFO','CATCHMENT')].unique())\n",
    "#         arcpy.management.CalculateField(dissolve_path + '\\\\msm_Catchment', \"Drains_To\", \"''\", \"PYTHON3\")\n",
    "#         with arcpy.da.UpdateCursor(dissolve_path + '\\\\msm_catchment', ['muid', 'Drains_To']) as cursor:\n",
    "#             for row in cursor:\n",
    "#                 if row[0] in catchments:\n",
    "#                     row[1] = node\n",
    "#                     cursor.updateRow(row)\n",
    "\n",
    "#         query = \"Drains_To = 'Test'\"\n",
    "#         arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\msm_catchment', \"temp_layer\", \"Drains_To = '\" + node + \"'\")\n",
    "#         dissolve_output = dissolve_path + '\\\\msm_Catchment_Dissolve_Single'\n",
    "#         arcpy.management.Dissolve(\"temp_layer\", dissolve_output, \"Drains_To\", \"\", \"MULTI_PART\")\n",
    "#         arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "#         arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\msm_Catchment_Dissolve_Single', dissolve_path, 'Node_Catchment_' + node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obsolete cell\n",
    "#Append individual dissolved catchments to one layer.\n",
    "\n",
    "# if run_dissolve_append:\n",
    "#     nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "#     for i, node in enumerate(nodes):    \n",
    "#         print('Appending for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "#         if i == 0:\n",
    "#             arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\Node_Catchment_' + node, out_path, 'Node_Catchment')\n",
    "#         else:\n",
    "#             arcpy.management.Append(dissolve_path + '\\\\Node_Catchment_' + node, \"Node_Catchment\", \"NO_TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 1 of 458 at time 2024-05-31 14:28:45.950136\n",
      "Printing jpg 2 of 458 at time 2024-05-31 14:28:48.131127\n",
      "Printing jpg 3 of 458 at time 2024-05-31 14:28:50.248059\n",
      "Printing jpg 4 of 458 at time 2024-05-31 14:28:52.531143\n",
      "Printing jpg 5 of 458 at time 2024-05-31 14:28:54.714136\n",
      "Printing jpg 6 of 458 at time 2024-05-31 14:28:57.064281\n",
      "Printing jpg 7 of 458 at time 2024-05-31 14:28:59.914883\n",
      "Printing jpg 8 of 458 at time 2024-05-31 14:29:02.147922\n",
      "Printing jpg 9 of 458 at time 2024-05-31 14:29:04.347930\n",
      "Printing jpg 10 of 458 at time 2024-05-31 14:29:06.818185\n",
      "Printing jpg 11 of 458 at time 2024-05-31 14:29:09.186852\n",
      "Printing jpg 12 of 458 at time 2024-05-31 14:29:11.483949\n",
      "Printing jpg 13 of 458 at time 2024-05-31 14:29:13.833093\n",
      "Printing jpg 14 of 458 at time 2024-05-31 14:29:16.415451\n",
      "Printing jpg 15 of 458 at time 2024-05-31 14:29:18.631474\n",
      "Printing jpg 16 of 458 at time 2024-05-31 14:29:20.826477\n",
      "Printing jpg 17 of 458 at time 2024-05-31 14:29:22.947413\n",
      "Printing jpg 18 of 458 at time 2024-05-31 14:29:25.185456\n",
      "Printing jpg 19 of 458 at time 2024-05-31 14:29:27.632690\n",
      "Printing jpg 20 of 458 at time 2024-05-31 14:29:30.195042\n",
      "Printing jpg 21 of 458 at time 2024-05-31 14:29:32.294959\n",
      "Printing jpg 22 of 458 at time 2024-05-31 14:29:34.566032\n",
      "Printing jpg 23 of 458 at time 2024-05-31 14:29:36.663947\n",
      "Printing jpg 24 of 458 at time 2024-05-31 14:29:39.095167\n",
      "Printing jpg 25 of 458 at time 2024-05-31 14:29:42.026843\n",
      "Printing jpg 26 of 458 at time 2024-05-31 14:29:44.847418\n",
      "Printing jpg 27 of 458 at time 2024-05-31 14:29:47.685847\n",
      "Printing jpg 28 of 458 at time 2024-05-31 14:29:50.931811\n",
      "Printing jpg 29 of 458 at time 2024-05-31 14:29:53.731366\n",
      "Printing jpg 30 of 458 at time 2024-05-31 14:29:56.247663\n",
      "Printing jpg 31 of 458 at time 2024-05-31 14:29:58.995171\n",
      "Printing jpg 32 of 458 at time 2024-05-31 14:30:01.547501\n",
      "Printing jpg 33 of 458 at time 2024-05-31 14:30:03.548327\n",
      "Printing jpg 34 of 458 at time 2024-05-31 14:30:06.063624\n",
      "Printing jpg 35 of 458 at time 2024-05-31 14:30:08.147526\n",
      "Printing jpg 36 of 458 at time 2024-05-31 14:30:10.625788\n",
      "Printing jpg 37 of 458 at time 2024-05-31 14:30:13.016971\n",
      "Printing jpg 38 of 458 at time 2024-05-31 14:30:15.147916\n",
      "Printing jpg 39 of 458 at time 2024-05-31 14:30:18.031548\n",
      "Printing jpg 40 of 458 at time 2024-05-31 14:30:20.731013\n",
      "Printing jpg 41 of 458 at time 2024-05-31 14:30:23.231295\n",
      "Printing jpg 42 of 458 at time 2024-05-31 14:30:25.762606\n",
      "Printing jpg 43 of 458 at time 2024-05-31 14:30:28.296919\n",
      "Printing jpg 44 of 458 at time 2024-05-31 14:30:30.842243\n",
      "Printing jpg 45 of 458 at time 2024-05-31 14:30:33.563727\n",
      "Printing jpg 46 of 458 at time 2024-05-31 14:30:35.830797\n",
      "Printing jpg 47 of 458 at time 2024-05-31 14:30:38.319068\n",
      "Printing jpg 48 of 458 at time 2024-05-31 14:30:40.594145\n",
      "Printing jpg 49 of 458 at time 2024-05-31 14:30:43.131461\n",
      "Printing jpg 50 of 458 at time 2024-05-31 14:30:45.132287\n",
      "Printing jpg 51 of 458 at time 2024-05-31 14:30:46.963959\n",
      "Printing jpg 52 of 458 at time 2024-05-31 14:30:48.914740\n",
      "Printing jpg 53 of 458 at time 2024-05-31 14:30:50.795457\n",
      "Printing jpg 54 of 458 at time 2024-05-31 14:30:53.015484\n",
      "Printing jpg 55 of 458 at time 2024-05-31 14:30:54.963262\n",
      "Printing jpg 56 of 458 at time 2024-05-31 14:30:57.014134\n",
      "Printing jpg 57 of 458 at time 2024-05-31 14:30:59.214142\n",
      "Printing jpg 58 of 458 at time 2024-05-31 14:31:01.395133\n",
      "Printing jpg 59 of 458 at time 2024-05-31 14:31:03.509063\n",
      "Printing jpg 60 of 458 at time 2024-05-31 14:31:05.650017\n",
      "Printing jpg 61 of 458 at time 2024-05-31 14:31:07.864038\n",
      "Printing jpg 62 of 458 at time 2024-05-31 14:31:09.814819\n",
      "Printing jpg 63 of 458 at time 2024-05-31 14:31:11.913735\n",
      "Printing jpg 64 of 458 at time 2024-05-31 14:31:13.781440\n",
      "Printing jpg 65 of 458 at time 2024-05-31 14:31:15.831311\n",
      "Printing jpg 66 of 458 at time 2024-05-31 14:31:17.880181\n",
      "Printing jpg 67 of 458 at time 2024-05-31 14:31:19.867996\n",
      "Printing jpg 68 of 458 at time 2024-05-31 14:31:21.764727\n",
      "Printing jpg 69 of 458 at time 2024-05-31 14:31:23.663461\n",
      "Printing jpg 70 of 458 at time 2024-05-31 14:31:25.789401\n",
      "Printing jpg 71 of 458 at time 2024-05-31 14:31:27.732175\n",
      "Printing jpg 72 of 458 at time 2024-05-31 14:31:30.032274\n",
      "Printing jpg 73 of 458 at time 2024-05-31 14:31:31.872458\n",
      "Printing jpg 74 of 458 at time 2024-05-31 14:31:33.764184\n",
      "Printing jpg 75 of 458 at time 2024-05-31 14:31:35.529796\n",
      "Printing jpg 76 of 458 at time 2024-05-31 14:31:37.581669\n",
      "Printing jpg 77 of 458 at time 2024-05-31 14:31:39.426353\n",
      "Printing jpg 78 of 458 at time 2024-05-31 14:31:41.414168\n",
      "Printing jpg 79 of 458 at time 2024-05-31 14:31:43.497069\n",
      "Printing jpg 80 of 458 at time 2024-05-31 14:31:45.398805\n",
      "Printing jpg 81 of 458 at time 2024-05-31 14:31:47.281524\n",
      "Printing jpg 82 of 458 at time 2024-05-31 14:31:49.048747\n",
      "Printing jpg 83 of 458 at time 2024-05-31 14:31:50.913449\n",
      "Printing jpg 84 of 458 at time 2024-05-31 14:31:52.981336\n",
      "Printing jpg 85 of 458 at time 2024-05-31 14:31:54.899087\n",
      "Printing jpg 86 of 458 at time 2024-05-31 14:31:56.813835\n",
      "Printing jpg 87 of 458 at time 2024-05-31 14:31:59.147966\n",
      "Printing jpg 88 of 458 at time 2024-05-31 14:32:01.160803\n",
      "Printing jpg 89 of 458 at time 2024-05-31 14:32:03.147617\n",
      "Printing jpg 90 of 458 at time 2024-05-31 14:32:05.364640\n",
      "Printing jpg 91 of 458 at time 2024-05-31 14:32:07.264375\n",
      "Printing jpg 92 of 458 at time 2024-05-31 14:32:09.631535\n",
      "Printing jpg 93 of 458 at time 2024-05-31 14:32:11.730452\n",
      "Printing jpg 94 of 458 at time 2024-05-31 14:32:14.047567\n",
      "Printing jpg 95 of 458 at time 2024-05-31 14:32:16.031378\n",
      "Printing jpg 96 of 458 at time 2024-05-31 14:32:18.064234\n",
      "Printing jpg 97 of 458 at time 2024-05-31 14:32:20.181166\n",
      "Printing jpg 98 of 458 at time 2024-05-31 14:32:22.442230\n",
      "Printing jpg 99 of 458 at time 2024-05-31 14:32:24.532137\n",
      "Printing jpg 100 of 458 at time 2024-05-31 14:32:26.897297\n",
      "Printing jpg 101 of 458 at time 2024-05-31 14:32:29.425605\n",
      "Printing jpg 102 of 458 at time 2024-05-31 14:32:31.830800\n",
      "Printing jpg 103 of 458 at time 2024-05-31 14:32:34.499236\n",
      "Printing jpg 104 of 458 at time 2024-05-31 14:32:36.969491\n",
      "Printing jpg 105 of 458 at time 2024-05-31 14:32:39.447753\n",
      "Printing jpg 106 of 458 at time 2024-05-31 14:32:42.031111\n",
      "Printing jpg 107 of 458 at time 2024-05-31 14:32:44.495361\n",
      "Printing jpg 108 of 458 at time 2024-05-31 14:32:47.128765\n",
      "Printing jpg 109 of 458 at time 2024-05-31 14:32:49.147608\n",
      "Printing jpg 110 of 458 at time 2024-05-31 14:32:51.825052\n",
      "Printing jpg 111 of 458 at time 2024-05-31 14:32:54.447446\n",
      "Printing jpg 112 of 458 at time 2024-05-31 14:32:56.869657\n",
      "Printing jpg 113 of 458 at time 2024-05-31 14:32:59.147736\n",
      "Printing jpg 114 of 458 at time 2024-05-31 14:33:01.864216\n",
      "Printing jpg 115 of 458 at time 2024-05-31 14:33:03.916089\n",
      "Printing jpg 116 of 458 at time 2024-05-31 14:33:07.557872\n",
      "Printing jpg 117 of 458 at time 2024-05-31 14:33:09.794455\n",
      "Printing jpg 118 of 458 at time 2024-05-31 14:33:11.829313\n",
      "Printing jpg 119 of 458 at time 2024-05-31 14:33:13.756072\n",
      "Printing jpg 120 of 458 at time 2024-05-31 14:33:15.824960\n",
      "Printing jpg 121 of 458 at time 2024-05-31 14:33:17.996943\n",
      "Printing jpg 122 of 458 at time 2024-05-31 14:33:19.963738\n",
      "Printing jpg 123 of 458 at time 2024-05-31 14:33:23.330832\n",
      "Printing jpg 124 of 458 at time 2024-05-31 14:33:25.696992\n",
      "Printing jpg 125 of 458 at time 2024-05-31 14:33:28.049139\n",
      "Printing jpg 126 of 458 at time 2024-05-31 14:33:30.313206\n",
      "Printing jpg 127 of 458 at time 2024-05-31 14:33:32.348063\n",
      "Printing jpg 128 of 458 at time 2024-05-31 14:33:34.531056\n",
      "Printing jpg 129 of 458 at time 2024-05-31 14:33:36.582929\n",
      "Printing jpg 130 of 458 at time 2024-05-31 14:33:38.447631\n",
      "Printing jpg 131 of 458 at time 2024-05-31 14:33:40.794774\n",
      "Printing jpg 132 of 458 at time 2024-05-31 14:33:42.665481\n",
      "Printing jpg 133 of 458 at time 2024-05-31 14:33:45.443017\n",
      "Printing jpg 134 of 458 at time 2024-05-31 14:33:47.955310\n",
      "Printing jpg 135 of 458 at time 2024-05-31 14:33:50.247415\n",
      "Printing jpg 136 of 458 at time 2024-05-31 14:33:52.264256\n",
      "Printing jpg 137 of 458 at time 2024-05-31 14:33:54.530325\n",
      "Printing jpg 138 of 458 at time 2024-05-31 14:33:56.731334\n",
      "Printing jpg 139 of 458 at time 2024-05-31 14:33:58.996402\n",
      "Printing jpg 140 of 458 at time 2024-05-31 14:34:01.094317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 141 of 458 at time 2024-05-31 14:34:03.080130\n",
      "Printing jpg 142 of 458 at time 2024-05-31 14:34:05.247108\n",
      "Printing jpg 143 of 458 at time 2024-05-31 14:34:07.263949\n",
      "Printing jpg 144 of 458 at time 2024-05-31 14:34:09.246759\n",
      "Printing jpg 145 of 458 at time 2024-05-31 14:34:11.830117\n",
      "Printing jpg 146 of 458 at time 2024-05-31 14:34:13.947049\n",
      "Printing jpg 147 of 458 at time 2024-05-31 14:34:16.263164\n",
      "Printing jpg 148 of 458 at time 2024-05-31 14:34:18.246975\n",
      "Printing jpg 149 of 458 at time 2024-05-31 14:34:20.606128\n",
      "Printing jpg 150 of 458 at time 2024-05-31 14:34:22.963280\n",
      "Printing jpg 151 of 458 at time 2024-05-31 14:34:25.330441\n",
      "Printing jpg 152 of 458 at time 2024-05-31 14:34:27.555472\n",
      "Printing jpg 153 of 458 at time 2024-05-31 14:34:29.813533\n",
      "Printing jpg 154 of 458 at time 2024-05-31 14:34:31.597161\n",
      "Printing jpg 155 of 458 at time 2024-05-31 14:34:33.596987\n",
      "Printing jpg 156 of 458 at time 2024-05-31 14:34:35.811008\n",
      "Printing jpg 157 of 458 at time 2024-05-31 14:34:37.880898\n",
      "Printing jpg 158 of 458 at time 2024-05-31 14:34:39.663525\n",
      "Printing jpg 159 of 458 at time 2024-05-31 14:34:41.763442\n",
      "Printing jpg 160 of 458 at time 2024-05-31 14:34:43.761265\n",
      "Printing jpg 161 of 458 at time 2024-05-31 14:34:45.780108\n",
      "Printing jpg 162 of 458 at time 2024-05-31 14:34:48.030162\n",
      "Printing jpg 163 of 458 at time 2024-05-31 14:34:50.247186\n",
      "Printing jpg 164 of 458 at time 2024-05-31 14:34:52.326084\n",
      "Printing jpg 165 of 458 at time 2024-05-31 14:34:54.447020\n",
      "Printing jpg 166 of 458 at time 2024-05-31 14:34:56.635017\n",
      "Printing jpg 167 of 458 at time 2024-05-31 14:34:58.963142\n",
      "Printing jpg 168 of 458 at time 2024-05-31 14:35:01.648593\n",
      "Printing jpg 169 of 458 at time 2024-05-31 14:35:03.494278\n",
      "Printing jpg 170 of 458 at time 2024-05-31 14:35:05.664258\n",
      "Printing jpg 171 of 458 at time 2024-05-31 14:35:07.530962\n",
      "Printing jpg 172 of 458 at time 2024-05-31 14:35:09.729969\n",
      "Printing jpg 173 of 458 at time 2024-05-31 14:35:12.013053\n",
      "Printing jpg 174 of 458 at time 2024-05-31 14:35:14.263107\n",
      "Printing jpg 175 of 458 at time 2024-05-31 14:35:16.530176\n",
      "Printing jpg 176 of 458 at time 2024-05-31 14:35:18.647108\n",
      "Printing jpg 177 of 458 at time 2024-05-31 14:35:21.063314\n",
      "Printing jpg 178 of 458 at time 2024-05-31 14:35:23.513550\n",
      "Printing jpg 179 of 458 at time 2024-05-31 14:35:25.779618\n",
      "Printing jpg 180 of 458 at time 2024-05-31 14:35:28.714297\n",
      "Printing jpg 181 of 458 at time 2024-05-31 14:35:30.963350\n",
      "Printing jpg 182 of 458 at time 2024-05-31 14:35:33.280465\n",
      "Printing jpg 183 of 458 at time 2024-05-31 14:35:35.547534\n",
      "Printing jpg 184 of 458 at time 2024-05-31 14:35:37.680481\n",
      "Printing jpg 185 of 458 at time 2024-05-31 14:35:40.029625\n",
      "Printing jpg 186 of 458 at time 2024-05-31 14:35:42.225630\n",
      "Printing jpg 187 of 458 at time 2024-05-31 14:35:44.662854\n",
      "Printing jpg 188 of 458 at time 2024-05-31 14:35:47.180152\n",
      "Printing jpg 189 of 458 at time 2024-05-31 14:35:50.062783\n",
      "Printing jpg 190 of 458 at time 2024-05-31 14:35:52.946928\n",
      "Printing jpg 191 of 458 at time 2024-05-31 14:35:55.294070\n",
      "Printing jpg 192 of 458 at time 2024-05-31 14:35:58.046583\n",
      "Printing jpg 193 of 458 at time 2024-05-31 14:36:00.481806\n",
      "Printing jpg 194 of 458 at time 2024-05-31 14:36:03.144236\n",
      "Printing jpg 195 of 458 at time 2024-05-31 14:36:06.094929\n",
      "Printing jpg 196 of 458 at time 2024-05-31 14:36:08.730334\n",
      "Printing jpg 197 of 458 at time 2024-05-31 14:36:10.894310\n",
      "Printing jpg 198 of 458 at time 2024-05-31 14:36:13.293499\n",
      "Printing jpg 199 of 458 at time 2024-05-31 14:36:15.727721\n",
      "Printing jpg 200 of 458 at time 2024-05-31 14:36:18.012807\n",
      "Printing jpg 201 of 458 at time 2024-05-31 14:36:20.330923\n",
      "Printing jpg 202 of 458 at time 2024-05-31 14:36:22.529945\n",
      "Printing jpg 203 of 458 at time 2024-05-31 14:36:24.663893\n",
      "Printing jpg 204 of 458 at time 2024-05-31 14:36:26.647704\n",
      "Printing jpg 205 of 458 at time 2024-05-31 14:36:28.763635\n",
      "Printing jpg 206 of 458 at time 2024-05-31 14:36:31.115782\n",
      "Printing jpg 207 of 458 at time 2024-05-31 14:36:33.840269\n",
      "Printing jpg 208 of 458 at time 2024-05-31 14:36:36.696876\n",
      "Printing jpg 209 of 458 at time 2024-05-31 14:36:39.364311\n",
      "Printing jpg 210 of 458 at time 2024-05-31 14:36:42.119826\n",
      "Printing jpg 211 of 458 at time 2024-05-31 14:36:44.362873\n",
      "Printing jpg 212 of 458 at time 2024-05-31 14:36:46.646958\n",
      "Printing jpg 213 of 458 at time 2024-05-31 14:36:49.039141\n",
      "Printing jpg 214 of 458 at time 2024-05-31 14:36:51.125045\n",
      "Printing jpg 215 of 458 at time 2024-05-31 14:36:53.663362\n",
      "Printing jpg 216 of 458 at time 2024-05-31 14:36:56.829252\n",
      "Printing jpg 217 of 458 at time 2024-05-31 14:36:59.955105\n",
      "Printing jpg 218 of 458 at time 2024-05-31 14:37:02.401338\n",
      "Printing jpg 219 of 458 at time 2024-05-31 14:37:04.929645\n",
      "Printing jpg 220 of 458 at time 2024-05-31 14:37:07.197716\n",
      "Printing jpg 221 of 458 at time 2024-05-31 14:37:09.663967\n",
      "Printing jpg 222 of 458 at time 2024-05-31 14:37:11.829944\n",
      "Printing jpg 223 of 458 at time 2024-05-31 14:37:13.931862\n",
      "Printing jpg 224 of 458 at time 2024-05-31 14:37:16.169905\n",
      "Printing jpg 225 of 458 at time 2024-05-31 14:37:18.463999\n",
      "Printing jpg 226 of 458 at time 2024-05-31 14:37:20.512869\n",
      "Printing jpg 227 of 458 at time 2024-05-31 14:37:22.546725\n",
      "Printing jpg 228 of 458 at time 2024-05-31 14:37:24.810792\n",
      "Printing jpg 229 of 458 at time 2024-05-31 14:37:27.553295\n",
      "Printing jpg 230 of 458 at time 2024-05-31 14:37:29.780328\n",
      "Printing jpg 231 of 458 at time 2024-05-31 14:37:31.794166\n",
      "Printing jpg 232 of 458 at time 2024-05-31 14:37:34.213374\n",
      "Printing jpg 233 of 458 at time 2024-05-31 14:37:36.347322\n",
      "Printing jpg 234 of 458 at time 2024-05-31 14:37:38.646420\n",
      "Printing jpg 235 of 458 at time 2024-05-31 14:37:40.596200\n",
      "Printing jpg 236 of 458 at time 2024-05-31 14:37:42.516953\n",
      "Printing jpg 237 of 458 at time 2024-05-31 14:37:44.863095\n",
      "Printing jpg 238 of 458 at time 2024-05-31 14:37:46.964012\n",
      "Printing jpg 239 of 458 at time 2024-05-31 14:37:49.384221\n",
      "Printing jpg 240 of 458 at time 2024-05-31 14:37:51.510764\n",
      "Printing jpg 241 of 458 at time 2024-05-31 14:37:53.684759\n",
      "Printing jpg 242 of 458 at time 2024-05-31 14:37:55.881764\n",
      "Printing jpg 243 of 458 at time 2024-05-31 14:37:57.914619\n",
      "Printing jpg 244 of 458 at time 2024-05-31 14:38:00.098613\n",
      "Printing jpg 245 of 458 at time 2024-05-31 14:38:02.081423\n",
      "Printing jpg 246 of 458 at time 2024-05-31 14:38:04.097263\n",
      "Printing jpg 247 of 458 at time 2024-05-31 14:38:06.296270\n",
      "Printing jpg 248 of 458 at time 2024-05-31 14:38:08.296095\n",
      "Printing jpg 249 of 458 at time 2024-05-31 14:38:11.831322\n",
      "Printing jpg 250 of 458 at time 2024-05-31 14:38:13.897208\n",
      "Printing jpg 251 of 458 at time 2024-05-31 14:38:15.996123\n",
      "Printing jpg 252 of 458 at time 2024-05-31 14:38:18.114056\n",
      "Printing jpg 253 of 458 at time 2024-05-31 14:38:20.230988\n",
      "Printing jpg 254 of 458 at time 2024-05-31 14:38:22.746284\n",
      "Printing jpg 255 of 458 at time 2024-05-31 14:38:25.279597\n",
      "Printing jpg 256 of 458 at time 2024-05-31 14:38:27.980062\n",
      "Printing jpg 257 of 458 at time 2024-05-31 14:38:30.609462\n",
      "Printing jpg 258 of 458 at time 2024-05-31 14:38:33.508107\n",
      "Printing jpg 259 of 458 at time 2024-05-31 14:38:35.748152\n",
      "Printing jpg 260 of 458 at time 2024-05-31 14:38:38.296478\n",
      "Printing jpg 261 of 458 at time 2024-05-31 14:38:40.496486\n",
      "Printing jpg 262 of 458 at time 2024-05-31 14:38:42.679992\n",
      "Printing jpg 263 of 458 at time 2024-05-31 14:38:44.933055\n",
      "Printing jpg 264 of 458 at time 2024-05-31 14:38:47.147076\n",
      "Printing jpg 265 of 458 at time 2024-05-31 14:38:49.613327\n",
      "Printing jpg 266 of 458 at time 2024-05-31 14:38:52.256740\n",
      "Printing jpg 267 of 458 at time 2024-05-31 14:38:54.462753\n",
      "Printing jpg 268 of 458 at time 2024-05-31 14:38:56.696792\n",
      "Printing jpg 269 of 458 at time 2024-05-31 14:38:59.063953\n",
      "Printing jpg 270 of 458 at time 2024-05-31 14:39:01.497174\n",
      "Printing jpg 271 of 458 at time 2024-05-31 14:39:04.191633\n",
      "Printing jpg 272 of 458 at time 2024-05-31 14:39:06.962162\n",
      "Printing jpg 273 of 458 at time 2024-05-31 14:39:09.415401\n",
      "Printing jpg 274 of 458 at time 2024-05-31 14:39:11.430240\n",
      "Printing jpg 275 of 458 at time 2024-05-31 14:39:13.783388\n",
      "Printing jpg 276 of 458 at time 2024-05-31 14:39:16.024434\n",
      "Printing jpg 277 of 458 at time 2024-05-31 14:39:18.146370\n",
      "Printing jpg 278 of 458 at time 2024-05-31 14:39:20.314349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 279 of 458 at time 2024-05-31 14:39:22.813631\n",
      "Printing jpg 280 of 458 at time 2024-05-31 14:39:24.873510\n",
      "Printing jpg 281 of 458 at time 2024-05-31 14:39:27.046494\n",
      "Printing jpg 282 of 458 at time 2024-05-31 14:39:29.232489\n",
      "Printing jpg 283 of 458 at time 2024-05-31 14:39:31.263343\n",
      "Printing jpg 284 of 458 at time 2024-05-31 14:39:33.424316\n",
      "Printing jpg 285 of 458 at time 2024-05-31 14:39:35.447162\n",
      "Printing jpg 286 of 458 at time 2024-05-31 14:39:37.563093\n",
      "Printing jpg 287 of 458 at time 2024-05-31 14:39:39.714056\n",
      "Printing jpg 288 of 458 at time 2024-05-31 14:39:41.687858\n",
      "Printing jpg 289 of 458 at time 2024-05-31 14:39:43.713707\n",
      "Printing jpg 290 of 458 at time 2024-05-31 14:39:45.695516\n",
      "Printing jpg 291 of 458 at time 2024-05-31 14:39:47.580236\n",
      "Printing jpg 292 of 458 at time 2024-05-31 14:39:49.723192\n",
      "Printing jpg 293 of 458 at time 2024-05-31 14:39:51.913191\n",
      "Printing jpg 294 of 458 at time 2024-05-31 14:39:54.030637\n",
      "Printing jpg 295 of 458 at time 2024-05-31 14:39:56.096523\n",
      "Printing jpg 296 of 458 at time 2024-05-31 14:39:58.096348\n",
      "Printing jpg 297 of 458 at time 2024-05-31 14:40:00.215282\n",
      "Printing jpg 298 of 458 at time 2024-05-31 14:40:02.341223\n",
      "Printing jpg 299 of 458 at time 2024-05-31 14:40:04.396098\n",
      "Printing jpg 300 of 458 at time 2024-05-31 14:40:06.446970\n",
      "Printing jpg 301 of 458 at time 2024-05-31 14:40:08.495840\n",
      "Printing jpg 302 of 458 at time 2024-05-31 14:40:10.695848\n",
      "Printing jpg 303 of 458 at time 2024-05-31 14:40:12.662644\n",
      "Printing jpg 304 of 458 at time 2024-05-31 14:40:14.912697\n",
      "Printing jpg 305 of 458 at time 2024-05-31 14:40:17.129721\n",
      "Printing jpg 306 of 458 at time 2024-05-31 14:40:19.062485\n",
      "Printing jpg 307 of 458 at time 2024-05-31 14:40:21.163403\n",
      "Printing jpg 308 of 458 at time 2024-05-31 14:40:23.281336\n",
      "Printing jpg 309 of 458 at time 2024-05-31 14:40:25.263145\n",
      "Printing jpg 310 of 458 at time 2024-05-31 14:40:27.763427\n",
      "Printing jpg 311 of 458 at time 2024-05-31 14:40:29.793280\n",
      "WARNING! 9673 could not be made\n",
      "Printing jpg 312 of 458 at time 2024-05-31 14:40:30.906296\n",
      "Printing jpg 313 of 458 at time 2024-05-31 14:40:33.662812\n",
      "Printing jpg 314 of 458 at time 2024-05-31 14:40:36.193121\n",
      "Printing jpg 315 of 458 at time 2024-05-31 14:40:38.846543\n",
      "Printing jpg 316 of 458 at time 2024-05-31 14:40:41.523987\n",
      "Printing jpg 317 of 458 at time 2024-05-31 14:40:45.093245\n",
      "Printing jpg 318 of 458 at time 2024-05-31 14:40:47.529468\n",
      "Printing jpg 319 of 458 at time 2024-05-31 14:40:50.192899\n",
      "Printing jpg 320 of 458 at time 2024-05-31 14:40:52.361879\n",
      "Printing jpg 321 of 458 at time 2024-05-31 14:40:54.929222\n",
      "Printing jpg 322 of 458 at time 2024-05-31 14:40:57.312398\n",
      "Printing jpg 323 of 458 at time 2024-05-31 14:40:59.862726\n",
      "Printing jpg 324 of 458 at time 2024-05-31 14:41:02.331979\n",
      "Printing jpg 325 of 458 at time 2024-05-31 14:41:04.962380\n",
      "Printing jpg 326 of 458 at time 2024-05-31 14:41:07.757932\n",
      "Printing jpg 327 of 458 at time 2024-05-31 14:41:10.247204\n",
      "Printing jpg 328 of 458 at time 2024-05-31 14:41:12.413181\n",
      "Printing jpg 329 of 458 at time 2024-05-31 14:41:14.993536\n",
      "Printing jpg 330 of 458 at time 2024-05-31 14:41:17.163516\n",
      "Printing jpg 331 of 458 at time 2024-05-31 14:41:19.446601\n",
      "Printing jpg 332 of 458 at time 2024-05-31 14:41:21.692651\n",
      "Printing jpg 333 of 458 at time 2024-05-31 14:41:23.945707\n",
      "Printing jpg 334 of 458 at time 2024-05-31 14:41:26.792306\n",
      "Printing jpg 335 of 458 at time 2024-05-31 14:41:29.193497\n",
      "Printing jpg 336 of 458 at time 2024-05-31 14:41:31.580676\n",
      "Printing jpg 337 of 458 at time 2024-05-31 14:41:33.912805\n",
      "Printing jpg 338 of 458 at time 2024-05-31 14:41:36.209901\n",
      "Printing jpg 339 of 458 at time 2024-05-31 14:41:38.428927\n",
      "Printing jpg 340 of 458 at time 2024-05-31 14:41:40.805096\n",
      "Printing jpg 341 of 458 at time 2024-05-31 14:41:43.029126\n",
      "Printing jpg 342 of 458 at time 2024-05-31 14:41:45.746606\n",
      "Printing jpg 343 of 458 at time 2024-05-31 14:41:48.212857\n",
      "Printing jpg 344 of 458 at time 2024-05-31 14:41:50.676105\n",
      "Printing jpg 345 of 458 at time 2024-05-31 14:41:53.118335\n",
      "Printing jpg 346 of 458 at time 2024-05-31 14:41:55.362159\n",
      "Printing jpg 347 of 458 at time 2024-05-31 14:41:57.992560\n",
      "Printing jpg 348 of 458 at time 2024-05-31 14:42:00.279647\n",
      "Printing jpg 349 of 458 at time 2024-05-31 14:42:02.779929\n",
      "Printing jpg 350 of 458 at time 2024-05-31 14:42:05.012968\n",
      "Printing jpg 351 of 458 at time 2024-05-31 14:42:07.146915\n",
      "Printing jpg 352 of 458 at time 2024-05-31 14:42:09.361937\n",
      "Printing jpg 353 of 458 at time 2024-05-31 14:42:11.446840\n",
      "Printing jpg 354 of 458 at time 2024-05-31 14:42:14.062227\n",
      "Printing jpg 355 of 458 at time 2024-05-31 14:42:16.313282\n",
      "Printing jpg 356 of 458 at time 2024-05-31 14:42:18.796549\n",
      "Printing jpg 357 of 458 at time 2024-05-31 14:42:21.239779\n",
      "Printing jpg 358 of 458 at time 2024-05-31 14:42:23.509851\n",
      "Printing jpg 359 of 458 at time 2024-05-31 14:42:26.310407\n",
      "Printing jpg 360 of 458 at time 2024-05-31 14:42:28.778660\n",
      "Printing jpg 361 of 458 at time 2024-05-31 14:42:31.298960\n",
      "Printing jpg 362 of 458 at time 2024-05-31 14:42:33.946377\n",
      "Printing jpg 363 of 458 at time 2024-05-31 14:42:36.528734\n",
      "Printing jpg 364 of 458 at time 2024-05-31 14:42:39.093074\n",
      "Printing jpg 365 of 458 at time 2024-05-31 14:42:41.707461\n",
      "Printing jpg 366 of 458 at time 2024-05-31 14:42:43.829397\n",
      "Printing jpg 367 of 458 at time 2024-05-31 14:42:46.264620\n",
      "Printing jpg 368 of 458 at time 2024-05-31 14:42:48.446612\n",
      "Printing jpg 369 of 458 at time 2024-05-31 14:42:50.724691\n",
      "Printing jpg 370 of 458 at time 2024-05-31 14:42:53.029795\n",
      "Printing jpg 371 of 458 at time 2024-05-31 14:42:55.529076\n",
      "Printing jpg 372 of 458 at time 2024-05-31 14:42:58.222535\n",
      "Printing jpg 373 of 458 at time 2024-05-31 14:43:00.677776\n",
      "Printing jpg 374 of 458 at time 2024-05-31 14:43:02.929831\n",
      "Printing jpg 375 of 458 at time 2024-05-31 14:43:05.364053\n",
      "Printing jpg 376 of 458 at time 2024-05-31 14:43:09.112474\n",
      "Printing jpg 377 of 458 at time 2024-05-31 14:43:11.529681\n",
      "Printing jpg 378 of 458 at time 2024-05-31 14:43:13.929871\n",
      "Printing jpg 379 of 458 at time 2024-05-31 14:43:16.379107\n",
      "Printing jpg 380 of 458 at time 2024-05-31 14:43:19.453914\n",
      "Printing jpg 381 of 458 at time 2024-05-31 14:43:22.291504\n",
      "Printing jpg 382 of 458 at time 2024-05-31 14:43:25.296246\n",
      "Printing jpg 383 of 458 at time 2024-05-31 14:43:28.261953\n",
      "Printing jpg 384 of 458 at time 2024-05-31 14:43:30.829296\n",
      "Printing jpg 385 of 458 at time 2024-05-31 14:43:33.213473\n",
      "Printing jpg 386 of 458 at time 2024-05-31 14:43:35.545601\n",
      "Printing jpg 387 of 458 at time 2024-05-31 14:43:38.107940\n",
      "Printing jpg 388 of 458 at time 2024-05-31 14:43:40.612226\n",
      "Printing jpg 389 of 458 at time 2024-05-31 14:43:43.262645\n",
      "Printing jpg 390 of 458 at time 2024-05-31 14:43:45.546730\n",
      "Printing jpg 391 of 458 at time 2024-05-31 14:43:48.062026\n",
      "Printing jpg 392 of 458 at time 2024-05-31 14:43:51.106805\n",
      "Printing jpg 393 of 458 at time 2024-05-31 14:43:53.743211\n",
      "Printing jpg 394 of 458 at time 2024-05-31 14:43:56.460669\n",
      "Printing jpg 395 of 458 at time 2024-05-31 14:43:58.479512\n",
      "Printing jpg 396 of 458 at time 2024-05-31 14:44:00.983798\n",
      "Printing jpg 397 of 458 at time 2024-05-31 14:44:03.296909\n",
      "Printing jpg 398 of 458 at time 2024-05-31 14:44:06.028402\n",
      "Printing jpg 399 of 458 at time 2024-05-31 14:44:08.879004\n",
      "Printing jpg 400 of 458 at time 2024-05-31 14:44:12.393212\n",
      "Printing jpg 401 of 458 at time 2024-05-31 14:44:15.122703\n",
      "Printing jpg 402 of 458 at time 2024-05-31 14:44:18.014342\n",
      "Printing jpg 403 of 458 at time 2024-05-31 14:44:20.428546\n",
      "Printing jpg 404 of 458 at time 2024-05-31 14:44:22.928828\n",
      "Printing jpg 405 of 458 at time 2024-05-31 14:44:25.562232\n",
      "Printing jpg 406 of 458 at time 2024-05-31 14:44:28.328757\n",
      "Printing jpg 407 of 458 at time 2024-05-31 14:44:31.062252\n",
      "Printing jpg 408 of 458 at time 2024-05-31 14:44:34.023955\n",
      "Printing jpg 409 of 458 at time 2024-05-31 14:44:36.409132\n",
      "Printing jpg 410 of 458 at time 2024-05-31 14:44:39.193674\n",
      "Printing jpg 411 of 458 at time 2024-05-31 14:44:41.702964\n",
      "Printing jpg 412 of 458 at time 2024-05-31 14:44:44.245285\n",
      "Printing jpg 413 of 458 at time 2024-05-31 14:44:46.662491\n",
      "Printing jpg 414 of 458 at time 2024-05-31 14:44:49.361955\n",
      "Printing jpg 415 of 458 at time 2024-05-31 14:44:51.824203\n",
      "Printing jpg 416 of 458 at time 2024-05-31 14:44:54.425577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 417 of 458 at time 2024-05-31 14:44:56.834775\n",
      "Printing jpg 418 of 458 at time 2024-05-31 14:44:59.502210\n",
      "Printing jpg 419 of 458 at time 2024-05-31 14:45:02.053538\n",
      "Printing jpg 420 of 458 at time 2024-05-31 14:45:04.492764\n",
      "Printing jpg 421 of 458 at time 2024-05-31 14:45:07.092137\n",
      "Printing jpg 422 of 458 at time 2024-05-31 14:45:09.472309\n",
      "Printing jpg 423 of 458 at time 2024-05-31 14:45:11.892518\n",
      "Printing jpg 424 of 458 at time 2024-05-31 14:45:14.312230\n",
      "Printing jpg 425 of 458 at time 2024-05-31 14:45:16.429162\n",
      "Printing jpg 426 of 458 at time 2024-05-31 14:45:18.646185\n",
      "Printing jpg 427 of 458 at time 2024-05-31 14:45:20.712070\n",
      "Printing jpg 428 of 458 at time 2024-05-31 14:45:22.863034\n",
      "Printing jpg 429 of 458 at time 2024-05-31 14:45:24.978965\n",
      "Printing jpg 430 of 458 at time 2024-05-31 14:45:27.614370\n",
      "Printing jpg 431 of 458 at time 2024-05-31 14:45:30.446955\n",
      "Printing jpg 432 of 458 at time 2024-05-31 14:45:32.916209\n",
      "Printing jpg 433 of 458 at time 2024-05-31 14:45:35.529594\n",
      "Printing jpg 434 of 458 at time 2024-05-31 14:45:37.945799\n",
      "Printing jpg 435 of 458 at time 2024-05-31 14:45:40.528156\n",
      "Printing jpg 436 of 458 at time 2024-05-31 14:45:42.961376\n",
      "Printing jpg 437 of 458 at time 2024-05-31 14:45:45.645826\n",
      "Printing jpg 438 of 458 at time 2024-05-31 14:45:48.423361\n",
      "Printing jpg 439 of 458 at time 2024-05-31 14:45:50.864589\n",
      "Printing jpg 440 of 458 at time 2024-05-31 14:45:53.245762\n",
      "Printing jpg 441 of 458 at time 2024-05-31 14:45:55.710011\n",
      "Printing jpg 442 of 458 at time 2024-05-31 14:45:58.393154\n",
      "Printing jpg 443 of 458 at time 2024-05-31 14:46:00.961498\n",
      "Printing jpg 444 of 458 at time 2024-05-31 14:46:03.609915\n",
      "Printing jpg 445 of 458 at time 2024-05-31 14:46:06.142226\n",
      "Printing jpg 446 of 458 at time 2024-05-31 14:46:08.783637\n",
      "Printing jpg 447 of 458 at time 2024-05-31 14:46:11.370036\n",
      "Printing jpg 448 of 458 at time 2024-05-31 14:46:14.053485\n",
      "Printing jpg 449 of 458 at time 2024-05-31 14:46:16.577789\n",
      "Printing jpg 450 of 458 at time 2024-05-31 14:46:19.345315\n",
      "Printing jpg 451 of 458 at time 2024-05-31 14:46:22.153878\n",
      "Printing jpg 452 of 458 at time 2024-05-31 14:46:25.102569\n",
      "Printing jpg 453 of 458 at time 2024-05-31 14:46:28.128331\n",
      "Printing jpg 454 of 458 at time 2024-05-31 14:46:30.753727\n",
      "Printing jpg 455 of 458 at time 2024-05-31 14:46:33.445183\n",
      "Printing jpg 456 of 458 at time 2024-05-31 14:46:35.908431\n",
      "Printing jpg 457 of 458 at time 2024-05-31 14:46:38.228549\n",
      "Printing jpg 458 of 458 at time 2024-05-31 14:46:41.428469\n",
      "the following pages failed: ['9673']\n",
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "#Export jpgs\n",
    "if run_jpg:\n",
    "    aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "    project_path = aprx.filePath\n",
    "\n",
    "    jpg_folder = output_folder + r'\\HTML\\Maps_And_CSS'\n",
    "    if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "\n",
    "    # project_directory = os.path.dirname(project_path)\n",
    "\n",
    "    layouts = aprx.listLayouts()\n",
    "    export_fails = []\n",
    "\n",
    "    for layout in layouts:\n",
    "\n",
    "        if layout.mapSeries is not None:\n",
    "            map_series = layout.mapSeries\n",
    "            # Loop through all pages in the map series\n",
    "            for page_number in range(1, map_series.pageCount + 1):\n",
    "                map_series.currentPageNumber = page_number\n",
    "                output_filename = os.path.join(jpg_folder, f\"{map_series.pageRow.Drains_To}.jpg\")\n",
    "                try:\n",
    "                    layout.exportToJPEG(output_filename, resolution=300)\n",
    "                except:\n",
    "                    print(f'WARNING! {map_series.pageRow.Drains_To} could not be made')\n",
    "                    export_fails.append(map_series.pageRow.Drains_To)\n",
    "                print (f'Printing jpg {page_number} of {map_series.pageCount} at time {datetime.datetime.now()}')\n",
    "\n",
    "    print(f'the following pages failed: {export_fails}')            \n",
    "    print(\"Export complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "#Create HTMLs\n",
    "\n",
    "if run_html:\n",
    "    shutil.copy2('style.css', html_folder + '\\\\style.css')\n",
    "    shutil.copy2('script.js', html_folder + '\\\\script.js')\n",
    "\n",
    "    for category in categories:\n",
    "        area_type = category[0]\n",
    "        area_names = category[1]\n",
    "        header_start = category[2]\n",
    "\n",
    "        f = open(html_folder + '\\\\Population_By_' + area_type + '_' + model_area + '.html', \"w\")\n",
    "        f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "        f.write('<script src=\"script.js\"></script>\\n')\n",
    "        f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "        f.write('<!DOCTYPE html>\\n')\n",
    "        f.write('<html>\\n')\n",
    "        f.write('<head>\\n')\n",
    "        f.write('<meta charset=\"utf-8\">\\n')\n",
    "        f.write('</head>\\n')\n",
    "        f.write('<body>\\n\\n')\n",
    "\n",
    "        f.write('<div class=\"tab\">\\n')\n",
    "        for area_name in area_names:\n",
    "            tab = area_name\n",
    "\n",
    "        #     color = ps_dict[first_year]\n",
    "        #     bg_color = color_dict[color][0]\n",
    "        #     text_color = color_dict[color][1]\n",
    "\n",
    "            f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "        f.write('</div>\\n')\n",
    "\n",
    "        pop_df = pop_dfss[0][2]\n",
    "\n",
    "        for area_name in area_names:\n",
    "\n",
    "            area_df = pop_df[pop_df[area_type]==area_name]\n",
    "            area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "\n",
    "            f.write('<div id=\"' + area_name + '\" class=\"tabcontent\">\\n') \n",
    "            f.write('<h1>' + area_name + '</h1>\\n')\n",
    "\n",
    "            f.write('<div class=\"sidenav\">\\n')\n",
    "\n",
    "            f.write('<table style=\\'width: 90%;\\'>\\n')\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<th>Year</th>\\n')\n",
    "            f.write('<th>Population</th>\\n')\n",
    "            f.write('</tr>\\n')\n",
    "\n",
    "            for index, row in area_df.iterrows():\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ str(index) + '</td>\\n')\n",
    "                population_with_separator = f\"{int(row['Population']):,}\"\n",
    "                f.write('<td>'+ population_with_separator + '</td>\\n')\n",
    "\n",
    "                f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "\n",
    "            for i in range(4):\n",
    "                f.write('<h1 style=\"color: white\">End of tables</h1>\\n')#Invisible, just to enable scroll to table bottoms\n",
    "\n",
    "            f.write('</div>\\n') #end sidenav\n",
    "\n",
    "\n",
    "            f.write('<div class=\"main\">\\n')\n",
    "\n",
    "            fig = go.Figure()\n",
    "\n",
    "\n",
    "            fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                         y = area_df.Population, \n",
    "                                         mode='lines',name=pop_dfss[0][0],line=dict(width=5)))\n",
    "\n",
    "            for pop_dfs in pop_dfss[1:]:\n",
    "                pop_df_past = pop_dfs[2]\n",
    "                area_df = pop_df_past[pop_df_past[area_type]==area_name]\n",
    "                area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "                fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                         y = area_df.Population, \n",
    "                                         mode='lines',name=pop_dfs[0],line=dict(width=2)))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=header_start + area_name,\n",
    "                autosize=False,\n",
    "                width = 1500,\n",
    "                height=850,\n",
    "                margin=dict(\n",
    "                    l=50,\n",
    "                    r=50,\n",
    "                    b=50,\n",
    "                    t=50,\n",
    "                    pad=4\n",
    "                    ),\n",
    "                    yaxis_title = 'Population'\n",
    "                )\n",
    "\n",
    "            f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "            f.write('</div>\\n') #end div main  \n",
    "\n",
    "            f.write('</div>\\n')  #end div tab   \n",
    "\n",
    "            f.write('</body>\\n')\n",
    "        f.write('</html>\\n')\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample DataFrame\n",
    "# data = {\n",
    "#     'Catchment': [1, 2, 3],\n",
    "#     'Node': [4, 5, 6]\n",
    "\n",
    "# }\n",
    "# accumulation_df = pd.DataFrame(data)\n",
    "\n",
    "# # Create a MultiIndex with the existing columns\n",
    "# existing_columns_multiindex = pd.MultiIndex.from_tuples([\n",
    "#     ('GENERAL INFO', 'Catchment'),  # Header with no subheaders\n",
    "#     ('GENERAL INFO', 'Node'),  # Header with no subheaders\n",
    "# ])\n",
    "\n",
    "# # Create a MultiIndex with the upper level 'GENERAL INFO'\n",
    "# upper_level = [('GENERAL INFO', '')] * len(existing_columns_multiindex)\n",
    "\n",
    "# # Concatenate the upper level and the existing columns MultiIndex\n",
    "# new_columns_multiindex = pd.MultiIndex.from_tuples(list(zip(upper_level, existing_columns_multiindex)))\n",
    "\n",
    "# # Assign the new MultiIndex to the DataFrame columns\n",
    "# accumulation_df.columns = new_columns_multiindex\n",
    "\n",
    "# accumulation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "# merge_set = set()\n",
    "# for node in accumulation_df[('GENERAL INFO','NODE')].unique():\n",
    "#     catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node][('GENERAL INFO','CATCHMENT')].unique())\n",
    "#     catchments = tuple(sorted(catchments))\n",
    "#     merge_set.add(catchments)\n",
    "# print(len(merge_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Merge_ID</th>\n",
       "      <th>Catchments</th>\n",
       "      <th>Catchment_Count</th>\n",
       "      <th>To_Dissolve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merge_ID_61</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merge_ID_25</td>\n",
       "      <td>(2052,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2052]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Merge_ID_80</td>\n",
       "      <td>(2122,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merge_ID_24</td>\n",
       "      <td>(2119, 2122)</td>\n",
       "      <td>2</td>\n",
       "      <td>[2119, Merge_ID_80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merge_ID_9</td>\n",
       "      <td>(2050, 2052)</td>\n",
       "      <td>2</td>\n",
       "      <td>[2050, Merge_ID_25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Merge_ID_6</td>\n",
       "      <td>(2059, 2062, 2063, 2064, 2066, 2068, 2069, 207...</td>\n",
       "      <td>95</td>\n",
       "      <td>[Merge_ID_81, Merge_ID_90]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Merge_ID_75</td>\n",
       "      <td>(2059, 2061, 2062, 2063, 2064, 2065, 2066, 206...</td>\n",
       "      <td>98</td>\n",
       "      <td>[2061, 2065, 2067, Merge_ID_6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Merge_ID_37</td>\n",
       "      <td>(2059, 2060, 2061, 2062, 2063, 2064, 2065, 206...</td>\n",
       "      <td>99</td>\n",
       "      <td>[2060, Merge_ID_75]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Merge_ID_20</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>104</td>\n",
       "      <td>[10108, 10110, 2070, 2075, 2089, Merge_ID_37]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Merge_ID_82</td>\n",
       "      <td>(10108, 10109, 10110, 2059, 2060, 2061, 2062, ...</td>\n",
       "      <td>105</td>\n",
       "      <td>[10109, Merge_ID_20]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Merge_ID  ...                                    To_Dissolve\n",
       "0    Merge_ID_61  ...                                         [2169]\n",
       "1    Merge_ID_25  ...                                         [2052]\n",
       "2    Merge_ID_80  ...                                         [2122]\n",
       "3    Merge_ID_24  ...                            [2119, Merge_ID_80]\n",
       "4     Merge_ID_9  ...                            [2050, Merge_ID_25]\n",
       "..           ...  ...                                            ...\n",
       "98    Merge_ID_6  ...                     [Merge_ID_81, Merge_ID_90]\n",
       "99   Merge_ID_75  ...                 [2061, 2065, 2067, Merge_ID_6]\n",
       "100  Merge_ID_37  ...                            [2060, Merge_ID_75]\n",
       "101  Merge_ID_20  ...  [10108, 10110, 2070, 2075, 2089, Merge_ID_37]\n",
       "102  Merge_ID_82  ...                           [10109, Merge_ID_20]\n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cou = 0\n",
    "\n",
    "# simpler_merge = []\n",
    "# for index1, row1 in merge_df.iterrows():\n",
    "#     catchments1 = list(row1['Catchments'])\n",
    "#     for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "#         catchments2 = row2['Catchments']\n",
    "        \n",
    "#         if len(catchments1) >= len(catchments2):\n",
    "#             if all(item in catchments1 for item in catchments2):\n",
    "#                 catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "#                 catchments1.append(row2['Merge_ID'])\n",
    "#     simpler_merge.append(catchments1)\n",
    "               \n",
    "# merge_df['To_Dissolve'] = simpler_merge\n",
    "# merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "# merge_df.reset_index(inplace=True,drop=True)\n",
    "# merge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StyleProxy in module openpyxl.styles.proxy object:\n",
      "\n",
      "class StyleProxy(builtins.object)\n",
      " |  StyleProxy(target)\n",
      " |  \n",
      " |  Proxy formatting objects so that they cannot be altered\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add proxied object to another instance and return the combined object\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |      Return a copy of the proxied object.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      " |  \n",
      " |  __init__(self, target)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, attr, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  copy(self, **kw)\n",
      " |      Return a copy of the proxied object. Keyword args will be passed through\n",
      " |      \n",
      " |      .. note::\n",
      " |          Deprecated: Use copy(obj) or cell.obj = cell.obj + other\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cell.border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method to_excel in module pandas.core.generic:\n",
      "\n",
      "to_excel(excel_writer, sheet_name: 'str' = 'Sheet1', na_rep: 'str' = '', float_format: 'str | None' = None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None, storage_options: 'StorageOptions' = None) -> 'None' method of pandas.core.frame.DataFrame instance\n",
      "    Write object to an Excel sheet.\n",
      "    \n",
      "    To write a single object to an Excel .xlsx file it is only necessary to\n",
      "    specify a target file name. To write to multiple sheets it is necessary to\n",
      "    create an `ExcelWriter` object with a target file name, and specify a sheet\n",
      "    in the file to write to.\n",
      "    \n",
      "    Multiple sheets may be written to by specifying unique `sheet_name`.\n",
      "    With all data written to the file it is necessary to save the changes.\n",
      "    Note that creating an `ExcelWriter` object with a file name that already\n",
      "    exists will result in the contents of the existing file being erased.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    excel_writer : path-like, file-like, or ExcelWriter object\n",
      "        File path or existing ExcelWriter.\n",
      "    sheet_name : str, default 'Sheet1'\n",
      "        Name of sheet which will contain DataFrame.\n",
      "    na_rep : str, default ''\n",
      "        Missing data representation.\n",
      "    float_format : str, optional\n",
      "        Format string for floating point numbers. For example\n",
      "        ``float_format=\"%.2f\"`` will format 0.1234 to 0.12.\n",
      "    columns : sequence or list of str, optional\n",
      "        Columns to write.\n",
      "    header : bool or list of str, default True\n",
      "        Write out the column names. If a list of string is given it is\n",
      "        assumed to be aliases for the column names.\n",
      "    index : bool, default True\n",
      "        Write row names (index).\n",
      "    index_label : str or sequence, optional\n",
      "        Column label for index column(s) if desired. If not specified, and\n",
      "        `header` and `index` are True, then the index names are used. A\n",
      "        sequence should be given if the DataFrame uses MultiIndex.\n",
      "    startrow : int, default 0\n",
      "        Upper left cell row to dump data frame.\n",
      "    startcol : int, default 0\n",
      "        Upper left cell column to dump data frame.\n",
      "    engine : str, optional\n",
      "        Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this\n",
      "        via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and\n",
      "        ``io.excel.xlsm.writer``.\n",
      "    \n",
      "        .. deprecated:: 1.2.0\n",
      "    \n",
      "            As the `xlwt <https://pypi.org/project/xlwt/>`__ package is no longer\n",
      "            maintained, the ``xlwt`` engine will be removed in a future version\n",
      "            of pandas.\n",
      "    \n",
      "    merge_cells : bool, default True\n",
      "        Write MultiIndex and Hierarchical Rows as merged cells.\n",
      "    encoding : str, optional\n",
      "        Encoding of the resulting excel file. Only necessary for xlwt,\n",
      "        other writers support unicode natively.\n",
      "    inf_rep : str, default 'inf'\n",
      "        Representation for infinity (there is no native representation for\n",
      "        infinity in Excel).\n",
      "    verbose : bool, default True\n",
      "        Display more information in the error logs.\n",
      "    freeze_panes : tuple of int (length 2), optional\n",
      "        Specifies the one-based bottommost row and rightmost column that\n",
      "        is to be frozen.\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    ExcelWriter : Class for writing DataFrame objects into excel sheets.\n",
      "    read_excel : Read an Excel file into a pandas DataFrame.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    For compatibility with :meth:`~DataFrame.to_csv`,\n",
      "    to_excel serializes lists and dicts to strings before writing.\n",
      "    \n",
      "    Once a workbook has been saved it is not possible to write further\n",
      "    data without rewriting the whole workbook.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    Create, write to and save a workbook:\n",
      "    \n",
      "    >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
      "    ...                    index=['row 1', 'row 2'],\n",
      "    ...                    columns=['col 1', 'col 2'])\n",
      "    >>> df1.to_excel(\"output.xlsx\")  # doctest: +SKIP\n",
      "    \n",
      "    To specify the sheet name:\n",
      "    \n",
      "    >>> df1.to_excel(\"output.xlsx\",\n",
      "    ...              sheet_name='Sheet_name_1')  # doctest: +SKIP\n",
      "    \n",
      "    If you wish to write to more than one sheet in the workbook, it is\n",
      "    necessary to specify an ExcelWriter object:\n",
      "    \n",
      "    >>> df2 = df1.copy()\n",
      "    >>> with pd.ExcelWriter('output.xlsx') as writer:  # doctest: +SKIP\n",
      "    ...     df1.to_excel(writer, sheet_name='Sheet_name_1')\n",
      "    ...     df2.to_excel(writer, sheet_name='Sheet_name_2')\n",
      "    \n",
      "    ExcelWriter can also be used to append to an existing Excel file:\n",
      "    \n",
      "    >>> with pd.ExcelWriter('output.xlsx',\n",
      "    ...                     mode='a') as writer:  # doctest: +SKIP\n",
      "    ...     df.to_excel(writer, sheet_name='Sheet_name_3')\n",
      "    \n",
      "    To set the library that is used to write the Excel file,\n",
      "    you can pass the `engine` keyword (the default engine is\n",
      "    automatically chosen depending on the file extension):\n",
      "    \n",
      "    >>> df1.to_excel('output1.xlsx', engine='xlsxwriter')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df1.to_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "node_single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_single_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_blue = \"ADD8E6\"\n",
    "hex_yellow = \"FFFACD\"\n",
    "border_style = Side(style='thin', color='000000')\n",
    "border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "border_style_none = Side(style=None, color='000000')\n",
    "border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "\n",
    "excel_folder = output_folder + '\\\\Excel'\n",
    "if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "    node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "    node_single_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "    startrow = 13\n",
    "    with pd.ExcelWriter(sheetpath) as writer:\n",
    "        node_single_df.to_excel(writer, sheet_name=\"Sheet1\",startrow=startrow)\n",
    "        info_df.to_excel(writer, sheet_name=\"Sheet1\",startrow=1,startcol=2)\n",
    "\n",
    "        workbook = writer.book\n",
    "        workbook.create_sheet(\"Map\")\n",
    "\n",
    "    workbook = load_workbook(sheetpath)    \n",
    "    sheet1 = workbook[id]\n",
    "\n",
    "    #Format infobox\n",
    "    merged_range = sheet1.merged_cells\n",
    "    for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "        for cell in col[1:2]:\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "        for cell in col[2:7]:\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "            cell.border = border\n",
    "    sheet1.column_dimensions['C'].width = 22 \n",
    "    sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "    #Remove index\n",
    "    for row in sheet1.iter_rows():\n",
    "        for cell in row[:1]:\n",
    "            cell.value = ''\n",
    "            cell.border = border_none\n",
    "    #Format main table header rows\n",
    "    merged_range = sheet1.merged_cells\n",
    "    for col in sheet1.iter_cols(min_col=2):\n",
    "        for cell in col[startrow+1:startrow+2]:\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "            cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "        for cell in col[startrow:startrow+1]:\n",
    "            if cell.coordinate in sheet1.merged_cells:\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "    sheet1.column_dimensions['C'].width = 22 \n",
    "    sheet1.column_dimensions['D'].width = 11   \n",
    "    sheet1.column_dimensions['F'].width = 13\n",
    "    sheet1.column_dimensions['H'].width = 13  \n",
    "    sheet1.column_dimensions['V'].width = 13     \n",
    "\n",
    "    sheet = workbook[\"Map\"]\n",
    "\n",
    "    # Add an image to the sheet\n",
    "    img_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output\\HTML\\Maps_And_CSS\\9908.jpg\"  # Replace with the path to your image\n",
    "    img = Image(img_path)\n",
    "\n",
    "    # Set the position for the image (e.g., 'B2' for cell B2)\n",
    "    sheet.add_image(img, 'B2')\n",
    "\n",
    "    workbook.save(sheetpath)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_CellRange__superset', '__add__', '__and__', '__attrs__', '__class__', '__contains__', '__copy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__elements__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__namespaced__', '__ne__', '__nested__', '__new__', '__or__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_title', '_get_borders', 'bottom', 'bounds', 'cells', 'cols', 'coord', 'expand', 'fill', 'format', 'from_tree', 'idx_base', 'intersection', 'isdisjoint', 'issubset', 'issuperset', 'left', 'max_col', 'max_row', 'min_col', 'min_row', 'namespace', 'right', 'rows', 'shift', 'shrink', 'size', 'start_cell', 'tagname', 'title', 'to_tree', 'top', 'union', 'ws']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(merged_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MultiCellRange [B14:F14 G14:J14 K14:M14 N14:P14 Q14:S14 T14:V14 W14:Y14]>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet1.merged_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_bind_value', '_comment', '_hyperlink', '_style', '_value', 'alignment', 'base_date', 'border', 'check_error', 'check_string', 'col_idx', 'column', 'column_letter', 'comment', 'coordinate', 'data_type', 'encoding', 'fill', 'font', 'has_style', 'hyperlink', 'internal_value', 'is_date', 'number_format', 'offset', 'parent', 'pivotButton', 'protection', 'quotePrefix', 'row', 'style', 'style_id', 'value']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ExcelWriter in module pandas.io.excel._base:\n",
      "\n",
      "class ExcelWriter(builtins.object)\n",
      " |  ExcelWriter(path: 'FilePath | WriteExcelBuffer | ExcelWriter', engine: 'str | None' = None, date_format: 'str | None' = None, datetime_format: 'str | None' = None, mode: 'str' = 'w', storage_options: 'StorageOptions' = None, if_sheet_exists: \"Literal['error', 'new', 'replace', 'overlay'] | None\" = None, engine_kwargs: 'dict | None' = None, **kwargs)\n",
      " |  \n",
      " |  Class for writing DataFrame objects into excel sheets.\n",
      " |  \n",
      " |  Default is to use :\n",
      " |  * xlwt for xls\n",
      " |  * xlsxwriter for xlsx if xlsxwriter is installed otherwise openpyxl\n",
      " |  * odf for ods.\n",
      " |  See DataFrame.to_excel for typical usage.\n",
      " |  \n",
      " |  The writer should be used as a context manager. Otherwise, call `close()` to save\n",
      " |  and close any opened file handles.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  path : str or typing.BinaryIO\n",
      " |      Path to xls or xlsx or ods file.\n",
      " |  engine : str (optional)\n",
      " |      Engine to use for writing. If None, defaults to\n",
      " |      ``io.excel.<extension>.writer``.  NOTE: can only be passed as a keyword\n",
      " |      argument.\n",
      " |  \n",
      " |      .. deprecated:: 1.2.0\n",
      " |  \n",
      " |          As the `xlwt <https://pypi.org/project/xlwt/>`__ package is no longer\n",
      " |          maintained, the ``xlwt`` engine will be removed in a future\n",
      " |          version of pandas.\n",
      " |  \n",
      " |  date_format : str, default None\n",
      " |      Format string for dates written into Excel files (e.g. 'YYYY-MM-DD').\n",
      " |  datetime_format : str, default None\n",
      " |      Format string for datetime objects written into Excel files.\n",
      " |      (e.g. 'YYYY-MM-DD HH:MM:SS').\n",
      " |  mode : {'w', 'a'}, default 'w'\n",
      " |      File mode to use (write or append). Append does not work with fsspec URLs.\n",
      " |  storage_options : dict, optional\n",
      " |      Extra options that make sense for a particular storage connection, e.g.\n",
      " |      host, port, username, password, etc., if using a URL that will\n",
      " |      be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\".\n",
      " |  \n",
      " |      .. versionadded:: 1.2.0\n",
      " |  \n",
      " |  if_sheet_exists : {'error', 'new', 'replace', 'overlay'}, default 'error'\n",
      " |      How to behave when trying to write to a sheet that already\n",
      " |      exists (append mode only).\n",
      " |  \n",
      " |      * error: raise a ValueError.\n",
      " |      * new: Create a new sheet, with a name determined by the engine.\n",
      " |      * replace: Delete the contents of the sheet before writing to it.\n",
      " |      * overlay: Write contents to the existing sheet without removing the old\n",
      " |        contents.\n",
      " |  \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |      .. versionchanged:: 1.4.0\n",
      " |  \n",
      " |         Added ``overlay`` option\n",
      " |  \n",
      " |  engine_kwargs : dict, optional\n",
      " |      Keyword arguments to be passed into the engine. These will be passed to\n",
      " |      the following functions of the respective engines:\n",
      " |  \n",
      " |      * xlsxwriter: ``xlsxwriter.Workbook(file, **engine_kwargs)``\n",
      " |      * openpyxl (write mode): ``openpyxl.Workbook(**engine_kwargs)``\n",
      " |      * openpyxl (append mode): ``openpyxl.load_workbook(file, **engine_kwargs)``\n",
      " |      * odswriter: ``odf.opendocument.OpenDocumentSpreadsheet(**engine_kwargs)``\n",
      " |  \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  **kwargs : dict, optional\n",
      " |      Keyword arguments to be passed into the engine.\n",
      " |  \n",
      " |      .. deprecated:: 1.3.0\n",
      " |  \n",
      " |          Use engine_kwargs instead.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  None\n",
      " |  \n",
      " |  Methods\n",
      " |  -------\n",
      " |  None\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  None of the methods and properties are considered public.\n",
      " |  \n",
      " |  For compatibility with CSV writers, ExcelWriter serializes lists\n",
      " |  and dicts to strings before writing.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Default usage:\n",
      " |  \n",
      " |  >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n",
      " |  >>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n",
      " |  ...     df.to_excel(writer)  # doctest: +SKIP\n",
      " |  \n",
      " |  To write to separate sheets in a single file:\n",
      " |  \n",
      " |  >>> df1 = pd.DataFrame([[\"AAA\", \"BBB\"]], columns=[\"Spam\", \"Egg\"])  # doctest: +SKIP\n",
      " |  >>> df2 = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n",
      " |  >>> with pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n",
      " |  ...     df1.to_excel(writer, sheet_name=\"Sheet1\")  # doctest: +SKIP\n",
      " |  ...     df2.to_excel(writer, sheet_name=\"Sheet2\")  # doctest: +SKIP\n",
      " |  \n",
      " |  You can set the date format or datetime format:\n",
      " |  \n",
      " |  >>> from datetime import date, datetime  # doctest: +SKIP\n",
      " |  >>> df = pd.DataFrame(\n",
      " |  ...     [\n",
      " |  ...         [date(2014, 1, 31), date(1999, 9, 24)],\n",
      " |  ...         [datetime(1998, 5, 26, 23, 33, 4), datetime(2014, 2, 28, 13, 5, 13)],\n",
      " |  ...     ],\n",
      " |  ...     index=[\"Date\", \"Datetime\"],\n",
      " |  ...     columns=[\"X\", \"Y\"],\n",
      " |  ... )  # doctest: +SKIP\n",
      " |  >>> with pd.ExcelWriter(\n",
      " |  ...     \"path_to_file.xlsx\",\n",
      " |  ...     date_format=\"YYYY-MM-DD\",\n",
      " |  ...     datetime_format=\"YYYY-MM-DD HH:MM:SS\"\n",
      " |  ... ) as writer:\n",
      " |  ...     df.to_excel(writer)  # doctest: +SKIP\n",
      " |  \n",
      " |  You can also append to an existing Excel file:\n",
      " |  \n",
      " |  >>> with pd.ExcelWriter(\"path_to_file.xlsx\", mode=\"a\", engine=\"openpyxl\") as writer:\n",
      " |  ...     df.to_excel(writer, sheet_name=\"Sheet3\")  # doctest: +SKIP\n",
      " |  \n",
      " |  Here, the `if_sheet_exists` parameter can be set to replace a sheet if it\n",
      " |  already exists:\n",
      " |  \n",
      " |  >>> with ExcelWriter(\n",
      " |  ...     \"path_to_file.xlsx\",\n",
      " |  ...     mode=\"a\",\n",
      " |  ...     engine=\"openpyxl\",\n",
      " |  ...     if_sheet_exists=\"replace\",\n",
      " |  ... ) as writer:\n",
      " |  ...     df.to_excel(writer, sheet_name=\"Sheet1\")  # doctest: +SKIP\n",
      " |  \n",
      " |  You can also write multiple DataFrames to a single sheet. Note that the\n",
      " |  ``if_sheet_exists`` parameter needs to be set to ``overlay``:\n",
      " |  \n",
      " |  >>> with ExcelWriter(\"path_to_file.xlsx\",\n",
      " |  ...     mode=\"a\",\n",
      " |  ...     engine=\"openpyxl\",\n",
      " |  ...     if_sheet_exists=\"overlay\",\n",
      " |  ... ) as writer:\n",
      " |  ...     df1.to_excel(writer, sheet_name=\"Sheet1\")\n",
      " |  ...     df2.to_excel(writer, sheet_name=\"Sheet1\", startcol=3)  # doctest: +SKIP\n",
      " |  \n",
      " |  You can store Excel file in RAM:\n",
      " |  \n",
      " |  >>> import io\n",
      " |  >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])\n",
      " |  >>> buffer = io.BytesIO()\n",
      " |  >>> with pd.ExcelWriter(buffer) as writer:\n",
      " |  ...     df.to_excel(writer)\n",
      " |  \n",
      " |  You can pack Excel file into zip archive:\n",
      " |  \n",
      " |  >>> import zipfile  # doctest: +SKIP\n",
      " |  >>> df = pd.DataFrame([[\"ABC\", \"XYZ\"]], columns=[\"Foo\", \"Bar\"])  # doctest: +SKIP\n",
      " |  >>> with zipfile.ZipFile(\"path_to_file.zip\", \"w\") as zf:\n",
      " |  ...     with zf.open(\"filename.xlsx\", \"w\") as buffer:\n",
      " |  ...         with pd.ExcelWriter(buffer) as writer:\n",
      " |  ...             df.to_excel(writer)  # doctest: +SKIP\n",
      " |  \n",
      " |  You can specify additional arguments to the underlying engine:\n",
      " |  \n",
      " |  >>> with pd.ExcelWriter(\n",
      " |  ...     \"path_to_file.xlsx\",\n",
      " |  ...     engine=\"xlsxwriter\",\n",
      " |  ...     engine_kwargs={\"options\": {\"nan_inf_to_errors\": True}}\n",
      " |  ... ) as writer:\n",
      " |  ...     df.to_excel(writer)  # doctest: +SKIP\n",
      " |  \n",
      " |  In append mode, ``engine_kwargs`` are passed through to\n",
      " |  openpyxl's ``load_workbook``:\n",
      " |  \n",
      " |  >>> with pd.ExcelWriter(\n",
      " |  ...     \"path_to_file.xlsx\",\n",
      " |  ...     engine=\"openpyxl\",\n",
      " |  ...     mode=\"a\",\n",
      " |  ...     engine_kwargs={\"keep_vba\": True}\n",
      " |  ... ) as writer:\n",
      " |  ...     df.to_excel(writer, sheet_name=\"Sheet2\")  # doctest: +SKIP\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      # Allow use as a contextmanager\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  __fspath__(self)\n",
      " |  \n",
      " |  __init__(self, path: 'FilePath | WriteExcelBuffer | ExcelWriter', engine: 'str | None' = None, date_format: 'str | None' = None, datetime_format: 'str | None' = None, mode: 'str' = 'w', storage_options: 'StorageOptions' = None, if_sheet_exists: 'str | None' = None, engine_kwargs: 'dict | None' = None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  close(self) -> 'None'\n",
      " |      synonym for save, to make it more file-like\n",
      " |  \n",
      " |  save(self) -> 'None'\n",
      " |      Save workbook to disk.\n",
      " |  \n",
      " |  write_cells(self, cells, sheet_name: 'str | None' = None, startrow: 'int' = 0, startcol: 'int' = 0, freeze_panes: 'tuple[int, int] | None' = None) -> 'None'\n",
      " |      Write given formatted cells into Excel an excel sheet\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cells : generator\n",
      " |          cell of formatted data to save to Excel sheet\n",
      " |      sheet_name : str, default None\n",
      " |          Name of Excel sheet, if None, then use self.cur_sheet\n",
      " |      startrow : upper left cell row to dump data frame\n",
      " |      startcol : upper left cell column to dump data frame\n",
      " |      freeze_panes: int tuple of length 2\n",
      " |          contains the bottom-most row and right-most column to freeze\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  check_extension(ext: 'str') -> 'Literal[True]' from abc.ABCMeta\n",
      " |      checks that path's extension against the Writer's supported\n",
      " |      extensions.  If it isn't supported, raises UnsupportedFiletypeError.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, path: 'FilePath | WriteExcelBuffer | ExcelWriter', engine: 'str | None' = None, date_format: 'str | None' = None, datetime_format: 'str | None' = None, mode: 'str' = 'w', storage_options: 'StorageOptions' = None, if_sheet_exists: \"Literal['error', 'new', 'replace', 'overlay'] | None\" = None, engine_kwargs: 'dict | None' = None, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  engine\n",
      " |      Name of engine.\n",
      " |  \n",
      " |  supported_extensions\n",
      " |      Extensions that writer engine supports.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'engine', 'save', 'supported_extensio...\n",
      " |  \n",
      " |  path = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(ExcelWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlsxwriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "In  \u001b[0;34m[35]\u001b[0m:\nLine \u001b[0;34m2\u001b[0m:     node_single_df\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\hloecke\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-mike\\Lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m, in \u001b[0;32m__init__\u001b[0m:\nLine \u001b[0;34m184\u001b[0m:   \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mxlsxwriter\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Workbook\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xlsxwriter'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "node_single_df\n",
    "writer = pd.ExcelWriter('file_name.xlsx', engine='xlsxwriter')\n",
    "workbook = writer.book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample data for the DataFrame\n",
    "# data = {\n",
    "#     ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "#     ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame with MultiIndex columns\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Set names for the levels of the MultiIndex\n",
    "# # df.columns.names = ['Header', 'Subheader']\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# def get_memory_usage(var):\n",
    "#     \"\"\"Returns the memory usage of a variable in bytes.\"\"\"\n",
    "#     return sys.getsizeof(var)\n",
    "\n",
    "# def list_variables_memory():\n",
    "#     # Get the local and global variables\n",
    "#     variables = {**globals(), **locals()}\n",
    "#     total_memory = 0\n",
    "    \n",
    "#     # Print the variables and their memory consumption\n",
    "#     for var_name, var_value in variables.items():\n",
    "#         # Filter out the built-in variables and functions\n",
    "#         if not var_name.startswith('__') and not callable(var_value):\n",
    "#             memory = get_memory_usage(var_value)\n",
    "#             total_memory += memory\n",
    "#             print(f\"Variable: {var_name}, Type: {type(var_value)}, Memory: {memory} bytes\")\n",
    "\n",
    "#     total_memory_mb = total_memory / (1024 * 1024)  # Convert bytes to megabytes\n",
    "#     print(f\"Total Memory Usage: {total_memory_mb:.2f} MB\")\n",
    "#     return total_memory_mb\n",
    "\n",
    "\n",
    "# # Call the function to list variables and their memory consumption\n",
    "# total_memory_usage_mb = list_variables_memory()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
