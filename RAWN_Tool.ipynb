{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-08-08, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model to list the upstream catchments of each manhole (in flow splits, each split direction will both get 100% of the upstream flow). It then uses the MPF sheets to summarise population and residential/ICI areas for each catchment. Aggregated catchment contributions to each manhole are then used as input to RAWN calculations. For each manhole, the upstream catchments are dissolved together and added to a layer 'Node_Catchment'. This is used as input to 'Map Series' (similar to 'Data Driven Pages' in ArcGIS Desktop). This is used to create one jpg image per manhole, highlighting the manhole and all its upstream catchments. The RAWN calculations and images are added to RAWN spreadsheets which are similar in layout to the original RAWN sheets which are derived from Neural Network. One important difference in RAWN calculations to previous is that no peaking factor is allowed to go below 1.5, as per official RAWN guidelines. Peaking factors that reach the minimum are highlighted in yellow. Also a Google Maps link is added to each sheet. The user can choose between adding RAWN calculations as values or Excel formulas. Excel formulas are recommended for full transparency of the calculations to those using the sheets. However, python calculations are maintained in the code to enable future additional outputs (Power BI, HTML etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, numbers\n",
    "from openpyxl.formatting.rule import FormulaRule\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "import subprocess\n",
    "import gc\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "\n",
    "#Create functions for to use SQL with the model sqlite database.\n",
    "\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "\n",
    "#User Input\n",
    "try:\n",
    "\n",
    "    model = 'VSA'\n",
    "\n",
    "    max_steps = 1005 #\n",
    "    use_formula = True\n",
    "    transfer_mixed_area_to_commercial = True\n",
    "\n",
    "    gdb_name = 'RAWN.gdb'\n",
    "    gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "    #Options to skip time consuming steps during debug (by setting to False), must be True during production runs.\n",
    "    run_dissolve = True\n",
    "    run_jpg = True\n",
    "\n",
    "    check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "    #                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "    #                      A more robust check may be developed in the future.\n",
    "\n",
    "\n",
    "    induced_values = [] #Only to be appended to under specific models where required\n",
    "\n",
    "    if model == 'NSSA':   \n",
    "        model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "        pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "        pop_sheet = 'MPF Update 4'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '22602'\n",
    "        line_exclusions = []\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 89\n",
    "        excluded_acronyms_csv = ''\n",
    "\n",
    "    if model == 'FSA':   \n",
    "        model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\01_MASTER_MODEL\\MODEL\\FSA_Base_2021pop.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'MPF Update 17'\n",
    "        scenario = '2030_Network'\n",
    "        wwtp_muid = '1162'\n",
    "        line_exclusions = ['GoldenEar_Dummy_Pump','GoldenEarSSOLink','GoldenEar_SSO_Tank_Cell_Dummy_Link3','MH35_Orifice','44473']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 154\n",
    "        excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model\\FSA_Excluded_Acronyms.csv\"\n",
    "        induced_values.append(['84674','Area_Ind',257.3]) #Vancouver landfill, equivalent area to get 520 L/s PWWF.\n",
    "        \n",
    "    if model == 'VSA':   \n",
    "        model_output_folder = r\"J:\\SEWER_AREA_MODELS\\VSA\\04_ANALYSIS_WORK\\79. RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\VSA\\03_SIMULATION_WORK\\Key_Flow_HGL_GIS_Sim_VSA\\MIKE+_Import\\VSA_V304.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\VSA\\02_MODEL_COMPONENTS\\03_NETWORKS\\11. NETWORK_UPDATE\\O. V304 Updates\\VSA_Master_Population_File.xlsx\"\n",
    "        pop_sheet = 'Export'\n",
    "        scenario = 'Base'\n",
    "        wwtp_muid = '7478'\n",
    "        line_exclusions = ['51833','51834','51835','FS-241','FS-253','SADR082 SALIB DIV TO VICT','Victoria_Drive_Weir']\n",
    "        years = [2060,2070,2080,2090,2100]\n",
    "        model_version = 304\n",
    "        excluded_acronyms_csv = ''\n",
    "        \n",
    "    if model == 'LISA':   \n",
    "        model_output_folder = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "        model_path = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\\Lisa_Base.sqlite\"\n",
    "        pop_book = r\"J:\\SEWER_AREA_MODELS\\LISA\\04_ANALYSIS_WORK\\RAWN_From_Model\\LISA_Master_Population.xlsx\"\n",
    "        pop_sheet = 'Export'\n",
    "        scenario = '2021_Network'\n",
    "        wwtp_muid = 'WWTP_Outlet'\n",
    "        line_exclusions = []\n",
    "        years = [2060,2070]\n",
    "        model_version = 304\n",
    "        excluded_acronyms_csv = ''   \n",
    "        \n",
    "\n",
    "    #Do not change the lines below\n",
    "    sewer_area = model \n",
    "    global_output_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder \n",
    "    \n",
    "    #Warn user if run_dissolve or run_jpg is False\n",
    "    if run_dissolve == False or run_jpg == False:\n",
    "        if MessageBox(None, b'run_dissolve and/or run_jpg is set to False.\\n\\nThis is only allowed in special cases!\\n\\nContinue?', b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please set both run_dissolve and run_jpg to True.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 3', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "    \n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    #Items: [Keyword (upper Excel header),Type ('lower Excel header'),Average(lower Excel header),Unit flow cell address, quantifyer column]\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)','$D$3','H']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$4','K']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$5','N']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)','$D$6','Q']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)','$D$7','T']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)','$D$7','T']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid == 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + str(altid)\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "    \n",
    "    sql = \"SELECT msm_Catchcon.catchid AS Catchment, msm_Catchcon.nodeid AS Connected_Node FROM msm_Catchcon INNER JOIN msm_catchment \"\n",
    "    sql += \"ON msm_Catchcon.catchid = msm_catchment.muid WHERE msm_Catchment.nettypeno <> 2 AND msm_Catchcon.Active = 1 AND msm_catchment.Active = 1 \"\n",
    "    sql += \"AND msm_catchment.enabled = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1 AND enabled = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1 AND enabled = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1 AND enabled = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1 AND enabled = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1 AND enabled = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "    \n",
    "    lines = lines[~lines['MUID'].isin(line_exclusions)]\n",
    "    \n",
    "    excluded_acronyms = []\n",
    "    if excluded_acronyms_csv != '':\n",
    "        excluded_acronyms = pd.read_csv(excluded_acronyms_csv)\n",
    "        excluded_acronyms = list(excluded_acronyms.iloc[:, 0])\n",
    "\n",
    "    sql = \"SELECT muid, UPPER(acronym) AS acronym, UPPER(owner) AS owner, UPPER(assetname) AS assetname, to_outfall FROM msm_Node WHERE active = 1 AND enabled = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(((node_id_df.assetname.str[:2]=='MH') & (node_id_df.acronym.notna()) & (node_id_df.acronym!='') & \n",
    "        (node_id_df.to_outfall!=1) & (node_id_df.owner=='GV') & (~node_id_df.acronym.isin(excluded_acronyms))) | (node_id_df.muid==wwtp_muid))]\n",
    "    \n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df.loc[node_id_df['Node'] == wwtp_muid, 'ID'] = 'WWTP'\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "    duplicate_ids = node_id_df[node_id_df.duplicated('ID', keep=False)]\n",
    "    node_id_df.loc[duplicate_ids.index, 'ID'] = node_id_df['ID'] + '_(' + node_id_df['Node'] + ')'\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "\n",
    "#Import population\n",
    "\n",
    "try:\n",
    "    def warning_message(message):\n",
    "        if MessageBox(None, message.encode('utf-8'), b'Warning', 4) == 7:\n",
    "            MessageBox(None, b\"Please report the issue to the Master Population File admin.\", b'Info', 0)\n",
    "            raise ValueError(\"The user chose to end the execution.\")\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df.fillna(0, inplace=True) #Fill NA with 0 or the sum of all will be NA\n",
    "    \n",
    "    if transfer_mixed_area_to_commercial:\n",
    "        pop_df.Area_Com = pop_df.Area_Com + pop_df.Area_Mixed\n",
    "        pop_df.Area_Mixed = 0\n",
    "    \n",
    "    for induced_value in induced_values:\n",
    "        catchment = induced_value[0]\n",
    "        column = induced_value[1] \n",
    "        value = induced_value[2]\n",
    "        pop_df.loc[pop_df.Catchment == catchment,column] += value\n",
    "        \n",
    "        \n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "        \n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    #Check if Pop_Total = 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (sum of all rows)\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "        message = f\"Warning. The sum of 'Population' in 'Pop_Total' ({pop_sum_total_col:,}) is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' ({pop_sum_sub_cols:,}).\"\n",
    "        message += \"\\n\\n'Pop_Total' will be used.\\n\\nContinue?\"\n",
    "        warning_message(message)\n",
    "     \n",
    "    #Check if some rows have population but 0 area\n",
    "    res_area_check_df = pop_df[(pop_df.Population > 0) & (pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed == 0)]\n",
    "    if len(res_area_check_df) > 0:\n",
    "        warning_message(f\"{len(res_area_check_df):,} out of {len(pop_df):,} rows have population but 0 residential area.\\n\\nPrint dataframe 'res_area_check_df' to see all.\\n\\nContinue?\")\n",
    "    \n",
    "    #Check is some catchment/year combinations are not found.\n",
    "    catchment_years = []\n",
    "    for muid in list(catchments.Catchment.unique()):\n",
    "        for year in years:\n",
    "            catchment_years.append([muid,year])\n",
    "    catchment_year_df = pd.DataFrame(catchment_years,columns=(['Catchment','Year']))\n",
    "    merged = catchment_year_df.merge(pop_df[['Catchment', 'Year']], on=['Catchment', 'Year'], how='left', indicator=True)\n",
    "    not_founds = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    if len(not_founds) > 0:\n",
    "        message = \"The following catchment/year combinations are not found:\\n\\n\"\n",
    "        for index, row in not_founds[:20].iterrows():\n",
    "            message += row[0] + ', ' + str(row[1]) + '.\\n'\n",
    "        if len(not_founds) > 20:\n",
    "             message += \"and more, print variable 'not_founds' to see the rest.\"                                                 \n",
    "        message += '\\n\\nContinue?'\n",
    "        warning_message(message)\n",
    "                                                                      \n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "        \n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]  \n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                \n",
    "#                 catchment_mus.append(catchment)\n",
    "#                 print('skipped for ' + catchment)\n",
    "#                 break\n",
    "                \n",
    "                \n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Last node traced is '\" + node + \"'\")\n",
    "                \n",
    "                \n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl']:\n",
    "                    adwf += avg_flow    \n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)),1.5) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*0.8,1.5)*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)),1.5) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    \n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = np.maximum(\n",
    "        0.8 * (\n",
    "            1 + 14 / (\n",
    "                4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5\n",
    "            )\n",
    "        ) * np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ), \n",
    "        1.5\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    \n",
    "    node_df[('FLOWS','PWWF (L/s)')] = node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] + node_df[('COMMERCIAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] + node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] + node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending msm_CatchCon\n",
      "Appending msm_Catchment\n",
      "Appending msm_Link\n",
      "Appending msm_Node\n",
      "Appending msm_Pump\n",
      "Appending msm_Weir\n",
      "Appending msm_Orifice\n",
      "Appending msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "\n",
    "#Import GIS from the model\n",
    "#Note that if the RAWN database does not already exist with the layers inside, then it will be created \n",
    "#but in this casethe map symbology will be reset and Map Series reset as well, creating extra work.\n",
    "#An extra word of caution: The first import of e.g. msm_Catchment sets the maximum extend, meaning if you import NSSA in a fresh database,\n",
    "#then later import FSA, then many catchments will not be imported due to being outside the extend of the old import.\n",
    "#In the current database, an initial model with added elements outside the extend of all MV models was used to prime the layer. \n",
    "#If the database is not maintained, this will need to be done again.\n",
    "#Efforts to programmatically increase the layer extend were unsuccessful but may later get resolved.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(out_path + '\\\\' + layer):\n",
    "            print('Appending ' + layer)\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:  \n",
    "            print('Creating ' + layer)\n",
    "\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer + '_Test')\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        arcpy.Project_management('msm_Node', 'msm_Node_Google',arcpy.SpatialReference(4326))\n",
    "        centroids = arcpy.da.FeatureClassToNumPyArray('msm_Node_Google', (\"MUID\",\"SHAPE@X\",\"SHAPE@Y\"))\n",
    "        centroids = centroids.tolist()\n",
    "        centroids_df = pd.DataFrame(centroids, columns =['MUID','X','Y'])\n",
    "        centroids_df.set_index('MUID',inplace=True)\n",
    "\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "\n",
    "#Create merge_df (realizing the terms 'merge' and 'dissolve' are used here but they mean the same), \n",
    "#minimizing the number of computation heavy dissolves that need to be done.\n",
    "#It prevents the duplicate effort of merging the same catchments together multiple times,\n",
    "#by moving downstream and reusing upstream merges for downstream merges.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_244, 0 of 268 at time 2024-08-08 15:51:31.011506\n",
      "Dissolving for Merge_ID_182, 1 of 268 at time 2024-08-08 15:52:00.174187\n",
      "Dissolving for Merge_ID_257, 2 of 268 at time 2024-08-08 15:52:26.070394\n",
      "Dissolving for Merge_ID_33, 3 of 268 at time 2024-08-08 15:52:50.983187\n",
      "Dissolving for Merge_ID_215, 4 of 268 at time 2024-08-08 15:53:52.655612\n",
      "Dissolving for Merge_ID_224, 5 of 268 at time 2024-08-08 15:54:28.839561\n",
      "Dissolving for Merge_ID_46, 6 of 268 at time 2024-08-08 15:54:47.744858\n",
      "Dissolving for Merge_ID_227, 7 of 268 at time 2024-08-08 15:55:07.026499\n",
      "Dissolving for Merge_ID_211, 8 of 268 at time 2024-08-08 15:56:30.035444\n",
      "Dissolving for Merge_ID_229, 9 of 268 at time 2024-08-08 15:56:47.423353\n",
      "Dissolving for Merge_ID_74, 10 of 268 at time 2024-08-08 15:57:17.631991\n",
      "Dissolving for Merge_ID_259, 11 of 268 at time 2024-08-08 15:57:34.778195\n",
      "Dissolving for Merge_ID_101, 12 of 268 at time 2024-08-08 15:57:50.606677\n",
      "Dissolving for Merge_ID_232, 13 of 268 at time 2024-08-08 15:58:24.670842\n",
      "Dissolving for Merge_ID_209, 14 of 268 at time 2024-08-08 15:58:41.094869\n",
      "Dissolving for Merge_ID_239, 15 of 268 at time 2024-08-08 15:59:03.044951\n",
      "Dissolving for Merge_ID_170, 16 of 268 at time 2024-08-08 15:59:18.373976\n",
      "Dissolving for Merge_ID_106, 17 of 268 at time 2024-08-08 15:59:33.817191\n",
      "Dissolving for Merge_ID_8, 18 of 268 at time 2024-08-08 15:59:48.793893\n",
      "Dissolving for Merge_ID_108, 19 of 268 at time 2024-08-08 16:00:08.474900\n",
      "Dissolving for Merge_ID_254, 20 of 268 at time 2024-08-08 16:01:58.207295\n",
      "Dissolving for Merge_ID_97, 21 of 268 at time 2024-08-08 16:02:26.053772\n",
      "Dissolving for Merge_ID_256, 22 of 268 at time 2024-08-08 16:02:48.125966\n",
      "Dissolving for Merge_ID_206, 23 of 268 at time 2024-08-08 16:03:01.804480\n",
      "Dissolving for Merge_ID_118, 24 of 268 at time 2024-08-08 16:03:19.800993\n",
      "Dissolving for Merge_ID_172, 25 of 268 at time 2024-08-08 16:03:41.741352\n",
      "Dissolving for Merge_ID_104, 26 of 268 at time 2024-08-08 16:05:53.418534\n",
      "Dissolving for Merge_ID_125, 27 of 268 at time 2024-08-08 16:06:08.658676\n",
      "Dissolving for Merge_ID_107, 28 of 268 at time 2024-08-08 16:06:42.544118\n",
      "Dissolving for Merge_ID_59, 29 of 268 at time 2024-08-08 16:07:20.901710\n",
      "Dissolving for Merge_ID_58, 30 of 268 at time 2024-08-08 16:07:37.389024\n",
      "Dissolving for Merge_ID_32, 31 of 268 at time 2024-08-08 16:08:07.278758\n",
      "Dissolving for Merge_ID_139, 32 of 268 at time 2024-08-08 16:08:40.532297\n",
      "Dissolving for Merge_ID_221, 33 of 268 at time 2024-08-08 16:09:17.450073\n",
      "Dissolving for Merge_ID_133, 34 of 268 at time 2024-08-08 16:09:32.503846\n",
      "Dissolving for Merge_ID_20, 35 of 268 at time 2024-08-08 16:10:07.380755\n",
      "Dissolving for Merge_ID_198, 36 of 268 at time 2024-08-08 16:10:22.585666\n",
      "Dissolving for Merge_ID_193, 37 of 268 at time 2024-08-08 16:10:48.351239\n",
      "Dissolving for Merge_ID_261, 38 of 268 at time 2024-08-08 16:11:12.775585\n",
      "Dissolving for Merge_ID_4, 39 of 268 at time 2024-08-08 16:11:43.702881\n",
      "Dissolving for Merge_ID_60, 40 of 268 at time 2024-08-08 16:12:04.219652\n",
      "Dissolving for Merge_ID_53, 41 of 268 at time 2024-08-08 16:12:24.974641\n",
      "Dissolving for Merge_ID_0, 42 of 268 at time 2024-08-08 16:12:46.098968\n",
      "Dissolving for Merge_ID_69, 43 of 268 at time 2024-08-08 16:13:16.265567\n",
      "Dissolving for Merge_ID_11, 44 of 268 at time 2024-08-08 16:13:59.801399\n",
      "Dissolving for Merge_ID_19, 45 of 268 at time 2024-08-08 16:14:18.760745\n",
      "Dissolving for Merge_ID_45, 46 of 268 at time 2024-08-08 16:15:39.234371\n",
      "Dissolving for Merge_ID_31, 47 of 268 at time 2024-08-08 16:16:11.610992\n",
      "Dissolving for Merge_ID_126, 48 of 268 at time 2024-08-08 16:16:26.487603\n",
      "Dissolving for Merge_ID_90, 49 of 268 at time 2024-08-08 16:16:41.186051\n",
      "Dissolving for Merge_ID_21, 50 of 268 at time 2024-08-08 16:17:52.550342\n",
      "Dissolving for Merge_ID_123, 51 of 268 at time 2024-08-08 16:18:13.341364\n",
      "Dissolving for Merge_ID_129, 52 of 268 at time 2024-08-08 16:19:07.191632\n",
      "Dissolving for Merge_ID_205, 53 of 268 at time 2024-08-08 16:20:34.135339\n",
      "Dissolving for Merge_ID_262, 54 of 268 at time 2024-08-08 16:21:04.701655\n",
      "Dissolving for Merge_ID_68, 55 of 268 at time 2024-08-08 16:21:26.760090\n",
      "Dissolving for Merge_ID_213, 56 of 268 at time 2024-08-08 16:21:59.196139\n",
      "Dissolving for Merge_ID_137, 57 of 268 at time 2024-08-08 16:22:24.921490\n",
      "Dissolving for Merge_ID_54, 58 of 268 at time 2024-08-08 16:22:52.568101\n",
      "Dissolving for Merge_ID_62, 59 of 268 at time 2024-08-08 16:23:27.671621\n",
      "Dissolving for Merge_ID_225, 60 of 268 at time 2024-08-08 16:23:48.825218\n",
      "Dissolving for Merge_ID_238, 61 of 268 at time 2024-08-08 16:24:13.266860\n",
      "Dissolving for Merge_ID_154, 62 of 268 at time 2024-08-08 16:24:29.466868\n",
      "Dissolving for Merge_ID_159, 63 of 268 at time 2024-08-08 16:25:07.321937\n",
      "Dissolving for Merge_ID_103, 64 of 268 at time 2024-08-08 16:25:54.138917\n",
      "Dissolving for Merge_ID_5, 65 of 268 at time 2024-08-08 16:26:09.727179\n",
      "Dissolving for Merge_ID_78, 66 of 268 at time 2024-08-08 16:26:46.591907\n",
      "Dissolving for Merge_ID_163, 67 of 268 at time 2024-08-08 16:27:19.921400\n",
      "Dissolving for Merge_ID_23, 68 of 268 at time 2024-08-08 16:27:35.057259\n",
      "Dissolving for Merge_ID_80, 69 of 268 at time 2024-08-08 16:27:53.402043\n",
      "Dissolving for Merge_ID_191, 70 of 268 at time 2024-08-08 16:28:12.494511\n",
      "Dissolving for Merge_ID_223, 71 of 268 at time 2024-08-08 16:28:40.003679\n",
      "Dissolving for Merge_ID_130, 72 of 268 at time 2024-08-08 16:28:55.030427\n",
      "Dissolving for Merge_ID_127, 73 of 268 at time 2024-08-08 16:29:13.662474\n",
      "Dissolving for Merge_ID_161, 74 of 268 at time 2024-08-08 16:29:27.724339\n",
      "Dissolving for Merge_ID_192, 75 of 268 at time 2024-08-08 16:29:41.831246\n",
      "Dissolving for Merge_ID_248, 76 of 268 at time 2024-08-08 16:30:08.061244\n",
      "Dissolving for Merge_ID_168, 77 of 268 at time 2024-08-08 16:30:46.088035\n",
      "Dissolving for Merge_ID_189, 78 of 268 at time 2024-08-08 16:30:59.675466\n",
      "Dissolving for Merge_ID_98, 79 of 268 at time 2024-08-08 16:31:13.519132\n",
      "Dissolving for Merge_ID_171, 80 of 268 at time 2024-08-08 16:31:27.335772\n",
      "Dissolving for Merge_ID_173, 81 of 268 at time 2024-08-08 16:31:53.472685\n",
      "Dissolving for Merge_ID_91, 82 of 268 at time 2024-08-08 16:32:32.270192\n",
      "Dissolving for Merge_ID_44, 83 of 268 at time 2024-08-08 16:32:46.264996\n",
      "Dissolving for Merge_ID_17, 84 of 268 at time 2024-08-08 16:33:04.150360\n",
      "Dissolving for Merge_ID_200, 85 of 268 at time 2024-08-08 16:33:17.726781\n",
      "Dissolving for Merge_ID_14, 86 of 268 at time 2024-08-08 16:33:31.724588\n",
      "Dissolving for Merge_ID_88, 87 of 268 at time 2024-08-08 16:33:45.376078\n",
      "Dissolving for Merge_ID_83, 88 of 268 at time 2024-08-08 16:34:03.031230\n",
      "Dissolving for Merge_ID_226, 89 of 268 at time 2024-08-08 16:34:21.282929\n",
      "Dissolving for Merge_ID_160, 90 of 268 at time 2024-08-08 16:34:54.662468\n",
      "Dissolving for Merge_ID_51, 91 of 268 at time 2024-08-08 16:35:08.239890\n",
      "Dissolving for Merge_ID_141, 92 of 268 at time 2024-08-08 16:35:21.596110\n",
      "Dissolving for Merge_ID_26, 93 of 268 at time 2024-08-08 16:35:36.138415\n",
      "Dissolving for Merge_ID_258, 94 of 268 at time 2024-08-08 16:35:54.613318\n",
      "Dissolving for Merge_ID_40, 95 of 268 at time 2024-08-08 16:36:12.196405\n",
      "Dissolving for Merge_ID_252, 96 of 268 at time 2024-08-08 16:36:30.275946\n",
      "Dissolving for Merge_ID_7, 97 of 268 at time 2024-08-08 16:36:53.225943\n",
      "Dissolving for Merge_ID_169, 98 of 268 at time 2024-08-08 16:37:38.308874\n",
      "Dissolving for Merge_ID_71, 99 of 268 at time 2024-08-08 16:37:59.671692\n",
      "Dissolving for Merge_ID_174, 100 of 268 at time 2024-08-08 16:38:17.593318\n",
      "Dissolving for Merge_ID_237, 101 of 268 at time 2024-08-08 16:38:43.403262\n",
      "Dissolving for Merge_ID_18, 102 of 268 at time 2024-08-08 16:39:01.222793\n",
      "Dissolving for Merge_ID_188, 103 of 268 at time 2024-08-08 16:39:26.774497\n",
      "Dissolving for Merge_ID_263, 104 of 268 at time 2024-08-08 16:39:40.232983\n",
      "Dissolving for Merge_ID_264, 105 of 268 at time 2024-08-08 16:39:53.700476\n",
      "Dissolving for Merge_ID_111, 106 of 268 at time 2024-08-08 16:40:11.481972\n",
      "Dissolving for Merge_ID_9, 107 of 268 at time 2024-08-08 16:40:45.271318\n",
      "Dissolving for Merge_ID_65, 108 of 268 at time 2024-08-08 16:41:11.118297\n",
      "Dissolving for Merge_ID_148, 109 of 268 at time 2024-08-08 16:41:28.911804\n",
      "Dissolving for Merge_ID_56, 110 of 268 at time 2024-08-08 16:41:43.103970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_230, 111 of 268 at time 2024-08-08 16:42:00.831415\n",
      "Dissolving for Merge_ID_136, 112 of 268 at time 2024-08-08 16:42:14.284896\n",
      "Dissolving for Merge_ID_66, 113 of 268 at time 2024-08-08 16:42:28.471012\n",
      "Dissolving for Merge_ID_121, 114 of 268 at time 2024-08-08 16:43:06.736021\n",
      "Dissolving for Merge_ID_220, 115 of 268 at time 2024-08-08 16:43:20.505619\n",
      "Dissolving for Merge_ID_180, 116 of 268 at time 2024-08-08 16:43:34.707612\n",
      "Dissolving for Merge_ID_22, 117 of 268 at time 2024-08-08 16:43:48.753463\n",
      "Dissolving for Merge_ID_204, 118 of 268 at time 2024-08-08 16:44:07.041194\n",
      "Dissolving for Merge_ID_245, 119 of 268 at time 2024-08-08 16:44:20.652648\n",
      "Dissolving for Merge_ID_67, 120 of 268 at time 2024-08-08 16:44:38.656119\n",
      "Dissolving for Merge_ID_3, 121 of 268 at time 2024-08-08 16:45:12.619192\n",
      "Dissolving for Merge_ID_55, 122 of 268 at time 2024-08-08 16:45:30.413472\n",
      "Dissolving for Merge_ID_235, 123 of 268 at time 2024-08-08 16:45:52.614784\n",
      "Dissolving for Merge_ID_42, 124 of 268 at time 2024-08-08 16:46:26.878132\n",
      "Dissolving for Merge_ID_57, 125 of 268 at time 2024-08-08 16:46:52.917956\n",
      "Dissolving for Merge_ID_207, 126 of 268 at time 2024-08-08 16:47:07.227048\n",
      "Dissolving for Merge_ID_208, 127 of 268 at time 2024-08-08 16:47:21.460070\n",
      "Dissolving for Merge_ID_142, 128 of 268 at time 2024-08-08 16:47:35.440861\n",
      "Dissolving for Merge_ID_138, 129 of 268 at time 2024-08-08 16:48:30.133939\n",
      "Dissolving for Merge_ID_119, 130 of 268 at time 2024-08-08 16:48:52.373286\n",
      "Dissolving for Merge_ID_81, 131 of 268 at time 2024-08-08 16:49:14.721733\n",
      "Dissolving for Merge_ID_132, 132 of 268 at time 2024-08-08 16:49:32.593084\n",
      "Dissolving for Merge_ID_63, 133 of 268 at time 2024-08-08 16:49:51.087004\n",
      "Dissolving for Merge_ID_79, 134 of 268 at time 2024-08-08 16:50:09.521870\n",
      "Dissolving for Merge_ID_47, 135 of 268 at time 2024-08-08 16:50:24.451530\n",
      "Dissolving for Merge_ID_202, 136 of 268 at time 2024-08-08 16:50:46.891060\n",
      "Dissolving for Merge_ID_2, 137 of 268 at time 2024-08-08 16:51:09.442692\n",
      "Dissolving for Merge_ID_61, 138 of 268 at time 2024-08-08 16:51:23.684722\n",
      "Dissolving for Merge_ID_219, 139 of 268 at time 2024-08-08 16:51:38.047863\n",
      "Dissolving for Merge_ID_187, 140 of 268 at time 2024-08-08 16:52:13.515313\n",
      "Dissolving for Merge_ID_197, 141 of 268 at time 2024-08-08 16:52:27.820412\n",
      "Dissolving for Merge_ID_241, 142 of 268 at time 2024-08-08 16:52:46.222248\n",
      "Dissolving for Merge_ID_236, 143 of 268 at time 2024-08-08 16:53:08.390529\n",
      "Dissolving for Merge_ID_89, 144 of 268 at time 2024-08-08 16:53:27.236772\n",
      "Dissolving for Merge_ID_117, 145 of 268 at time 2024-08-08 16:53:41.230575\n",
      "Dissolving for Merge_ID_201, 146 of 268 at time 2024-08-08 16:54:11.521288\n",
      "Dissolving for Merge_ID_41, 147 of 268 at time 2024-08-08 16:54:25.654219\n",
      "Dissolving for Merge_ID_75, 148 of 268 at time 2024-08-08 16:54:48.297163\n",
      "Dissolving for Merge_ID_146, 149 of 268 at time 2024-08-08 16:55:11.334500\n",
      "Dissolving for Merge_ID_105, 150 of 268 at time 2024-08-08 16:55:25.856950\n",
      "Dissolving for Merge_ID_267, 151 of 268 at time 2024-08-08 16:55:52.638756\n",
      "Dissolving for Merge_ID_64, 152 of 268 at time 2024-08-08 16:56:39.905534\n",
      "Dissolving for Merge_ID_77, 153 of 268 at time 2024-08-08 16:56:54.467021\n",
      "Dissolving for Merge_ID_49, 154 of 268 at time 2024-08-08 16:57:08.727229\n",
      "Dissolving for Merge_ID_114, 155 of 268 at time 2024-08-08 16:57:27.319460\n",
      "Dissolving for Merge_ID_36, 156 of 268 at time 2024-08-08 16:57:42.010066\n",
      "Dissolving for Merge_ID_25, 157 of 268 at time 2024-08-08 16:58:08.940008\n",
      "Dissolving for Merge_ID_184, 158 of 268 at time 2024-08-08 16:58:23.279289\n",
      "Dissolving for Merge_ID_183, 159 of 268 at time 2024-08-08 16:58:37.673621\n",
      "Dissolving for Merge_ID_13, 160 of 268 at time 2024-08-08 16:58:52.162041\n",
      "Dissolving for Merge_ID_120, 161 of 268 at time 2024-08-08 16:59:23.760307\n",
      "Dissolving for Merge_ID_128, 162 of 268 at time 2024-08-08 16:59:54.911479\n",
      "Dissolving for Merge_ID_181, 163 of 268 at time 2024-08-08 17:00:13.747712\n",
      "Dissolving for Merge_ID_92, 164 of 268 at time 2024-08-08 17:00:32.404782\n",
      "Dissolving for Merge_ID_52, 165 of 268 at time 2024-08-08 17:00:51.622900\n",
      "Dissolving for Merge_ID_249, 166 of 268 at time 2024-08-08 17:01:39.258483\n",
      "Dissolving for Merge_ID_153, 167 of 268 at time 2024-08-08 17:02:23.208693\n",
      "Dissolving for Merge_ID_112, 168 of 268 at time 2024-08-08 17:03:27.366391\n",
      "Dissolving for Merge_ID_122, 169 of 268 at time 2024-08-08 17:03:46.031468\n",
      "Dissolving for Merge_ID_242, 170 of 268 at time 2024-08-08 17:04:17.491251\n",
      "Dissolving for Merge_ID_179, 171 of 268 at time 2024-08-08 17:04:41.164910\n",
      "Dissolving for Merge_ID_50, 172 of 268 at time 2024-08-08 17:05:00.173301\n",
      "Dissolving for Merge_ID_167, 173 of 268 at time 2024-08-08 17:05:23.037220\n",
      "Dissolving for Merge_ID_214, 174 of 268 at time 2024-08-08 17:05:38.891725\n",
      "Dissolving for Merge_ID_166, 175 of 268 at time 2024-08-08 17:05:53.299907\n",
      "Dissolving for Merge_ID_199, 176 of 268 at time 2024-08-08 17:06:16.406047\n",
      "Dissolving for Merge_ID_85, 177 of 268 at time 2024-08-08 17:06:41.213744\n",
      "Dissolving for Merge_ID_29, 178 of 268 at time 2024-08-08 17:06:55.874157\n",
      "Dissolving for Merge_ID_212, 179 of 268 at time 2024-08-08 17:07:19.397679\n",
      "Dissolving for Merge_ID_217, 180 of 268 at time 2024-08-08 17:07:43.264515\n",
      "Dissolving for Merge_ID_222, 181 of 268 at time 2024-08-08 17:08:10.934830\n",
      "Dissolving for Merge_ID_99, 182 of 268 at time 2024-08-08 17:08:33.640604\n",
      "Dissolving for Merge_ID_100, 183 of 268 at time 2024-08-08 17:08:48.574267\n",
      "Dissolving for Merge_ID_253, 184 of 268 at time 2024-08-08 17:09:12.301976\n",
      "Dissolving for Merge_ID_76, 185 of 268 at time 2024-08-08 17:09:27.008431\n",
      "Dissolving for Merge_ID_113, 186 of 268 at time 2024-08-08 17:09:42.349466\n",
      "Dissolving for Merge_ID_231, 187 of 268 at time 2024-08-08 17:10:10.085842\n",
      "Dissolving for Merge_ID_175, 188 of 268 at time 2024-08-08 17:10:25.078559\n",
      "Dissolving for Merge_ID_156, 189 of 268 at time 2024-08-08 17:10:43.704601\n",
      "Dissolving for Merge_ID_93, 190 of 268 at time 2024-08-08 17:11:11.071639\n",
      "Dissolving for Merge_ID_243, 191 of 268 at time 2024-08-08 17:11:30.688586\n",
      "Dissolving for Merge_ID_228, 192 of 268 at time 2024-08-08 17:11:45.719506\n",
      "Dissolving for Merge_ID_84, 193 of 268 at time 2024-08-08 17:12:38.782716\n",
      "Dissolving for Merge_ID_152, 194 of 268 at time 2024-08-08 17:13:02.456667\n",
      "Dissolving for Merge_ID_265, 195 of 268 at time 2024-08-08 17:13:17.161301\n",
      "Dissolving for Merge_ID_24, 196 of 268 at time 2024-08-08 17:13:32.301339\n",
      "Dissolving for Merge_ID_151, 197 of 268 at time 2024-08-08 17:13:47.638559\n",
      "Dissolving for Merge_ID_35, 198 of 268 at time 2024-08-08 17:14:03.111906\n",
      "Dissolving for Merge_ID_190, 199 of 268 at time 2024-08-08 17:14:26.503595\n",
      "Dissolving for Merge_ID_155, 200 of 268 at time 2024-08-08 17:14:49.943328\n",
      "Dissolving for Merge_ID_70, 201 of 268 at time 2024-08-08 17:15:04.735043\n",
      "Dissolving for Merge_ID_266, 202 of 268 at time 2024-08-08 17:15:19.570799\n",
      "Dissolving for Merge_ID_109, 203 of 268 at time 2024-08-08 17:15:34.551689\n",
      "Dissolving for Merge_ID_131, 204 of 268 at time 2024-08-08 17:15:53.631379\n",
      "Dissolving for Merge_ID_162, 205 of 268 at time 2024-08-08 17:16:12.934277\n",
      "Dissolving for Merge_ID_203, 206 of 268 at time 2024-08-08 17:16:32.443362\n",
      "Dissolving for Merge_ID_177, 207 of 268 at time 2024-08-08 17:17:01.591029\n",
      "Dissolving for Merge_ID_240, 208 of 268 at time 2024-08-08 17:17:16.505675\n",
      "Dissolving for Merge_ID_246, 209 of 268 at time 2024-08-08 17:17:40.228414\n",
      "Dissolving for Merge_ID_94, 210 of 268 at time 2024-08-08 17:17:55.501892\n",
      "Dissolving for Merge_ID_178, 211 of 268 at time 2024-08-08 17:18:27.567228\n",
      "Dissolving for Merge_ID_96, 212 of 268 at time 2024-08-08 17:19:27.407977\n",
      "Dissolving for Merge_ID_95, 213 of 268 at time 2024-08-08 17:19:51.431957\n",
      "Dissolving for Merge_ID_30, 214 of 268 at time 2024-08-08 17:20:10.860732\n",
      "Dissolving for Merge_ID_82, 215 of 268 at time 2024-08-08 17:20:25.680291\n",
      "Dissolving for Merge_ID_144, 216 of 268 at time 2024-08-08 17:20:40.816139\n",
      "Dissolving for Merge_ID_143, 217 of 268 at time 2024-08-08 17:21:00.221893\n",
      "Dissolving for Merge_ID_86, 218 of 268 at time 2024-08-08 17:21:15.391772\n",
      "Dissolving for Merge_ID_87, 219 of 268 at time 2024-08-08 17:21:43.582564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_233, 220 of 268 at time 2024-08-08 17:22:07.133111\n",
      "Dissolving for Merge_ID_134, 221 of 268 at time 2024-08-08 17:22:22.301989\n",
      "Dissolving for Merge_ID_195, 222 of 268 at time 2024-08-08 17:22:42.011021\n",
      "Dissolving for Merge_ID_43, 223 of 268 at time 2024-08-08 17:23:01.592937\n",
      "Dissolving for Merge_ID_165, 224 of 268 at time 2024-08-08 17:23:25.375696\n",
      "Dissolving for Merge_ID_185, 225 of 268 at time 2024-08-08 17:24:19.750443\n",
      "Dissolving for Merge_ID_1, 226 of 268 at time 2024-08-08 17:24:35.039432\n",
      "Dissolving for Merge_ID_247, 227 of 268 at time 2024-08-08 17:24:50.142249\n",
      "Dissolving for Merge_ID_10, 228 of 268 at time 2024-08-08 17:25:10.737092\n",
      "Dissolving for Merge_ID_15, 229 of 268 at time 2024-08-08 17:25:26.180221\n",
      "Dissolving for Merge_ID_250, 230 of 268 at time 2024-08-08 17:25:41.695416\n",
      "Dissolving for Merge_ID_115, 231 of 268 at time 2024-08-08 17:26:01.277331\n",
      "Dissolving for Merge_ID_164, 232 of 268 at time 2024-08-08 17:26:21.023397\n",
      "Dissolving for Merge_ID_140, 233 of 268 at time 2024-08-08 17:26:45.009342\n",
      "Dissolving for Merge_ID_218, 234 of 268 at time 2024-08-08 17:27:00.285318\n",
      "Dissolving for Merge_ID_255, 235 of 268 at time 2024-08-08 17:27:15.646372\n",
      "Dissolving for Merge_ID_268, 236 of 268 at time 2024-08-08 17:27:31.249648\n",
      "Dissolving for Merge_ID_196, 237 of 268 at time 2024-08-08 17:27:46.508608\n",
      "Dissolving for Merge_ID_260, 238 of 268 at time 2024-08-08 17:28:23.641581\n",
      "Dissolving for Merge_ID_16, 239 of 268 at time 2024-08-08 17:28:39.054720\n",
      "Dissolving for Merge_ID_39, 240 of 268 at time 2024-08-08 17:29:11.925211\n",
      "Dissolving for Merge_ID_6, 241 of 268 at time 2024-08-08 17:29:32.019850\n",
      "Dissolving for Merge_ID_210, 242 of 268 at time 2024-08-08 17:29:47.405122\n",
      "Dissolving for Merge_ID_102, 243 of 268 at time 2024-08-08 17:30:03.201775\n",
      "Dissolving for Merge_ID_216, 244 of 268 at time 2024-08-08 17:30:23.113244\n",
      "Dissolving for Merge_ID_158, 245 of 268 at time 2024-08-08 17:30:38.340369\n",
      "Dissolving for Merge_ID_251, 246 of 268 at time 2024-08-08 17:31:02.681948\n",
      "Dissolving for Merge_ID_176, 247 of 268 at time 2024-08-08 17:31:18.116265\n",
      "Dissolving for Merge_ID_147, 248 of 268 at time 2024-08-08 17:31:42.366760\n",
      "Dissolving for Merge_ID_124, 249 of 268 at time 2024-08-08 17:31:57.677962\n",
      "Dissolving for Merge_ID_186, 250 of 268 at time 2024-08-08 17:32:40.018237\n",
      "Dissolving for Merge_ID_38, 251 of 268 at time 2024-08-08 17:33:00.295046\n",
      "Dissolving for Merge_ID_72, 252 of 268 at time 2024-08-08 17:33:42.612755\n",
      "Dissolving for Merge_ID_194, 253 of 268 at time 2024-08-08 17:33:58.336140\n",
      "Dissolving for Merge_ID_116, 254 of 268 at time 2024-08-08 17:34:18.389487\n",
      "Dissolving for Merge_ID_27, 255 of 268 at time 2024-08-08 17:34:43.394365\n",
      "Dissolving for Merge_ID_73, 256 of 268 at time 2024-08-08 17:35:03.332606\n",
      "Dissolving for Merge_ID_135, 257 of 268 at time 2024-08-08 17:35:28.285436\n",
      "Dissolving for Merge_ID_150, 258 of 268 at time 2024-08-08 17:35:53.125162\n",
      "Dissolving for Merge_ID_28, 259 of 268 at time 2024-08-08 17:36:08.730439\n",
      "Dissolving for Merge_ID_157, 260 of 268 at time 2024-08-08 17:36:37.282562\n",
      "Dissolving for Merge_ID_110, 261 of 268 at time 2024-08-08 17:37:01.692895\n",
      "Dissolving for Merge_ID_48, 262 of 268 at time 2024-08-08 17:37:17.643488\n",
      "Dissolving for Merge_ID_37, 263 of 268 at time 2024-08-08 17:37:42.193950\n",
      "Dissolving for Merge_ID_234, 264 of 268 at time 2024-08-08 17:38:02.382420\n",
      "Dissolving for Merge_ID_12, 265 of 268 at time 2024-08-08 17:38:26.745710\n",
      "Dissolving for Merge_ID_149, 266 of 268 at time 2024-08-08 17:38:47.273491\n",
      "Dissolving for Merge_ID_34, 267 of 268 at time 2024-08-08 17:39:04.367131\n",
      "Dissolving for Merge_ID_145, 268 of 268 at time 2024-08-08 17:39:24.965977\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "\n",
    "#Run dissolve (also referred to as merge).\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_name_dissolve)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df.iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "\n",
    "#Export jpgs. \n",
    "#This process was too heavy to run inside this notebook, causing failed exports and freeze of the program.\n",
    "#It was therefore moved to an external script, called from here by writing a batch file and executing it.\n",
    "\n",
    "try:\n",
    "    \n",
    "    if run_jpg:\n",
    "       \n",
    "        aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "        project_path = aprx.filePath\n",
    "        project_folder = os.path.dirname(project_path)\n",
    "\n",
    "        jpg_folder = model_output_folder + r'\\jpg'\n",
    "        if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "        jpg_script = project_folder + '\\\\JPG_Subprocess.py'\n",
    "        bat_file_path = project_folder + '\\\\Execute_JPG.bat'\n",
    "        bat_file = open(bat_file_path, \"w\")\n",
    "        python_installation = sys.executable\n",
    "        python_installation = os.path.dirname(sys.executable) + r'\\Python\\envs\\arcgispro-py3\\python.exe'\n",
    "\n",
    "        bat_file_text = '@echo off\\n'\n",
    "        bat_file_text += 'set PYTHON_PATH=\"' + python_installation + '\"\\n'\n",
    "        bat_file_text += 'set SCRIPT_PATH=\"JPG_Subprocess.py\"\\n'\n",
    "        bat_file_text += 'set ARG1=\"' + project_path + '\"\\n'\n",
    "        bat_file_text += 'set ARG2=\"' + jpg_folder + '\"\\n'\n",
    "        bat_file_text += '%PYTHON_PATH% %SCRIPT_PATH% %ARG1% %ARG2%\\n'\n",
    "\n",
    "        bat_file.write(bat_file_text)\n",
    "        bat_file.close()\n",
    "        result = subprocess.call([bat_file_path]) \n",
    "\n",
    "        if result == 1: #Error\n",
    "            raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "            then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "\n",
    "        print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "    \n",
    "    username = os.getlogin()\n",
    "    date_created = str(datetime.datetime.today()).split(' ')[0]\n",
    "    \n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    backup_folder = f'{excel_folder}\\\\Backup_{date_created}_{model}_V{model_version}_{pop_sheet}'\n",
    "    \n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    if not os.path.isdir(backup_folder): os.makedirs(backup_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "        if use_formula: #Replace spreadsheet values with formulas\n",
    "            value_startrow = 17\n",
    "            for i in node_single_df.index:\n",
    "                currentrow = i + value_startrow -1\n",
    "                for avg_calc_dict_key in avg_calc_dict:\n",
    "#                     currentrow = i + value_startrow\n",
    "                    inputs = avg_calc_dict[avg_calc_dict_key]\n",
    "                    header = inputs[0]\n",
    "                    subheader = inputs[2]\n",
    "                    unitflow_ref = inputs[3]\n",
    "                    col_ref = inputs[4]\n",
    "                    formula = f'{r\"=\"}{unitflow_ref}*{col_ref}{currentrow}/86400'\n",
    "                    node_single_df.loc[i,(header,subheader)] = formula                \n",
    "                \n",
    "                node_single_df.loc[i,('RESIDENTIAL','PEAK FLOW (L/s)')] = f'{r\"=\"}MAX(1.5,(1+(14/(4+((H{currentrow}/1000)^0.5)))))*I{currentrow}'\n",
    "                node_single_df.loc[i,('COMMERCIAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"com\"][3]}*K{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5))))*0.8)*L{currentrow}'\n",
    "                \n",
    "                ind_formula = f'{r\"=\"}MAX(1.5,0.8*(1+((14)/(4+(((N{currentrow}*{avg_calc_dict[\"ind\"][3]}/{avg_calc_dict[\"res\"][3]}'\n",
    "                ind_formula += f')/1000)^0.5))))*IF(N{currentrow}<121,1.7,(2.505-0.1673*LN(N{currentrow}))))*O{currentrow}'\n",
    "                node_single_df.loc[i,('INDUSTRIAL','PEAK FLOW (L/s)')] = ind_formula\n",
    "                \n",
    "                node_single_df.loc[i,('INSTITUTIONAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"inst\"][3]}*Q{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5)))))*R{currentrow}'\n",
    "\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','AREA (Ha)')] = f'{r\"=\"}G{currentrow}+K{currentrow}+N{currentrow}+Q{currentrow}'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFLOW (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infl\"][3]}/86400'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFILTRATION (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infi\"][3]}/86400'\n",
    "                \n",
    "                node_single_df.loc[i,('FLOWS','AVG. SAN. FLOW (L/s)')] = f'{r\"=\"}I{currentrow}+L{currentrow}+O{currentrow}+R{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','ADWF (L/s)')] = f'{r\"=\"}W{currentrow}+V{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','PWWF (L/s)')] = \\\n",
    "                    f'{r\"=\"}J{currentrow}+M{currentrow}+P{currentrow}+S{currentrow}+U{currentrow}+V{currentrow}'\n",
    "                \n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        id = id.replace('\\\\','_')\n",
    "        id = id.upper()\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 23 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13\n",
    "        \n",
    "        #Delete empty row between header and data\n",
    "        sheet1.delete_rows(16)\n",
    "        \n",
    "        #Find the minimum factor in the sheet. Array formulas do not work with openpyxl, do divisions one by one instead\n",
    "        cols = [['I','J'],['L','M'],['O','P'],['R','S']]\n",
    "        rows = list(range(16,currentrow + 1))      \n",
    "        formula = r'=ROUND(MIN('\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                formula += f'IFERROR({col[1]}{row}/{col[0]}{row},999),'\n",
    "        formula = formula[:-1] + '),2)'        \n",
    "        sheet1['J9'] = formula\n",
    "        sheet1['H9'] = 'Lowest peaking factor:'\n",
    "        sheet1['H10'] = 'Lowest peaking factors at lower limit 1.5 highlighted in yellow.'\n",
    "                      \n",
    "        #Color code cells where peaking facor is at minimum.\n",
    "        format_formula = 'J16/I16<1.501'\n",
    "        format_range = ''\n",
    "        for col in cols:\n",
    "            format_range += f'{col[1]}16:{col[1]}{currentrow} '\n",
    "        format_range = format_range[:-1] #Remove last space.               \n",
    "        fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "        rule = FormulaRule(formula=[format_formula], fill=fill)\n",
    "        sheet1.conditional_formatting.add(format_range, rule)\n",
    "        \n",
    "        decimals = [['#,##0','H'],['#,##0.0','IJLMOPRSUVWXY'],['#,##0.00','GKNQT']]\n",
    "        for decimal in decimals:\n",
    "            for col in decimal[1]:\n",
    "                for row in range(16,currentrow+1):\n",
    "                    sheet1[f'{col}{row}'].number_format = decimal[0]\n",
    "        \n",
    "        display_name = \"Open in Google Maps\"\n",
    "        google_map_string = 'https://maps.google.com/?q='\n",
    "        google_map_string += str(centroids_df.loc[muid,'Y']) + ', '\n",
    "        google_map_string += str(centroids_df.loc[muid,'X']) \n",
    "\n",
    "        # Adding a hyperlink with a display name\n",
    "        cell = sheet1.cell(row=10, column=3, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "                      \n",
    "        sheet = workbook[\"Map\"]\n",
    "        \n",
    "        #Add source info\n",
    "        source_infos = []\n",
    "        source_infos.append(['Created by:',username])\n",
    "        source_infos.append(['Date:',date_created])\n",
    "        source_infos.append(['Model area:',model])\n",
    "        source_infos.append(['Model version:',model_version])\n",
    "        source_infos.append(['Population file:',os.path.basename(pop_book)])\n",
    "        source_infos.append(['Population sheet:',pop_sheet ])\n",
    "        \n",
    "        for i, source_info in enumerate(source_infos):\n",
    "            sheet1[f'H{i+2}']  = source_info[0]\n",
    "            sheet1[f'J{i+2}']  = source_info[1]\n",
    "        \n",
    "        for row in sheet1['H2:H9']:\n",
    "            for cell in row:\n",
    "                cell.font = Font(bold=True)\n",
    "        \n",
    "\n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        sheet.add_image(img, 'B3')\n",
    "        \n",
    "        cell = sheet.cell(row=1, column=2, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "        shutil.copy(sheetpath,backup_folder + \"\\\\\" + id + \".xlsx\")\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = 'All cells ran successfully.\\n\\nPlease check the following folder (not its backup subfolders) for obsolete sheets (from previous rounds not overwritten) and delete them. Sort by date to see which ones.\\n\\n' + model_output_folder + '\\\\Excel'\n",
    "MessageBox(None,message.encode('utf-8'), b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
