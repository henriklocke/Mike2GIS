{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "# User Input, to move to separate sheet so no permanent cell\n",
    "#stop trace if more pipes than max_steps traced from catchment, must be an endless loop. \n",
    "max_steps = 1000 \n",
    "\n",
    "update_field_in_model = True\n",
    "update_field = 'Description'\n",
    "\n",
    "output_folder = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output\"\n",
    "model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "sewer_area = 'NSSA'\n",
    "pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "pop_sheet = 'MPF Update 4'\n",
    "model = 'NSSA'\n",
    "gdb_name = 'RAWN.gdb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "#Set up column names\n",
    "\n",
    "years = [2060,2070,2080,2090,2100]\n",
    "categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "mpf_col_dict = {}\n",
    "\n",
    "area_col_dict = {}\n",
    "area_col_dict['res'] = 'Area_Res'\n",
    "area_col_dict['com'] = 'Area_Com'\n",
    "area_col_dict['ind'] = 'Area_Ind'\n",
    "area_col_dict['inst'] = 'Area_Inst'\n",
    "area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "per_unit_dict = {}\n",
    "per_unit_dict['res'] = 320\n",
    "per_unit_dict['com'] = 33700 \n",
    "per_unit_dict['ind'] = 56200\n",
    "per_unit_dict['inst'] = 33700\n",
    "per_unit_dict['infl'] = 5600\n",
    "per_unit_dict['infi'] = 5600\n",
    "\n",
    "header_dict = {}\n",
    "# header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "avg_calc_dict = {}\n",
    "avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)']\n",
    "avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)']\n",
    "\n",
    "header_tuples = []\n",
    "for header in header_dict:\n",
    "    for sub_header in (header_dict[header][1]):\n",
    "        header_tuples.append((header_dict[header][0],sub_header))\n",
    "header_tuples\n",
    "\n",
    "# columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "#Import population\n",
    "pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "pop_sum_total_col = int(pop_df.Population.sum())\n",
    "pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "if pop_sum_total_col != pop_sum_sub_cols:\n",
    "      raise ValueError(\"Error. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "#Import model data\n",
    "\n",
    "node_types = {}\n",
    "node_types[1] = 'Manhole'\n",
    "node_types[2] = 'Basin'\n",
    "node_types[3] = 'Outlet'\n",
    "node_types[4] = 'Junction'\n",
    "node_types[5] = 'Soakaway'\n",
    "node_types[6] = 'River Junction'\n",
    "\n",
    "sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "lines = sql_to_df(sql,model_path)\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "orifices = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,orifices])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "valves = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,valves])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "weirs = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,weirs])\n",
    "\n",
    "sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "pumps = sql_to_df(sql,model_path)\n",
    "lines = pd.concat([lines,pumps])\n",
    "\n",
    "lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "\n",
    "sql = \"SELECT muid, acronym, assetname FROM msm_Node WHERE active = 1\"\n",
    "node_id_df = sql_to_df(sql,model_path)\n",
    "node_id_df = node_id_df[(node_id_df.assetname.str[:2]=='MH') & (node_id_df.assetname.str.len() > 2) & (node_id_df.acronym.notna())]\n",
    "node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "node_id_df = node_id_df[['Node','ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "accumulated_catchment_set = set()\n",
    "\n",
    "for index1, row1 in catchments.iterrows():\n",
    "    catchment = row1['Catchment']\n",
    "    nodes = [row1['Connected_Node']]\n",
    "    start_node = row1['Connected_Node']\n",
    "    steps = 0\n",
    "    \n",
    "    accumulated_catchment_set.add((start_node,catchment))\n",
    "        \n",
    "    while steps <= max_steps:\n",
    "        steps += 1\n",
    "        downstream_df = lines[lines['From'].isin(nodes)]\n",
    "\n",
    "        if len(downstream_df) > 0:\n",
    "            nodes = list(downstream_df.To.unique())\n",
    "\n",
    "            nodes = [node for node in nodes if len(node)>0]\n",
    "            for node in nodes:\n",
    "                accumulated_catchment_set.add((node,catchment))       \n",
    "        else:\n",
    "            break\n",
    "        if steps == max_steps:\n",
    "            raise ValueError(\"Maximum steps were reached, indicating a loop. Start catchment is '\" + catchment + \"'\")\n",
    "           \n",
    "        accumulated_catchment_set.add((node,catchment))\n",
    "        \n",
    "accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "data = {\n",
    "    ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "    ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "    ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "}\n",
    "\n",
    "# Create a DataFrame with MultiIndex columns\n",
    "accumulation_df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "catchment_df = df_template.copy()\n",
    "for catchment in catchments:\n",
    "    for year in years:\n",
    "        key = model + '@' + catchment + '@' + str(year)\n",
    "        catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'UNKNOWN'\n",
    "        catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "        catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "        catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "        for area_col_dict_key in area_col_dict:\n",
    "            catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "        catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "        san_flow = 0\n",
    "        adwf = 0\n",
    "        for avg_calc_dict_key in avg_calc_dict:\n",
    "            input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "            input2 = per_unit_dict[avg_calc_dict_key]\n",
    "            avg_flow = input1 * input2 / 86400\n",
    "            adwf += avg_flow\n",
    "            if avg_calc_dict_key not in ['infl','infi']:\n",
    "                san_flow += avg_flow\n",
    "            catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "        catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "        catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "        \n",
    "catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "node_df = catchment_node_df.copy()\n",
    "node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "node_df.reset_index(inplace=True)\n",
    "node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]*0.8\n",
    "node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = (\n",
    "    0.8 * (1 + 14 / (4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5)) *\n",
    "    np.where(\n",
    "        node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "        1.7,\n",
    "        2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    ")\n",
    "\n",
    "node_df[('FLOWS','PWWF (L/s)')] = (\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] +\n",
    "    node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    ")\n",
    "\n",
    "excel_folder = output_folder + '\\\\Excel'\n",
    "if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "    node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id]\n",
    "    id = id.replace('/','-') if '/' in id else id\n",
    "    node_single_df.to_excel(excel_folder + '\\\\' + id + '.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">GENERAL INFO</th>\n",
       "      <th colspan=\"4\" halign=\"left\">RESIDENTIAL</th>\n",
       "      <th colspan=\"3\" halign=\"left\">COMMERCIAL</th>\n",
       "      <th colspan=\"3\" halign=\"left\">INDUSTRIAL</th>\n",
       "      <th colspan=\"3\" halign=\"left\">INSTITUTIONAL</th>\n",
       "      <th colspan=\"3\" halign=\"left\">INFLOW / INFILTRATION</th>\n",
       "      <th colspan=\"3\" halign=\"left\">FLOWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>NODE</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ID</th>\n",
       "      <th>AREA (Ha)</th>\n",
       "      <th>POPULATION</th>\n",
       "      <th>AVG. FLOW (L/s)</th>\n",
       "      <th>PEAK FLOW (L/s)</th>\n",
       "      <th>AREA (Ha)</th>\n",
       "      <th>AVG. FLOW (L/s)</th>\n",
       "      <th>PEAK FLOW (L/s)</th>\n",
       "      <th>AREA (Ha)</th>\n",
       "      <th>AVG. FLOW (L/s)</th>\n",
       "      <th>PEAK FLOW (L/s)</th>\n",
       "      <th>AREA (Ha)</th>\n",
       "      <th>AVG. FLOW (L/s)</th>\n",
       "      <th>PEAK FLOW (L/s)</th>\n",
       "      <th>AREA (Ha)</th>\n",
       "      <th>INFLOW (L/s)</th>\n",
       "      <th>INFILTRATION (L/s)</th>\n",
       "      <th>AVG. SAN. FLOW (L/s)</th>\n",
       "      <th>ADWF (L/s)</th>\n",
       "      <th>PWWF (L/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>9984</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2060</td>\n",
       "      <td>NSSA</td>\n",
       "      <td>MKT_MH7A</td>\n",
       "      <td>259.610835</td>\n",
       "      <td>54942.804138</td>\n",
       "      <td>203.491867</td>\n",
       "      <td>453.123915</td>\n",
       "      <td>17.349367</td>\n",
       "      <td>6.767056</td>\n",
       "      <td>19.575681</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.339652</td>\n",
       "      <td>1.964883</td>\n",
       "      <td>20.596848</td>\n",
       "      <td>8.033724</td>\n",
       "      <td>28.584883</td>\n",
       "      <td>298.07922</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>218.632300</td>\n",
       "      <td>257.272198</td>\n",
       "      <td>541.889261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>9984</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2070</td>\n",
       "      <td>NSSA</td>\n",
       "      <td>MKT_MH7A</td>\n",
       "      <td>259.610835</td>\n",
       "      <td>61158.136516</td>\n",
       "      <td>226.511617</td>\n",
       "      <td>494.791163</td>\n",
       "      <td>17.349367</td>\n",
       "      <td>6.767056</td>\n",
       "      <td>19.575681</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.339652</td>\n",
       "      <td>1.964883</td>\n",
       "      <td>20.596848</td>\n",
       "      <td>8.033724</td>\n",
       "      <td>28.584883</td>\n",
       "      <td>298.07922</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>241.652049</td>\n",
       "      <td>280.291948</td>\n",
       "      <td>583.556508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>9984</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2080</td>\n",
       "      <td>NSSA</td>\n",
       "      <td>MKT_MH7A</td>\n",
       "      <td>259.610835</td>\n",
       "      <td>67373.468894</td>\n",
       "      <td>249.531366</td>\n",
       "      <td>535.688042</td>\n",
       "      <td>17.349367</td>\n",
       "      <td>6.767056</td>\n",
       "      <td>19.575681</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.339652</td>\n",
       "      <td>1.964883</td>\n",
       "      <td>20.596848</td>\n",
       "      <td>8.033724</td>\n",
       "      <td>28.584883</td>\n",
       "      <td>298.07922</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>264.671799</td>\n",
       "      <td>303.311698</td>\n",
       "      <td>624.453387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>9984</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2090</td>\n",
       "      <td>NSSA</td>\n",
       "      <td>MKT_MH7A</td>\n",
       "      <td>259.610835</td>\n",
       "      <td>73588.801272</td>\n",
       "      <td>272.551116</td>\n",
       "      <td>575.905930</td>\n",
       "      <td>17.349367</td>\n",
       "      <td>6.767056</td>\n",
       "      <td>19.575681</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.339652</td>\n",
       "      <td>1.964883</td>\n",
       "      <td>20.596848</td>\n",
       "      <td>8.033724</td>\n",
       "      <td>28.584883</td>\n",
       "      <td>298.07922</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>287.691548</td>\n",
       "      <td>326.331447</td>\n",
       "      <td>664.671275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>9984</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2100</td>\n",
       "      <td>NSSA</td>\n",
       "      <td>MKT_MH7A</td>\n",
       "      <td>259.610835</td>\n",
       "      <td>79804.133650</td>\n",
       "      <td>295.570865</td>\n",
       "      <td>615.519140</td>\n",
       "      <td>17.349367</td>\n",
       "      <td>6.767056</td>\n",
       "      <td>19.575681</td>\n",
       "      <td>0.522169</td>\n",
       "      <td>0.339652</td>\n",
       "      <td>1.964883</td>\n",
       "      <td>20.596848</td>\n",
       "      <td>8.033724</td>\n",
       "      <td>28.584883</td>\n",
       "      <td>298.07922</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>19.319949</td>\n",
       "      <td>310.711298</td>\n",
       "      <td>349.351197</td>\n",
       "      <td>704.284485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GENERAL INFO                 ...                FLOWS                        \n",
       "             NODE     TYPE  YEAR  ... AVG. SAN. FLOW (L/s)  ADWF (L/s)  PWWF (L/s)\n",
       "2340         9984  UNKNOWN  2060  ...           218.632300  257.272198  541.889261\n",
       "2341         9984  UNKNOWN  2070  ...           241.652049  280.291948  583.556508\n",
       "2342         9984  UNKNOWN  2080  ...           264.671799  303.311698  624.453387\n",
       "2343         9984  UNKNOWN  2090  ...           287.691548  326.331447  664.671275\n",
       "2344         9984  UNKNOWN  2100  ...           310.711298  349.351197  704.284485\n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_single_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__from_scripting_arc_object__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_arc_object', '_createRendererType', '_create_classify_colorizer', '_create_graduated_colors', '_create_graduated_symbols', '_create_single_symbol', '_create_stretch_colorizer', '_create_unclassed_colors', '_create_unique_value', '_create_unique_value_colorizer', '_getColorizer', '_getRenderer', '_renderer', '_rendererType', '_updateColorizer', '_updateRenderer', '_visualizer', 'renderer', 'updateRenderer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msm_CatchCon\n",
      "msm_Valve\n",
      "msm_Orifice\n",
      "msm_Weir\n",
      "msm_Pump\n",
      "msm_Link\n",
      "msm_CatchCon\n",
      "msm_Catchment_Dissolve_Single\n",
      "msm_Catchment\n"
     ]
    }
   ],
   "source": [
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = aprx.listMaps('Map')[0]\n",
    "for lyr in m.listLayers():\n",
    "  if lyr.isFeatureLayer:\n",
    "    print (lyr.name)\n",
    "    sym = lyr.symbology\n",
    "    if lyr.name == 'msm_CatchCon':\n",
    "        sym.renderer.symbol.color = {'RGB': [255, 0, 0]}  # Red color\n",
    "        lyr.symbology = sym\n",
    "aprx.save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "out_path = r'J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output' + '\\\\' + gdb_name\n",
    "arcpy.env.workspace = out_path\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "layers = ['msm_CatchCon']\n",
    "arcpy.env.overwriteOutput = True\n",
    "for layer in layers:\n",
    "    arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "    arcpy.DefineProjection_management(layer, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Friday, May 10, 2024 8:24:52 AM\",\"Adding Drains_To to msm_Catchment...\",\"Succeeded at Friday, May 10, 2024 8:24:53 AM (Elapsed Time: 1.11 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'msm_Catchment'>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "#Import GIS from the model\n",
    "\n",
    "arcpy.management.CreateFileGDB(output_folder, gdb_name)\n",
    "out_path = r'J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output' + '\\\\' + gdb_name\n",
    "arcpy.env.workspace = out_path\n",
    "sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "for layer in layers:\n",
    "#     arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "   \n",
    "    arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")  \n",
    "    arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer)\n",
    "    arcpy.management.Delete(\"temp_layer\")\n",
    "    arcpy.DefineProjection_management(layer, sr)\n",
    "    \n",
    "\n",
    "arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for node 0 of 466 at time 2024-05-10 08:24:55.665100\n",
      "Dissolving for node 1 of 466 at time 2024-05-10 08:25:01.888875\n",
      "Dissolving for node 2 of 466 at time 2024-05-10 08:25:08.672169\n",
      "Dissolving for node 3 of 466 at time 2024-05-10 08:25:15.605602\n",
      "Dissolving for node 4 of 466 at time 2024-05-10 08:25:22.106634\n",
      "Dissolving for node 5 of 466 at time 2024-05-10 08:25:29.173191\n",
      "Dissolving for node 6 of 466 at time 2024-05-10 08:25:36.103622\n",
      "Dissolving for node 7 of 466 at time 2024-05-10 08:25:43.058075\n",
      "Dissolving for node 8 of 466 at time 2024-05-10 08:25:50.473956\n",
      "Dissolving for node 9 of 466 at time 2024-05-10 08:25:57.442421\n",
      "Dissolving for node 10 of 466 at time 2024-05-10 08:26:04.805252\n",
      "Dissolving for node 11 of 466 at time 2024-05-10 08:26:12.039965\n",
      "Dissolving for node 12 of 466 at time 2024-05-10 08:26:19.694068\n",
      "Dissolving for node 13 of 466 at time 2024-05-10 08:26:27.034878\n",
      "Dissolving for node 14 of 466 at time 2024-05-10 08:26:34.990260\n",
      "Dissolving for node 15 of 466 at time 2024-05-10 08:26:42.499229\n",
      "Dissolving for node 16 of 466 at time 2024-05-10 08:26:50.177351\n",
      "Dissolving for node 17 of 466 at time 2024-05-10 08:26:57.569787\n",
      "Dissolving for node 18 of 466 at time 2024-05-10 08:27:05.011692\n",
      "Dissolving for node 19 of 466 at time 2024-05-10 08:27:12.872986\n",
      "Dissolving for node 20 of 466 at time 2024-05-10 08:27:20.607163\n",
      "Dissolving for node 21 of 466 at time 2024-05-10 08:27:28.628602\n",
      "Dissolving for node 22 of 466 at time 2024-05-10 08:27:35.957398\n",
      "Dissolving for node 23 of 466 at time 2024-05-10 08:27:43.598481\n",
      "Dissolving for node 24 of 466 at time 2024-05-10 08:27:51.186518\n",
      "Dissolving for node 25 of 466 at time 2024-05-10 08:27:59.416147\n",
      "Dissolving for node 26 of 466 at time 2024-05-10 08:28:07.233395\n",
      "Dissolving for node 27 of 466 at time 2024-05-10 08:28:18.520860\n",
      "Dissolving for node 28 of 466 at time 2024-05-10 08:28:27.843503\n",
      "Dissolving for node 29 of 466 at time 2024-05-10 08:28:35.477581\n",
      "Dissolving for node 30 of 466 at time 2024-05-10 08:28:43.463986\n",
      "Dissolving for node 31 of 466 at time 2024-05-10 08:28:50.980955\n",
      "Dissolving for node 32 of 466 at time 2024-05-10 08:28:58.782778\n",
      "Dissolving for node 33 of 466 at time 2024-05-10 08:29:06.761175\n",
      "Dissolving for node 34 of 466 at time 2024-05-10 08:29:14.543390\n",
      "Dissolving for node 35 of 466 at time 2024-05-10 08:29:22.769017\n",
      "Dissolving for node 36 of 466 at time 2024-05-10 08:29:30.764430\n",
      "Dissolving for node 37 of 466 at time 2024-05-10 08:29:39.045109\n",
      "Dissolving for node 38 of 466 at time 2024-05-10 08:29:47.259724\n",
      "Dissolving for node 39 of 466 at time 2024-05-10 08:29:55.278158\n",
      "Dissolving for node 40 of 466 at time 2024-05-10 08:30:03.268567\n",
      "Dissolving for node 41 of 466 at time 2024-05-10 08:30:11.291005\n",
      "Dissolving for node 42 of 466 at time 2024-05-10 08:30:19.762860\n",
      "Dissolving for node 43 of 466 at time 2024-05-10 08:30:29.404799\n",
      "Dissolving for node 44 of 466 at time 2024-05-10 08:30:37.537340\n",
      "Dissolving for node 45 of 466 at time 2024-05-10 08:30:45.723929\n",
      "Dissolving for node 46 of 466 at time 2024-05-10 08:30:54.157749\n",
      "Dissolving for node 47 of 466 at time 2024-05-10 08:31:04.193059\n",
      "Dissolving for node 48 of 466 at time 2024-05-10 08:31:12.435701\n",
      "Dissolving for node 49 of 466 at time 2024-05-10 08:31:22.278827\n",
      "Dissolving for node 50 of 466 at time 2024-05-10 08:31:31.043954\n",
      "Dissolving for node 51 of 466 at time 2024-05-10 08:31:39.075400\n",
      "Dissolving for node 52 of 466 at time 2024-05-10 08:31:47.511221\n",
      "Dissolving for node 53 of 466 at time 2024-05-10 08:31:56.122205\n",
      "Dissolving for node 54 of 466 at time 2024-05-10 08:32:04.429948\n",
      "Dissolving for node 55 of 466 at time 2024-05-10 08:32:12.141097\n",
      "Dissolving for node 56 of 466 at time 2024-05-10 08:32:19.801199\n",
      "Dissolving for node 57 of 466 at time 2024-05-10 08:32:26.923802\n",
      "Dissolving for node 58 of 466 at time 2024-05-10 08:32:34.513835\n",
      "Dissolving for node 59 of 466 at time 2024-05-10 08:32:42.003774\n",
      "Dissolving for node 60 of 466 at time 2024-05-10 08:32:49.869062\n",
      "Dissolving for node 61 of 466 at time 2024-05-10 08:32:57.453089\n",
      "Dissolving for node 62 of 466 at time 2024-05-10 08:33:04.997778\n",
      "Dissolving for node 63 of 466 at time 2024-05-10 08:33:12.453686\n",
      "Dissolving for node 64 of 466 at time 2024-05-10 08:33:20.233894\n",
      "Dissolving for node 65 of 466 at time 2024-05-10 08:33:27.755864\n",
      "Dissolving for node 66 of 466 at time 2024-05-10 08:33:35.501040\n",
      "Dissolving for node 67 of 466 at time 2024-05-10 08:33:43.081064\n",
      "Dissolving for node 68 of 466 at time 2024-05-10 08:33:50.727148\n",
      "Dissolving for node 69 of 466 at time 2024-05-10 08:33:58.893715\n",
      "Dissolving for node 70 of 466 at time 2024-05-10 08:34:06.328604\n",
      "Dissolving for node 71 of 466 at time 2024-05-10 08:34:13.978736\n",
      "Dissolving for node 72 of 466 at time 2024-05-10 08:34:22.231383\n",
      "Dissolving for node 73 of 466 at time 2024-05-10 08:34:30.149719\n",
      "Dissolving for node 74 of 466 at time 2024-05-10 08:34:37.462495\n",
      "Dissolving for node 75 of 466 at time 2024-05-10 08:34:45.127597\n",
      "Dissolving for node 76 of 466 at time 2024-05-10 08:34:52.746670\n",
      "Dissolving for node 77 of 466 at time 2024-05-10 08:35:00.278649\n",
      "Dissolving for node 78 of 466 at time 2024-05-10 08:35:07.797848\n",
      "Dissolving for node 79 of 466 at time 2024-05-10 08:35:15.496982\n",
      "Dissolving for node 80 of 466 at time 2024-05-10 08:35:23.448349\n",
      "Dissolving for node 81 of 466 at time 2024-05-10 08:35:30.782144\n",
      "Dissolving for node 82 of 466 at time 2024-05-10 08:35:38.482279\n",
      "Dissolving for node 83 of 466 at time 2024-05-10 08:35:45.702969\n",
      "Dissolving for node 84 of 466 at time 2024-05-10 08:35:52.941676\n",
      "Dissolving for node 85 of 466 at time 2024-05-10 08:36:00.678845\n",
      "Dissolving for node 86 of 466 at time 2024-05-10 08:36:08.329934\n",
      "Dissolving for node 87 of 466 at time 2024-05-10 08:36:16.133164\n",
      "Dissolving for node 88 of 466 at time 2024-05-10 08:36:24.025477\n",
      "Dissolving for node 89 of 466 at time 2024-05-10 08:36:32.149004\n",
      "Dissolving for node 90 of 466 at time 2024-05-10 08:36:39.834125\n",
      "Dissolving for node 91 of 466 at time 2024-05-10 08:36:47.348087\n",
      "Dissolving for node 92 of 466 at time 2024-05-10 08:36:55.129821\n",
      "Dissolving for node 93 of 466 at time 2024-05-10 08:37:02.515300\n",
      "Dissolving for node 94 of 466 at time 2024-05-10 08:37:10.498697\n",
      "Dissolving for node 95 of 466 at time 2024-05-10 08:37:18.032678\n",
      "Dissolving for node 96 of 466 at time 2024-05-10 08:37:26.330365\n",
      "Dissolving for node 97 of 466 at time 2024-05-10 08:37:34.145602\n",
      "Dissolving for node 98 of 466 at time 2024-05-10 08:37:43.363138\n",
      "Dissolving for node 99 of 466 at time 2024-05-10 08:37:51.430609\n",
      "Dissolving for node 100 of 466 at time 2024-05-10 08:37:59.463047\n",
      "Dissolving for node 101 of 466 at time 2024-05-10 08:38:07.253262\n",
      "Dissolving for node 102 of 466 at time 2024-05-10 08:38:16.001363\n",
      "Dissolving for node 103 of 466 at time 2024-05-10 08:38:24.783496\n",
      "Dissolving for node 104 of 466 at time 2024-05-10 08:38:33.698752\n",
      "Dissolving for node 105 of 466 at time 2024-05-10 08:38:42.316733\n",
      "Dissolving for node 106 of 466 at time 2024-05-10 08:38:50.731525\n",
      "Dissolving for node 107 of 466 at time 2024-05-10 08:38:59.448598\n",
      "Dissolving for node 108 of 466 at time 2024-05-10 08:39:07.898108\n",
      "Dissolving for node 109 of 466 at time 2024-05-10 08:39:16.334921\n",
      "Dissolving for node 110 of 466 at time 2024-05-10 08:39:25.122564\n",
      "Dissolving for node 111 of 466 at time 2024-05-10 08:39:33.846643\n",
      "Dissolving for node 112 of 466 at time 2024-05-10 08:39:42.255430\n",
      "Dissolving for node 113 of 466 at time 2024-05-10 08:39:50.658212\n",
      "Dissolving for node 114 of 466 at time 2024-05-10 08:39:58.634112\n",
      "Dissolving for node 115 of 466 at time 2024-05-10 08:40:06.617505\n",
      "Dissolving for node 116 of 466 at time 2024-05-10 08:40:15.001269\n",
      "Dissolving for node 117 of 466 at time 2024-05-10 08:40:23.052228\n",
      "Dissolving for node 118 of 466 at time 2024-05-10 08:40:31.766802\n",
      "Dissolving for node 119 of 466 at time 2024-05-10 08:40:40.299703\n",
      "Dissolving for node 120 of 466 at time 2024-05-10 08:40:49.781484\n",
      "Dissolving for node 121 of 466 at time 2024-05-10 08:40:58.273348\n",
      "Dissolving for node 122 of 466 at time 2024-05-10 08:41:07.498621\n",
      "Dissolving for node 123 of 466 at time 2024-05-10 08:41:16.215693\n",
      "Dissolving for node 124 of 466 at time 2024-05-10 08:41:25.304110\n",
      "Dissolving for node 125 of 466 at time 2024-05-10 08:41:34.465594\n",
      "Dissolving for node 126 of 466 at time 2024-05-10 08:41:43.287763\n",
      "Dissolving for node 127 of 466 at time 2024-05-10 08:41:52.114938\n",
      "Dissolving for node 128 of 466 at time 2024-05-10 08:42:00.350602\n",
      "Dissolving for node 129 of 466 at time 2024-05-10 08:42:08.315978\n",
      "Dissolving for node 130 of 466 at time 2024-05-10 08:42:16.432494\n",
      "Dissolving for node 131 of 466 at time 2024-05-10 08:42:24.652106\n",
      "Dissolving for node 132 of 466 at time 2024-05-10 08:42:33.003835\n",
      "Dissolving for node 133 of 466 at time 2024-05-10 08:42:41.385592\n",
      "Dissolving for node 134 of 466 at time 2024-05-10 08:42:49.982548\n",
      "Dissolving for node 135 of 466 at time 2024-05-10 08:42:59.319189\n",
      "Dissolving for node 136 of 466 at time 2024-05-10 08:43:08.253208\n",
      "Dissolving for node 137 of 466 at time 2024-05-10 08:43:17.422694\n",
      "Dissolving for node 138 of 466 at time 2024-05-10 08:43:26.708287\n",
      "Dissolving for node 139 of 466 at time 2024-05-10 08:43:35.956847\n",
      "Dissolving for node 140 of 466 at time 2024-05-10 08:43:45.216416\n",
      "Dissolving for node 141 of 466 at time 2024-05-10 08:43:54.317839\n",
      "Dissolving for node 142 of 466 at time 2024-05-10 08:44:03.533367\n",
      "Dissolving for node 143 of 466 at time 2024-05-10 08:44:13.017144\n",
      "Dissolving for node 144 of 466 at time 2024-05-10 08:44:22.017474\n",
      "Dissolving for node 145 of 466 at time 2024-05-10 08:44:30.551372\n",
      "Dissolving for node 146 of 466 at time 2024-05-10 08:44:38.971163\n",
      "Dissolving for node 147 of 466 at time 2024-05-10 08:44:47.468046\n",
      "Dissolving for node 148 of 466 at time 2024-05-10 08:44:56.118052\n",
      "Dissolving for node 149 of 466 at time 2024-05-10 08:45:04.574381\n",
      "Dissolving for node 150 of 466 at time 2024-05-10 08:45:13.496810\n",
      "Dissolving for node 151 of 466 at time 2024-05-10 08:45:22.607242\n",
      "Dissolving for node 152 of 466 at time 2024-05-10 08:45:32.080008\n",
      "Dissolving for node 153 of 466 at time 2024-05-10 08:45:41.240496\n",
      "Dissolving for node 154 of 466 at time 2024-05-10 08:45:50.575638\n",
      "Dissolving for node 155 of 466 at time 2024-05-10 08:46:00.801101\n",
      "Dissolving for node 156 of 466 at time 2024-05-10 08:46:10.118724\n",
      "Dissolving for node 157 of 466 at time 2024-05-10 08:46:19.758645\n",
      "Dissolving for node 158 of 466 at time 2024-05-10 08:46:28.874082\n",
      "Dissolving for node 159 of 466 at time 2024-05-10 08:46:37.369944\n",
      "Dissolving for node 160 of 466 at time 2024-05-10 08:46:45.642600\n",
      "Dissolving for node 161 of 466 at time 2024-05-10 08:46:54.102429\n",
      "Dissolving for node 162 of 466 at time 2024-05-10 08:47:05.669134\n",
      "Dissolving for node 163 of 466 at time 2024-05-10 08:47:14.565870\n",
      "Dissolving for node 164 of 466 at time 2024-05-10 08:47:23.104773\n",
      "Dissolving for node 165 of 466 at time 2024-05-10 08:47:31.520558\n",
      "Dissolving for node 166 of 466 at time 2024-05-10 08:47:40.968296\n",
      "Dissolving for node 167 of 466 at time 2024-05-10 08:47:50.105748\n",
      "Dissolving for node 168 of 466 at time 2024-05-10 08:47:59.457409\n",
      "Dissolving for node 169 of 466 at time 2024-05-10 08:48:08.626434\n",
      "Dissolving for node 170 of 466 at time 2024-05-10 08:48:18.457527\n",
      "Dissolving for node 171 of 466 at time 2024-05-10 08:48:28.107453\n",
      "Dissolving for node 172 of 466 at time 2024-05-10 08:48:37.803421\n",
      "Dissolving for node 173 of 466 at time 2024-05-10 08:48:47.158074\n",
      "Dissolving for node 174 of 466 at time 2024-05-10 08:48:56.699900\n",
      "Dissolving for node 175 of 466 at time 2024-05-10 08:49:06.437410\n",
      "Dissolving for node 176 of 466 at time 2024-05-10 08:49:15.470772\n",
      "Dissolving for node 177 of 466 at time 2024-05-10 08:49:24.140792\n",
      "Dissolving for node 178 of 466 at time 2024-05-10 08:49:32.793795\n",
      "Dissolving for node 179 of 466 at time 2024-05-10 08:49:41.861182\n",
      "Dissolving for node 180 of 466 at time 2024-05-10 08:49:51.016656\n",
      "Dissolving for node 181 of 466 at time 2024-05-10 08:49:59.708695\n",
      "Dissolving for node 182 of 466 at time 2024-05-10 08:50:09.443700\n",
      "Dissolving for node 183 of 466 at time 2024-05-10 08:50:19.326842\n",
      "Dissolving for node 184 of 466 at time 2024-05-10 08:50:28.494328\n",
      "Dissolving for node 185 of 466 at time 2024-05-10 08:50:38.661234\n",
      "Dissolving for node 186 of 466 at time 2024-05-10 08:50:48.177058\n",
      "Dissolving for node 187 of 466 at time 2024-05-10 08:50:57.841998\n",
      "Dissolving for node 188 of 466 at time 2024-05-10 08:51:07.347790\n",
      "Dissolving for node 189 of 466 at time 2024-05-10 08:51:16.963346\n",
      "Dissolving for node 190 of 466 at time 2024-05-10 08:51:26.339018\n",
      "Dissolving for node 191 of 466 at time 2024-05-10 08:51:36.118063\n",
      "Dissolving for node 192 of 466 at time 2024-05-10 08:51:45.701928\n",
      "Dissolving for node 193 of 466 at time 2024-05-10 08:51:55.130649\n",
      "Dissolving for node 194 of 466 at time 2024-05-10 08:52:05.155922\n",
      "Dissolving for node 195 of 466 at time 2024-05-10 08:52:14.832873\n",
      "Dissolving for node 196 of 466 at time 2024-05-10 08:52:24.928210\n",
      "Dissolving for node 197 of 466 at time 2024-05-10 08:52:35.009530\n",
      "Dissolving for node 198 of 466 at time 2024-05-10 08:52:45.293037\n",
      "Dissolving for node 199 of 466 at time 2024-05-10 08:52:55.389370\n",
      "Dissolving for node 200 of 466 at time 2024-05-10 08:53:05.624833\n",
      "Dissolving for node 201 of 466 at time 2024-05-10 08:53:15.235418\n",
      "Dissolving for node 202 of 466 at time 2024-05-10 08:53:24.581057\n",
      "Dissolving for node 203 of 466 at time 2024-05-10 08:53:35.559206\n",
      "Dissolving for node 204 of 466 at time 2024-05-10 08:53:45.839710\n",
      "Dissolving for node 205 of 466 at time 2024-05-10 08:53:55.639770\n",
      "Dissolving for node 206 of 466 at time 2024-05-10 08:54:05.659032\n",
      "Dissolving for node 207 of 466 at time 2024-05-10 08:54:16.107691\n",
      "Dissolving for node 208 of 466 at time 2024-05-10 08:54:26.348158\n",
      "Dissolving for node 209 of 466 at time 2024-05-10 08:54:36.624658\n",
      "Dissolving for node 210 of 466 at time 2024-05-10 08:54:46.478768\n",
      "Dissolving for node 211 of 466 at time 2024-05-10 08:54:56.229782\n",
      "Dissolving for node 212 of 466 at time 2024-05-10 08:55:06.506833\n",
      "Dissolving for node 213 of 466 at time 2024-05-10 08:55:17.762264\n",
      "Dissolving for node 214 of 466 at time 2024-05-10 08:55:27.915651\n",
      "Dissolving for node 215 of 466 at time 2024-05-10 08:55:38.045015\n",
      "Dissolving for node 216 of 466 at time 2024-05-10 08:55:48.243443\n",
      "Dissolving for node 217 of 466 at time 2024-05-10 08:55:58.474902\n",
      "Dissolving for node 218 of 466 at time 2024-05-10 08:56:09.123746\n",
      "Dissolving for node 219 of 466 at time 2024-05-10 08:56:19.668494\n",
      "Dissolving for node 220 of 466 at time 2024-05-10 08:56:29.501585\n",
      "Dissolving for node 221 of 466 at time 2024-05-10 08:56:39.554878\n",
      "Dissolving for node 222 of 466 at time 2024-05-10 08:56:50.071600\n",
      "Dissolving for node 223 of 466 at time 2024-05-10 08:57:00.115811\n",
      "Dissolving for node 224 of 466 at time 2024-05-10 08:57:10.613516\n",
      "Dissolving for node 225 of 466 at time 2024-05-10 08:57:20.054748\n",
      "Dissolving for node 226 of 466 at time 2024-05-10 08:57:29.747705\n",
      "Dissolving for node 227 of 466 at time 2024-05-10 08:57:39.123368\n",
      "Dissolving for node 228 of 466 at time 2024-05-10 08:57:49.477942\n",
      "Dissolving for node 229 of 466 at time 2024-05-10 08:57:59.225949\n",
      "Dissolving for node 230 of 466 at time 2024-05-10 08:58:08.875865\n",
      "Dissolving for node 231 of 466 at time 2024-05-10 08:58:18.929154\n",
      "Dissolving for node 232 of 466 at time 2024-05-10 08:58:29.197649\n",
      "Dissolving for node 233 of 466 at time 2024-05-10 08:58:39.443159\n",
      "Dissolving for node 234 of 466 at time 2024-05-10 08:58:52.759462\n",
      "Dissolving for node 235 of 466 at time 2024-05-10 08:59:02.727673\n",
      "Dissolving for node 236 of 466 at time 2024-05-10 08:59:12.910081\n",
      "Dissolving for node 237 of 466 at time 2024-05-10 08:59:23.057960\n",
      "Dissolving for node 238 of 466 at time 2024-05-10 08:59:32.965691\n",
      "Dissolving for node 239 of 466 at time 2024-05-10 08:59:43.606522\n",
      "Dissolving for node 240 of 466 at time 2024-05-10 08:59:53.460627\n",
      "Dissolving for node 241 of 466 at time 2024-05-10 09:00:04.093452\n",
      "Dissolving for node 242 of 466 at time 2024-05-10 09:00:14.114753\n",
      "Dissolving for node 243 of 466 at time 2024-05-10 09:00:25.666427\n",
      "Dissolving for node 244 of 466 at time 2024-05-10 09:00:35.936916\n",
      "Dissolving for node 245 of 466 at time 2024-05-10 09:00:46.880531\n",
      "Dissolving for node 246 of 466 at time 2024-05-10 09:00:57.702530\n",
      "Dissolving for node 247 of 466 at time 2024-05-10 09:01:07.934984\n",
      "Dissolving for node 248 of 466 at time 2024-05-10 09:01:18.084866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for node 249 of 466 at time 2024-05-10 09:01:28.501491\n",
      "Dissolving for node 250 of 466 at time 2024-05-10 09:01:39.146326\n",
      "Dissolving for node 251 of 466 at time 2024-05-10 09:01:50.301633\n",
      "Dissolving for node 252 of 466 at time 2024-05-10 09:02:01.163669\n",
      "Dissolving for node 253 of 466 at time 2024-05-10 09:02:10.966726\n",
      "Dissolving for node 254 of 466 at time 2024-05-10 09:02:21.420385\n",
      "Dissolving for node 255 of 466 at time 2024-05-10 09:02:31.656840\n",
      "Dissolving for node 256 of 466 at time 2024-05-10 09:02:41.637056\n",
      "Dissolving for node 257 of 466 at time 2024-05-10 09:02:52.732302\n",
      "Dissolving for node 258 of 466 at time 2024-05-10 09:03:03.426177\n",
      "Dissolving for node 259 of 466 at time 2024-05-10 09:03:14.828707\n",
      "Dissolving for node 260 of 466 at time 2024-05-10 09:03:25.611969\n",
      "Dissolving for node 261 of 466 at time 2024-05-10 09:03:37.201672\n",
      "Dissolving for node 262 of 466 at time 2024-05-10 09:03:47.826483\n",
      "Dissolving for node 263 of 466 at time 2024-05-10 09:03:58.567402\n",
      "Dissolving for node 264 of 466 at time 2024-05-10 09:04:10.266205\n",
      "Dissolving for node 265 of 466 at time 2024-05-10 09:04:20.724864\n",
      "Dissolving for node 266 of 466 at time 2024-05-10 09:04:31.037387\n",
      "Dissolving for node 267 of 466 at time 2024-05-10 09:04:41.400957\n",
      "Dissolving for node 268 of 466 at time 2024-05-10 09:04:52.696388\n",
      "Dissolving for node 269 of 466 at time 2024-05-10 09:05:03.315194\n",
      "Dissolving for node 270 of 466 at time 2024-05-10 09:05:14.865422\n",
      "Dissolving for node 271 of 466 at time 2024-05-10 09:05:25.729964\n",
      "Dissolving for node 272 of 466 at time 2024-05-10 09:05:36.617678\n",
      "Dissolving for node 273 of 466 at time 2024-05-10 09:05:47.449236\n",
      "Dissolving for node 274 of 466 at time 2024-05-10 09:05:59.135116\n",
      "Dissolving for node 275 of 466 at time 2024-05-10 09:06:09.828585\n",
      "Dissolving for node 276 of 466 at time 2024-05-10 09:06:20.799737\n",
      "Dissolving for node 277 of 466 at time 2024-05-10 09:06:31.918508\n",
      "Dissolving for node 278 of 466 at time 2024-05-10 09:06:42.928178\n",
      "Dissolving for node 279 of 466 at time 2024-05-10 09:06:56.851555\n",
      "Dissolving for node 280 of 466 at time 2024-05-10 09:07:07.430361\n",
      "Dissolving for node 281 of 466 at time 2024-05-10 09:07:18.492112\n",
      "Dissolving for node 282 of 466 at time 2024-05-10 09:07:29.214532\n",
      "Dissolving for node 283 of 466 at time 2024-05-10 09:07:40.414372\n",
      "Dissolving for node 284 of 466 at time 2024-05-10 09:07:51.135770\n",
      "Dissolving for node 285 of 466 at time 2024-05-10 09:08:02.018875\n",
      "Dissolving for node 286 of 466 at time 2024-05-10 09:08:13.253792\n",
      "Dissolving for node 287 of 466 at time 2024-05-10 09:08:24.335575\n",
      "Dissolving for node 288 of 466 at time 2024-05-10 09:08:35.469374\n",
      "Dissolving for node 289 of 466 at time 2024-05-10 09:08:46.745815\n",
      "Dissolving for node 290 of 466 at time 2024-05-10 09:08:58.851041\n",
      "Dissolving for node 291 of 466 at time 2024-05-10 09:09:10.077572\n",
      "Dissolving for node 292 of 466 at time 2024-05-10 09:09:20.741506\n",
      "Dissolving for node 293 of 466 at time 2024-05-10 09:09:31.839901\n",
      "Dissolving for node 294 of 466 at time 2024-05-10 09:09:42.746649\n",
      "Dissolving for node 295 of 466 at time 2024-05-10 09:09:54.584128\n",
      "Dissolving for node 296 of 466 at time 2024-05-10 09:10:05.898577\n",
      "Dissolving for node 297 of 466 at time 2024-05-10 09:10:20.400968\n",
      "Dissolving for node 298 of 466 at time 2024-05-10 09:10:32.051242\n",
      "Dissolving for node 299 of 466 at time 2024-05-10 09:10:42.934791\n",
      "Dissolving for node 300 of 466 at time 2024-05-10 09:10:53.926146\n",
      "Dissolving for node 301 of 466 at time 2024-05-10 09:11:06.034335\n",
      "Dissolving for node 302 of 466 at time 2024-05-10 09:11:17.686612\n",
      "Dissolving for node 303 of 466 at time 2024-05-10 09:11:30.277239\n",
      "Dissolving for node 304 of 466 at time 2024-05-10 09:11:41.950039\n",
      "Dissolving for node 305 of 466 at time 2024-05-10 09:11:53.198932\n",
      "Dissolving for node 306 of 466 at time 2024-05-10 09:12:04.079981\n",
      "Dissolving for node 307 of 466 at time 2024-05-10 09:12:15.464039\n",
      "Dissolving for node 308 of 466 at time 2024-05-10 09:12:27.571861\n",
      "Dissolving for node 309 of 466 at time 2024-05-10 09:12:38.932563\n",
      "Dissolving for node 310 of 466 at time 2024-05-10 09:12:50.684084\n",
      "Dissolving for node 311 of 466 at time 2024-05-10 09:13:02.610120\n",
      "Dissolving for node 312 of 466 at time 2024-05-10 09:13:14.256417\n",
      "Dissolving for node 313 of 466 at time 2024-05-10 09:13:26.400663\n",
      "Dissolving for node 314 of 466 at time 2024-05-10 09:13:37.715652\n",
      "Dissolving for node 315 of 466 at time 2024-05-10 09:13:49.921549\n",
      "Dissolving for node 316 of 466 at time 2024-05-10 09:14:01.695534\n",
      "Dissolving for node 317 of 466 at time 2024-05-10 09:14:13.978990\n",
      "Dissolving for node 318 of 466 at time 2024-05-10 09:14:26.250001\n",
      "Dissolving for node 319 of 466 at time 2024-05-10 09:14:39.036038\n",
      "Dissolving for node 320 of 466 at time 2024-05-10 09:14:50.938828\n",
      "Dissolving for node 321 of 466 at time 2024-05-10 09:15:03.021220\n",
      "Dissolving for node 322 of 466 at time 2024-05-10 09:15:14.646610\n",
      "Dissolving for node 323 of 466 at time 2024-05-10 09:15:25.999684\n",
      "Dissolving for node 324 of 466 at time 2024-05-10 09:15:38.136437\n",
      "Dissolving for node 325 of 466 at time 2024-05-10 09:15:49.636556\n",
      "Dissolving for node 326 of 466 at time 2024-05-10 09:16:00.953003\n",
      "Dissolving for node 327 of 466 at time 2024-05-10 09:16:13.273397\n",
      "Dissolving for node 328 of 466 at time 2024-05-10 09:16:24.721485\n",
      "Dissolving for node 329 of 466 at time 2024-05-10 09:16:36.345725\n",
      "Dissolving for node 330 of 466 at time 2024-05-10 09:16:48.824786\n",
      "Dissolving for node 331 of 466 at time 2024-05-10 09:17:00.736937\n",
      "Dissolving for node 332 of 466 at time 2024-05-10 09:17:12.160270\n",
      "Dissolving for node 333 of 466 at time 2024-05-10 09:17:24.046051\n",
      "Dissolving for node 334 of 466 at time 2024-05-10 09:17:35.487760\n",
      "Dissolving for node 335 of 466 at time 2024-05-10 09:17:46.841797\n",
      "Dissolving for node 336 of 466 at time 2024-05-10 09:17:58.320414\n",
      "Dissolving for node 337 of 466 at time 2024-05-10 09:18:09.396416\n",
      "Dissolving for node 338 of 466 at time 2024-05-10 09:18:21.389789\n",
      "Dissolving for node 339 of 466 at time 2024-05-10 09:18:33.210780\n",
      "Dissolving for node 340 of 466 at time 2024-05-10 09:18:44.372685\n",
      "Dissolving for node 341 of 466 at time 2024-05-10 09:18:56.721202\n",
      "Dissolving for node 342 of 466 at time 2024-05-10 09:19:08.723467\n",
      "Dissolving for node 343 of 466 at time 2024-05-10 09:19:20.597184\n",
      "Dissolving for node 344 of 466 at time 2024-05-10 09:19:32.640505\n",
      "Dissolving for node 345 of 466 at time 2024-05-10 09:19:44.581671\n",
      "Dissolving for node 346 of 466 at time 2024-05-10 09:19:57.302531\n",
      "Dissolving for node 347 of 466 at time 2024-05-10 09:20:09.241104\n",
      "Dissolving for node 348 of 466 at time 2024-05-10 09:20:21.464911\n",
      "Dissolving for node 349 of 466 at time 2024-05-10 09:20:32.790859\n",
      "Dissolving for node 350 of 466 at time 2024-05-10 09:20:45.022642\n",
      "Dissolving for node 351 of 466 at time 2024-05-10 09:20:57.141342\n",
      "Dissolving for node 352 of 466 at time 2024-05-10 09:21:09.287052\n",
      "Dissolving for node 353 of 466 at time 2024-05-10 09:21:21.348696\n",
      "Dissolving for node 354 of 466 at time 2024-05-10 09:21:33.157204\n",
      "Dissolving for node 355 of 466 at time 2024-05-10 09:21:45.321651\n",
      "Dissolving for node 356 of 466 at time 2024-05-10 09:21:57.289419\n",
      "Dissolving for node 357 of 466 at time 2024-05-10 09:22:09.975688\n",
      "Dissolving for node 358 of 466 at time 2024-05-10 09:22:22.463771\n",
      "Dissolving for node 359 of 466 at time 2024-05-10 09:22:35.222123\n",
      "Dissolving for node 360 of 466 at time 2024-05-10 09:22:47.524611\n",
      "Dissolving for node 361 of 466 at time 2024-05-10 09:22:59.422206\n",
      "Dissolving for node 362 of 466 at time 2024-05-10 09:23:12.134508\n",
      "Dissolving for node 363 of 466 at time 2024-05-10 09:23:24.225774\n",
      "Dissolving for node 364 of 466 at time 2024-05-10 09:23:36.124363\n",
      "Dissolving for node 365 of 466 at time 2024-05-10 09:23:48.343761\n",
      "Dissolving for node 366 of 466 at time 2024-05-10 09:24:00.375509\n",
      "Dissolving for node 367 of 466 at time 2024-05-10 09:24:12.909261\n",
      "Dissolving for node 368 of 466 at time 2024-05-10 09:24:24.976045\n",
      "Dissolving for node 369 of 466 at time 2024-05-10 09:24:37.146430\n",
      "Dissolving for node 370 of 466 at time 2024-05-10 09:24:49.315272\n",
      "Dissolving for node 371 of 466 at time 2024-05-10 09:25:01.556676\n",
      "Dissolving for node 372 of 466 at time 2024-05-10 09:25:13.715495\n",
      "Dissolving for node 373 of 466 at time 2024-05-10 09:25:26.009907\n",
      "Dissolving for node 374 of 466 at time 2024-05-10 09:25:38.523972\n",
      "Dissolving for node 375 of 466 at time 2024-05-10 09:25:50.974516\n",
      "Dissolving for node 376 of 466 at time 2024-05-10 09:26:04.180314\n",
      "Dissolving for node 377 of 466 at time 2024-05-10 09:26:16.577859\n",
      "Dissolving for node 378 of 466 at time 2024-05-10 09:26:29.277302\n",
      "Dissolving for node 379 of 466 at time 2024-05-10 09:26:41.328540\n",
      "Dissolving for node 380 of 466 at time 2024-05-10 09:26:53.965438\n",
      "Dissolving for node 381 of 466 at time 2024-05-10 09:27:06.649593\n",
      "Dissolving for node 382 of 466 at time 2024-05-10 09:27:20.886862\n",
      "Dissolving for node 383 of 466 at time 2024-05-10 09:27:33.843408\n",
      "Dissolving for node 384 of 466 at time 2024-05-10 09:27:47.011628\n",
      "Dissolving for node 385 of 466 at time 2024-05-10 09:28:00.759929\n",
      "Dissolving for node 386 of 466 at time 2024-05-10 09:28:13.745036\n",
      "Dissolving for node 387 of 466 at time 2024-05-10 09:28:26.424877\n",
      "Dissolving for node 388 of 466 at time 2024-05-10 09:28:39.392516\n",
      "Dissolving for node 389 of 466 at time 2024-05-10 09:28:53.110842\n",
      "Dissolving for node 390 of 466 at time 2024-05-10 09:29:05.479899\n",
      "Dissolving for node 391 of 466 at time 2024-05-10 09:29:18.172768\n",
      "Dissolving for node 392 of 466 at time 2024-05-10 09:29:31.378137\n",
      "Dissolving for node 393 of 466 at time 2024-05-10 09:29:44.415296\n",
      "Dissolving for node 394 of 466 at time 2024-05-10 09:29:57.377094\n",
      "Dissolving for node 395 of 466 at time 2024-05-10 09:30:10.615356\n",
      "Dissolving for node 396 of 466 at time 2024-05-10 09:30:23.827122\n",
      "Dissolving for node 397 of 466 at time 2024-05-10 09:30:36.568867\n",
      "Dissolving for node 398 of 466 at time 2024-05-10 09:30:49.127005\n",
      "Dissolving for node 399 of 466 at time 2024-05-10 09:31:02.128637\n",
      "Dissolving for node 400 of 466 at time 2024-05-10 09:31:16.131231\n",
      "Dissolving for node 401 of 466 at time 2024-05-10 09:31:29.261942\n",
      "Dissolving for node 402 of 466 at time 2024-05-10 09:31:42.512218\n",
      "Dissolving for node 403 of 466 at time 2024-05-10 09:31:55.931748\n",
      "Dissolving for node 404 of 466 at time 2024-05-10 09:32:12.463079\n",
      "Dissolving for node 405 of 466 at time 2024-05-10 09:32:25.656350\n",
      "Dissolving for node 406 of 466 at time 2024-05-10 09:32:38.914199\n",
      "Dissolving for node 407 of 466 at time 2024-05-10 09:32:51.955397\n",
      "Dissolving for node 408 of 466 at time 2024-05-10 09:33:04.988084\n",
      "Dissolving for node 409 of 466 at time 2024-05-10 09:33:18.398775\n",
      "Dissolving for node 410 of 466 at time 2024-05-10 09:33:31.720238\n",
      "Dissolving for node 411 of 466 at time 2024-05-10 09:33:48.022368\n",
      "Dissolving for node 412 of 466 at time 2024-05-10 09:34:01.459326\n",
      "Dissolving for node 413 of 466 at time 2024-05-10 09:34:19.828318\n",
      "Dissolving for node 414 of 466 at time 2024-05-10 09:34:34.076494\n",
      "Dissolving for node 415 of 466 at time 2024-05-10 09:34:46.425382\n",
      "Dissolving for node 416 of 466 at time 2024-05-10 09:34:59.482918\n",
      "Dissolving for node 417 of 466 at time 2024-05-10 09:35:12.646062\n",
      "Dissolving for node 418 of 466 at time 2024-05-10 09:35:34.051142\n",
      "Dissolving for node 419 of 466 at time 2024-05-10 09:35:46.463840\n",
      "Dissolving for node 420 of 466 at time 2024-05-10 09:35:58.813358\n",
      "Dissolving for node 421 of 466 at time 2024-05-10 09:36:15.119409\n",
      "Dissolving for node 422 of 466 at time 2024-05-10 09:36:31.211270\n",
      "Dissolving for node 423 of 466 at time 2024-05-10 09:36:48.612428\n",
      "Dissolving for node 424 of 466 at time 2024-05-10 09:37:01.462876\n",
      "Dissolving for node 425 of 466 at time 2024-05-10 09:37:14.427935\n",
      "Dissolving for node 426 of 466 at time 2024-05-10 09:37:27.627780\n",
      "Dissolving for node 427 of 466 at time 2024-05-10 09:37:40.246123\n",
      "Dissolving for node 428 of 466 at time 2024-05-10 09:37:53.911501\n",
      "Dissolving for node 429 of 466 at time 2024-05-10 09:38:06.799053\n",
      "Dissolving for node 430 of 466 at time 2024-05-10 09:38:19.676070\n",
      "Dissolving for node 431 of 466 at time 2024-05-10 09:38:33.061503\n",
      "Dissolving for node 432 of 466 at time 2024-05-10 09:38:45.830802\n",
      "Dissolving for node 433 of 466 at time 2024-05-10 09:38:58.785255\n",
      "Dissolving for node 434 of 466 at time 2024-05-10 09:39:12.056491\n",
      "Dissolving for node 435 of 466 at time 2024-05-10 09:39:25.515898\n",
      "Dissolving for node 436 of 466 at time 2024-05-10 09:39:38.945288\n",
      "Dissolving for node 437 of 466 at time 2024-05-10 09:39:52.529345\n",
      "Dissolving for node 438 of 466 at time 2024-05-10 09:40:06.302717\n",
      "Dissolving for node 439 of 466 at time 2024-05-10 09:40:19.588742\n",
      "Dissolving for node 440 of 466 at time 2024-05-10 09:40:33.088484\n",
      "Dissolving for node 441 of 466 at time 2024-05-10 09:40:47.309684\n",
      "Dissolving for node 442 of 466 at time 2024-05-10 09:41:01.213047\n",
      "Dissolving for node 443 of 466 at time 2024-05-10 09:41:14.249598\n",
      "Dissolving for node 444 of 466 at time 2024-05-10 09:41:27.309218\n",
      "Dissolving for node 445 of 466 at time 2024-05-10 09:41:40.272251\n",
      "Dissolving for node 446 of 466 at time 2024-05-10 09:41:53.739763\n",
      "Dissolving for node 447 of 466 at time 2024-05-10 09:42:08.115630\n",
      "Dissolving for node 448 of 466 at time 2024-05-10 09:42:23.633037\n",
      "Dissolving for node 449 of 466 at time 2024-05-10 09:42:37.124512\n",
      "Dissolving for node 450 of 466 at time 2024-05-10 09:42:51.404354\n",
      "Dissolving for node 451 of 466 at time 2024-05-10 09:43:05.560605\n",
      "Dissolving for node 452 of 466 at time 2024-05-10 09:43:28.480068\n",
      "Dissolving for node 453 of 466 at time 2024-05-10 09:43:43.015021\n",
      "Dissolving for node 454 of 466 at time 2024-05-10 09:43:58.071880\n",
      "Dissolving for node 455 of 466 at time 2024-05-10 09:44:13.197435\n",
      "Dissolving for node 456 of 466 at time 2024-05-10 09:44:27.504253\n",
      "Dissolving for node 457 of 466 at time 2024-05-10 09:44:41.534721\n",
      "Dissolving for node 458 of 466 at time 2024-05-10 09:44:55.260691\n",
      "Dissolving for node 459 of 466 at time 2024-05-10 09:45:09.416260\n",
      "Dissolving for node 460 of 466 at time 2024-05-10 09:45:24.081287\n",
      "Dissolving for node 461 of 466 at time 2024-05-10 09:45:38.416107\n",
      "Dissolving for node 462 of 466 at time 2024-05-10 09:45:52.187170\n",
      "Dissolving for node 463 of 466 at time 2024-05-10 09:46:06.436310\n",
      "Dissolving for node 464 of 466 at time 2024-05-10 09:46:20.990411\n",
      "Dissolving for node 465 of 466 at time 2024-05-10 09:46:35.168731\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 10\n",
    "#Dissolve catchments\n",
    "\n",
    "nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "for i, node in enumerate(nodes):\n",
    "    print('Dissolving for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "    catchment_df = accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node]\n",
    "    catchments = list(catchment_df[('GENERAL INFO','CATCHMENT')].unique())\n",
    "    arcpy.management.CalculateField('msm_Catchment', \"Drains_To\", \"''\", \"PYTHON3\")\n",
    "    with arcpy.da.UpdateCursor('msm_catchment', ['muid', 'Drains_To']) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0] in catchments:\n",
    "                row[1] = node\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "    query = \"Drains_To = 'Test'\"\n",
    "    arcpy.management.MakeFeatureLayer('msm_catchment', \"temp_layer\", \"Drains_To = '\" + node + \"'\")\n",
    "    dissolve_output = out_path + '\\\\msm_Catchment_Dissolve_Single'\n",
    "    arcpy.management.Dissolve(\"temp_layer\", dissolve_output, \"Drains_To\", \"\", \"MULTI_PART\")\n",
    "    arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "    arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment_Dissolve_Single', out_path, 'Node_Catchment_' + node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending for node 0 of 466 at time 2024-05-10 14:37:37.026020\n",
      "Appending for node 1 of 466 at time 2024-05-10 14:37:44.709064\n",
      "Appending for node 2 of 466 at time 2024-05-10 14:37:47.337474\n",
      "Appending for node 3 of 466 at time 2024-05-10 14:37:49.882808\n",
      "Appending for node 4 of 466 at time 2024-05-10 14:37:52.131870\n",
      "Appending for node 5 of 466 at time 2024-05-10 14:37:54.349903\n",
      "Appending for node 6 of 466 at time 2024-05-10 14:37:57.453749\n",
      "Appending for node 7 of 466 at time 2024-05-10 14:37:59.886980\n",
      "Appending for node 8 of 466 at time 2024-05-10 14:38:02.080992\n",
      "Appending for node 9 of 466 at time 2024-05-10 14:38:04.573277\n",
      "Appending for node 10 of 466 at time 2024-05-10 14:38:06.889400\n",
      "Appending for node 11 of 466 at time 2024-05-10 14:38:09.293605\n",
      "Appending for node 12 of 466 at time 2024-05-10 14:38:12.188259\n",
      "Appending for node 13 of 466 at time 2024-05-10 14:38:14.962803\n",
      "Appending for node 14 of 466 at time 2024-05-10 14:38:17.457089\n",
      "Appending for node 15 of 466 at time 2024-05-10 14:38:19.894326\n",
      "Appending for node 16 of 466 at time 2024-05-10 14:38:22.380604\n",
      "Appending for node 17 of 466 at time 2024-05-10 14:38:24.701732\n",
      "Appending for node 18 of 466 at time 2024-05-10 14:38:27.543337\n",
      "Appending for node 19 of 466 at time 2024-05-10 14:38:29.870471\n",
      "Appending for node 20 of 466 at time 2024-05-10 14:38:32.582958\n",
      "Appending for node 21 of 466 at time 2024-05-10 14:38:34.952130\n",
      "Appending for node 22 of 466 at time 2024-05-10 14:38:37.469114\n",
      "Appending for node 23 of 466 at time 2024-05-10 14:38:40.151573\n",
      "Appending for node 24 of 466 at time 2024-05-10 14:38:42.695906\n",
      "Appending for node 25 of 466 at time 2024-05-10 14:38:45.020037\n",
      "Appending for node 26 of 466 at time 2024-05-10 14:38:47.548355\n",
      "Appending for node 27 of 466 at time 2024-05-10 14:38:49.931540\n",
      "Appending for node 28 of 466 at time 2024-05-10 14:38:52.402806\n",
      "Appending for node 29 of 466 at time 2024-05-10 14:38:55.039223\n",
      "Appending for node 30 of 466 at time 2024-05-10 14:38:57.711674\n",
      "Appending for node 31 of 466 at time 2024-05-10 14:39:00.249000\n",
      "Appending for node 32 of 466 at time 2024-05-10 14:39:02.669220\n",
      "Appending for node 33 of 466 at time 2024-05-10 14:39:05.205544\n",
      "Appending for node 34 of 466 at time 2024-05-10 14:39:07.708840\n",
      "Appending for node 35 of 466 at time 2024-05-10 14:39:10.167094\n",
      "Appending for node 36 of 466 at time 2024-05-10 14:39:12.771481\n",
      "Appending for node 37 of 466 at time 2024-05-10 14:39:15.346844\n",
      "Appending for node 38 of 466 at time 2024-05-10 14:39:17.837126\n",
      "Appending for node 39 of 466 at time 2024-05-10 14:39:20.299384\n",
      "Appending for node 40 of 466 at time 2024-05-10 14:39:22.904772\n",
      "Appending for node 41 of 466 at time 2024-05-10 14:39:25.415074\n",
      "Appending for node 42 of 466 at time 2024-05-10 14:39:27.923374\n",
      "Appending for node 43 of 466 at time 2024-05-10 14:39:30.386633\n",
      "Appending for node 44 of 466 at time 2024-05-10 14:39:32.986015\n",
      "Appending for node 45 of 466 at time 2024-05-10 14:39:35.571387\n",
      "Appending for node 46 of 466 at time 2024-05-10 14:39:38.181781\n",
      "Appending for node 47 of 466 at time 2024-05-10 14:39:40.697086\n",
      "Appending for node 48 of 466 at time 2024-05-10 14:39:43.254430\n",
      "Appending for node 49 of 466 at time 2024-05-10 14:39:45.780747\n",
      "Appending for node 50 of 466 at time 2024-05-10 14:39:48.355108\n",
      "Appending for node 51 of 466 at time 2024-05-10 14:39:51.013544\n",
      "Appending for node 52 of 466 at time 2024-05-10 14:39:53.518344\n",
      "Appending for node 53 of 466 at time 2024-05-10 14:39:56.360950\n",
      "Appending for node 54 of 466 at time 2024-05-10 14:39:58.915294\n",
      "Appending for node 55 of 466 at time 2024-05-10 14:40:01.452619\n",
      "Appending for node 56 of 466 at time 2024-05-10 14:40:04.046997\n",
      "Appending for node 57 of 466 at time 2024-05-10 14:40:06.617354\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "#Append individual dissolved catchments to one layer.\n",
    "\n",
    "nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "for i, node in enumerate(nodes[408:]):    \n",
    "    print('Appending for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "    if i == 0:\n",
    "        arcpy.conversion.FeatureClassToFeatureClass('Node_Catchment_' + node, out_path, 'Node_Catchment')\n",
    "    else:\n",
    "        arcpy.management.Append('Node_Catchment_' + node, \"Node_Catchment\", \"NO_TEST\")\n",
    "    arcpy.management.Delete('Node_Catchment_' + node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7113'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7113'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 220 of 466\n",
      "Printing jpg 221 of 466\n",
      "Printing jpg 222 of 466\n",
      "Printing jpg 223 of 466\n",
      "Printing jpg 224 of 466\n",
      "Printing jpg 225 of 466\n",
      "Printing jpg 226 of 466\n",
      "Printing jpg 227 of 466\n",
      "Printing jpg 228 of 466\n",
      "Printing jpg 229 of 466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "In  \u001b[0;34m[133]\u001b[0m:\nLine \u001b[0;34m22\u001b[0m:    layout.exportToJPEG(output_filename, resolution=\u001b[34m300\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\utils.py\u001b[0m, in \u001b[0;32mfn_\u001b[0m:\nLine \u001b[0;34m191\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m fn(*args, **kw)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\_mp.py\u001b[0m, in \u001b[0;32mexportToJPEG\u001b[0m:\nLine \u001b[0;34m1680\u001b[0m:  \u001b[34mreturn\u001b[39;49;00m convertArcObjectToPythonObject(\u001b[36mself\u001b[39;49;00m._arc_object.exportToJPEG(*gp_fixargs((out_jpg, resolution, \u001b[34mFalse\u001b[39;49;00m, jpeg_color_mode, jpeg_quality, embed_color_profile, clip_to_elements), \u001b[34mTrue\u001b[39;49;00m)))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "#Export jpgs\n",
    "\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "project_path = aprx.filePath\n",
    "\n",
    "jpg_folder = output_folder + r'\\HTML\\Maps_And_CSS'\n",
    "if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "    \n",
    "\n",
    "# project_directory = os.path.dirname(project_path)\n",
    "\n",
    "layouts = aprx.listLayouts()\n",
    "\n",
    "for layout in layouts:\n",
    "\n",
    "    if layout.mapSeries is not None:\n",
    "        map_series = layout.mapSeries\n",
    "        # Loop through all pages in the map series\n",
    "        for page_number in range(1, map_series.pageCount + 1):\n",
    "            map_series.currentPageNumber = page_number\n",
    "            output_filename = os.path.join(jpg_folder, f\"{map_series.pageRow.Drains_To}.jpg\")\n",
    "            layout.exportToJPEG(output_filename, resolution=300)\n",
    "            print (f'Printing jpg {page_number} of {map_series.pageCount}')\n",
    "\n",
    "print(\"Export complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "del layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "#Create HTMLs\n",
    "\n",
    "\n",
    "shutil.copy2('style.css', html_folder + '\\\\style.css')\n",
    "shutil.copy2('script.js', html_folder + '\\\\script.js')\n",
    "\n",
    "for category in categories:\n",
    "    area_type = category[0]\n",
    "    area_names = category[1]\n",
    "    header_start = category[2]\n",
    "    \n",
    "    f = open(html_folder + '\\\\Population_By_' + area_type + '_' + model_area + '.html', \"w\")\n",
    "    f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "    f.write('<script src=\"script.js\"></script>\\n')\n",
    "    f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "    f.write('<!DOCTYPE html>\\n')\n",
    "    f.write('<html>\\n')\n",
    "    f.write('<head>\\n')\n",
    "    f.write('<meta charset=\"utf-8\">\\n')\n",
    "    f.write('</head>\\n')\n",
    "    f.write('<body>\\n\\n')\n",
    "\n",
    "    f.write('<div class=\"tab\">\\n')\n",
    "    for area_name in area_names:\n",
    "        tab = area_name\n",
    "\n",
    "    #     color = ps_dict[first_year]\n",
    "    #     bg_color = color_dict[color][0]\n",
    "    #     text_color = color_dict[color][1]\n",
    "\n",
    "        f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "    f.write('</div>\\n')\n",
    "    \n",
    "    pop_df = pop_dfss[0][2]\n",
    "\n",
    "    for area_name in area_names:\n",
    "        \n",
    "        area_df = pop_df[pop_df[area_type]==area_name]\n",
    "        area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "\n",
    "        f.write('<div id=\"' + area_name + '\" class=\"tabcontent\">\\n') \n",
    "        f.write('<h1>' + area_name + '</h1>\\n')\n",
    "\n",
    "        f.write('<div class=\"sidenav\">\\n')\n",
    "\n",
    "        f.write('<table style=\\'width: 90%;\\'>\\n')\n",
    "        f.write('<tr>\\n')\n",
    "        f.write('<th>Year</th>\\n')\n",
    "        f.write('<th>Population</th>\\n')\n",
    "        f.write('</tr>\\n')\n",
    "\n",
    "        for index, row in area_df.iterrows():\n",
    "            f.write('<tr>\\n')\n",
    "            f.write('<td>'+ str(index) + '</td>\\n')\n",
    "            population_with_separator = f\"{int(row['Population']):,}\"\n",
    "            f.write('<td>'+ population_with_separator + '</td>\\n')\n",
    "\n",
    "            f.write('</tr>\\n')\n",
    "        f.write('</table>\\n')\n",
    "        \n",
    "        for i in range(4):\n",
    "            f.write('<h1 style=\"color: white\">End of tables</h1>\\n')#Invisible, just to enable scroll to table bottoms\n",
    "\n",
    "        f.write('</div>\\n') #end sidenav\n",
    "\n",
    "\n",
    "        f.write('<div class=\"main\">\\n')\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                     y = area_df.Population, \n",
    "                                     mode='lines',name=pop_dfss[0][0],line=dict(width=5)))\n",
    "        \n",
    "        for pop_dfs in pop_dfss[1:]:\n",
    "            pop_df_past = pop_dfs[2]\n",
    "            area_df = pop_df_past[pop_df_past[area_type]==area_name]\n",
    "            area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "            fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                     y = area_df.Population, \n",
    "                                     mode='lines',name=pop_dfs[0],line=dict(width=2)))\n",
    "                    \n",
    "        fig.update_layout(\n",
    "            title=header_start + area_name,\n",
    "            autosize=False,\n",
    "            width = 1500,\n",
    "            height=850,\n",
    "            margin=dict(\n",
    "                l=50,\n",
    "                r=50,\n",
    "                b=50,\n",
    "                t=50,\n",
    "                pad=4\n",
    "                ),\n",
    "                yaxis_title = 'Population'\n",
    "            )\n",
    "        \n",
    "        f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "        f.write('</div>\\n') #end div main  \n",
    "\n",
    "        f.write('</div>\\n')  #end div tab   \n",
    "\n",
    "        f.write('</body>\\n')\n",
    "    f.write('</html>\\n')\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J:\\\\SEWER_AREA_MODELS\\\\FSA\\\\04_ANALYSIS_WORK\\\\Model_Result_To_GIS\\\\Automation\\\\Rawn_Tool\\\\Output\\\\HTML\\\\Maps_And_CSS'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpg_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample DataFrame\n",
    "# data = {\n",
    "#     'Catchment': [1, 2, 3],\n",
    "#     'Node': [4, 5, 6]\n",
    "\n",
    "# }\n",
    "# accumulation_df = pd.DataFrame(data)\n",
    "\n",
    "# # Create a MultiIndex with the existing columns\n",
    "# existing_columns_multiindex = pd.MultiIndex.from_tuples([\n",
    "#     ('GENERAL INFO', 'Catchment'),  # Header with no subheaders\n",
    "#     ('GENERAL INFO', 'Node'),  # Header with no subheaders\n",
    "# ])\n",
    "\n",
    "# # Create a MultiIndex with the upper level 'GENERAL INFO'\n",
    "# upper_level = [('GENERAL INFO', '')] * len(existing_columns_multiindex)\n",
    "\n",
    "# # Concatenate the upper level and the existing columns MultiIndex\n",
    "# new_columns_multiindex = pd.MultiIndex.from_tuples(list(zip(upper_level, existing_columns_multiindex)))\n",
    "\n",
    "# # Assign the new MultiIndex to the DataFrame columns\n",
    "# accumulation_df.columns = new_columns_multiindex\n",
    "\n",
    "# accumulation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Sample data for the DataFrame\n",
    "# data = {\n",
    "#     ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "#     ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame with MultiIndex columns\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Set names for the levels of the MultiIndex\n",
    "# # df.columns.names = ['Header', 'Subheader']\n",
    "\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
