{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-06-19, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model to list the upstream catchments of each manhole (in flow splits, each split direction will both get 100% of the upstream flow). It then uses the MPF sheets to summarise population and residential/ICI areas for each catchment. Aggregated catchment contributions to each manhole are then used as input to RAWN calculations. For each manhole, the upstream catchments are dissolved together and added to a layer 'Node_Catchment'. This is used as input to 'Map Series' (similar to 'Data Driven Pages' in ArcGIS Desktop). This is used to create one jpg image per manhole, highlighting the manhole and all its upstream catchments. The RAWN calculations and images are added to RAWN spreadsheets which are similar in layout to the original RAWN sheets which are derived from Neural Network. One important difference in RAWN calculations to previous is that no peaking factor is allowed to go below 1.5, as per official RAWN guidelines. Peaking factors that reach the minimum are highlighted in yellow. Also a Google Maps link is added to each sheet. The user can choose between adding RAWN calculations as values or Excel formulas. Excel formulas are recommended for full transparency of the calculations to those using the sheets. However, python calculations are maintained in the code to enable future additional outputs (Power BI, HTML etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, numbers\n",
    "from openpyxl.formatting.rule import FormulaRule\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "from multiprocessing import Process\n",
    "import gc\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "\n",
    "#Create functions for to use SQL with the model sqlite database.\n",
    "\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "\n",
    "#User Input, to move to separate sheet so no permanent cell\n",
    "\n",
    "\n",
    "model = 'FSA'\n",
    "\n",
    "max_steps = 1000 \n",
    "use_formula = True\n",
    "\n",
    "check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "#                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "#                      A more robust check may be developed in the future.\n",
    "\n",
    "global_output_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "\n",
    "if model == 'NSSA':   \n",
    "    model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "    model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "    pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "    pop_sheet = 'MPF Update 4'\n",
    "    scenario = 'Base'\n",
    "    wwtp_muid = '22602'\n",
    "    line_exclusions = []\n",
    "    years = [2060,2070,2080,2090,2100]\n",
    "    model_version = 89\n",
    "    excluded_acronyms_csv = ''\n",
    "    \n",
    "if model == 'FSA':   \n",
    "    model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "    model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\FSA_Base_2021pop.sqlite\"\n",
    "    pop_book = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File.xlsx\"\n",
    "    pop_sheet = 'MPF Update 16'\n",
    "    scenario = '2030_Network'\n",
    "    wwtp_muid = '1162'\n",
    "    line_exclusions = ['GoldenEar_Dummy_Pump','GoldenEarSSOLink','GoldenEar_SSO_Tank_Cell_Dummy_Link3','MH35_Orifice','44473']\n",
    "    years = [2060,2070,2076]\n",
    "    model_version = 150\n",
    "    excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model\\FSA_Excluded_Acronyms.csv\"\n",
    "    \n",
    "    \n",
    "sewer_area = model\n",
    "gdb_name = 'RAWN.gdb'\n",
    "gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "#Options to skip time consuming steps during debug (by setting to False), must be True during production runs.\n",
    "run_dissolve = True\n",
    "run_jpg = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "    \n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    #Items: [Keyword (upper Excel header),Type ('lower Excel header'),Average(lower Excel header),Unit flow cell address, quantifyer column]\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)','$D$3','H']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$4','K']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$5','N']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)','$D$6','Q']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)','$D$7','T']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)','$D$7','T']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning. The sum of 'Population' (113071710) is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (112676302)\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 5\n",
    "\n",
    "#Import population\n",
    "\n",
    "try:\n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df.fillna(0, inplace=True) #Fill NA with 0 or the sum of all will be NA\n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "        \n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "          print(\"Warning. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Link WHERE Active = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid == 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + str(altid)\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "    \n",
    "    sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "    \n",
    "    lines = lines[~lines['MUID'].isin(line_exclusions)]\n",
    "    \n",
    "    excluded_acronyms = []\n",
    "    if excluded_acronyms_csv != '':\n",
    "        excluded_acronyms = pd.read_csv(excluded_acronyms_csv)\n",
    "        excluded_acronyms = list(excluded_acronyms.iloc[:, 0])\n",
    "\n",
    "    sql = \"SELECT muid, acronym, owner, assetname FROM msm_Node WHERE active = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(((node_id_df.assetname.str[:2]=='MH') & (node_id_df.acronym.notna()) &  \n",
    "        (node_id_df.owner=='GV') & (~node_id_df.acronym.isin(excluded_acronyms))) | (node_id_df.muid==wwtp_muid))]\n",
    "    \n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df.loc[node_id_df['Node'] == wwtp_muid, 'ID'] = 'WWTP'\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "    duplicate_ids = node_id_df[node_id_df.duplicated('ID', keep=False)]\n",
    "    node_id_df.loc[duplicate_ids.index, 'ID'] = node_id_df['ID'] + '_(' + node_id_df['Node'] + ')'\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]  \n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Last node traced is '\" + node + \"'\")\n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl']:\n",
    "                    adwf += avg_flow    \n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)),1.5) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*0.8,1.5)*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)),1.5) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    \n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = np.maximum(\n",
    "        0.8 * (\n",
    "            1 + 14 / (\n",
    "                4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5\n",
    "            )\n",
    "        ) * np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ), \n",
    "        1.5\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    \n",
    "    node_df[('FLOWS','PWWF (L/s)')] = node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] + node_df[('COMMERCIAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] + node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] + node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending msm_CatchCon\n",
      "Appending msm_Catchment\n",
      "Appending msm_Link\n",
      "Appending msm_Node\n",
      "Appending msm_Pump\n",
      "Appending msm_Weir\n",
      "Appending msm_Orifice\n",
      "Appending msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "\n",
    "#Import GIS from the model\n",
    "#Note that if the RAWN database does not already exist with the layers inside, then it will be created \n",
    "#but in this casethe map symbology will be reset and Map Series reset as well, creating extra work.\n",
    "#An extra word of caution: The first import of e.g. msm_Catchment sets the maximum extend, meaning if you import NSSA in a fresh database,\n",
    "#then later import FSA, then many catchments will not be imported due to being outside the extend of the old import.\n",
    "#In the current database, an initial model with added elements outside the extend of all MV models was used to prime the layer. \n",
    "#If the database is not maintained, this will need to be done again.\n",
    "#Efforts to programmatically increase the layer extend were unsuccessful but may later get resolved.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(out_path + '\\\\' + layer):\n",
    "            print('Appending ' + layer)\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:  \n",
    "            print('Creating ' + layer)\n",
    "\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer + '_Test')\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        arcpy.Project_management('msm_Node', 'msm_Node_Google',arcpy.SpatialReference(4326))\n",
    "        centroids = arcpy.da.FeatureClassToNumPyArray('msm_Node_Google', (\"MUID\",\"SHAPE@X\",\"SHAPE@Y\"))\n",
    "        centroids = centroids.tolist()\n",
    "        centroids_df = pd.DataFrame(centroids, columns =['MUID','X','Y'])\n",
    "        centroids_df.set_index('MUID',inplace=True)\n",
    "\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "\n",
    "#Create merge_df (realizing the terms 'merge' and 'dissolve' are used here but they mean the same), \n",
    "#minimizing the number of computation heavy dissolves that need to be done.\n",
    "#It prevents the duplicate effort of merging the same catchments together multiple times,\n",
    "#by moving downstream and reusing upstream merges for downstream merges.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 11\n",
    "\n",
    "#Run dissolve (also referred to as merge).\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_name_dissolve)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df.iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 501 of 1772 at time 2024-06-20 10:35:25.008500\n",
      "Printing jpg 502 of 1772 at time 2024-06-20 10:35:27.543356\n",
      "Printing jpg 503 of 1772 at time 2024-06-20 10:35:30.058186\n",
      "Printing jpg 504 of 1772 at time 2024-06-20 10:35:32.575515\n",
      "Printing jpg 505 of 1772 at time 2024-06-20 10:35:35.110363\n",
      "Printing jpg 506 of 1772 at time 2024-06-20 10:35:37.773337\n",
      "Printing jpg 507 of 1772 at time 2024-06-20 10:35:40.327694\n",
      "Printing jpg 508 of 1772 at time 2024-06-20 10:35:42.976665\n",
      "Printing jpg 509 of 1772 at time 2024-06-20 10:35:45.510512\n",
      "Printing jpg 510 of 1772 at time 2024-06-20 10:35:48.025833\n",
      "Printing jpg 511 of 1772 at time 2024-06-20 10:35:50.559176\n",
      "Printing jpg 512 of 1772 at time 2024-06-20 10:35:53.226140\n",
      "WARNING! 7291 could not be made, try one more time\n",
      "WARNING! 7291 could still not be made.\n",
      "Printing jpg 513 of 1772 at time 2024-06-20 10:35:54.937725\n",
      "Printing jpg 514 of 1772 at time 2024-06-20 10:35:58.342892\n",
      "Printing jpg 515 of 1772 at time 2024-06-20 10:36:01.641948\n",
      "Printing jpg 516 of 1772 at time 2024-06-20 10:36:04.742319\n",
      "Printing jpg 517 of 1772 at time 2024-06-20 10:36:07.909259\n",
      "WARNING! 7379 could not be made, try one more time\n",
      "WARNING! 7379 could still not be made.\n",
      "Printing jpg 518 of 1772 at time 2024-06-20 10:36:10.119816\n",
      "Printing jpg 519 of 1772 at time 2024-06-20 10:36:13.826775\n",
      "Printing jpg 520 of 1772 at time 2024-06-20 10:36:17.609802\n",
      "WARNING! 7384 could not be made, try one more time\n",
      "WARNING! 7384 could still not be made.\n",
      "Printing jpg 521 of 1772 at time 2024-06-20 10:36:20.318307\n",
      "WARNING! 7385 could not be made, try one more time\n",
      "WARNING! 7385 could still not be made.\n",
      "Printing jpg 522 of 1772 at time 2024-06-20 10:36:30.052314\n",
      "Printing jpg 523 of 1772 at time 2024-06-20 10:36:34.826221\n",
      "Printing jpg 524 of 1772 at time 2024-06-20 10:36:39.326373\n",
      "Printing jpg 525 of 1772 at time 2024-06-20 10:36:43.942632\n",
      "Printing jpg 526 of 1772 at time 2024-06-20 10:36:48.525861\n",
      "Printing jpg 527 of 1772 at time 2024-06-20 10:36:53.144122\n",
      "Printing jpg 528 of 1772 at time 2024-06-20 10:36:57.709334\n",
      "Printing jpg 529 of 1772 at time 2024-06-20 10:37:02.175455\n",
      "Printing jpg 530 of 1772 at time 2024-06-20 10:37:06.659095\n",
      "Printing jpg 531 of 1772 at time 2024-06-20 10:37:11.158246\n",
      "Printing jpg 532 of 1772 at time 2024-06-20 10:37:15.659399\n",
      "Printing jpg 533 of 1772 at time 2024-06-20 10:37:20.275658\n",
      "Printing jpg 534 of 1772 at time 2024-06-20 10:37:24.725764\n",
      "Printing jpg 535 of 1772 at time 2024-06-20 10:37:29.326008\n",
      "Printing jpg 536 of 1772 at time 2024-06-20 10:37:33.810145\n",
      "Printing jpg 537 of 1772 at time 2024-06-20 10:37:38.176174\n",
      "Printing jpg 538 of 1772 at time 2024-06-20 10:37:42.642294\n",
      "Printing jpg 539 of 1772 at time 2024-06-20 10:37:47.009323\n",
      "Printing jpg 540 of 1772 at time 2024-06-20 10:37:51.510476\n",
      "Printing jpg 541 of 1772 at time 2024-06-20 10:37:55.925550\n",
      "Printing jpg 542 of 1772 at time 2024-06-20 10:38:00.692949\n",
      "Printing jpg 543 of 1772 at time 2024-06-20 10:38:05.376270\n",
      "Printing jpg 544 of 1772 at time 2024-06-20 10:38:09.993530\n",
      "Printing jpg 545 of 1772 at time 2024-06-20 10:38:14.709384\n",
      "Printing jpg 546 of 1772 at time 2024-06-20 10:38:19.493798\n",
      "Printing jpg 547 of 1772 at time 2024-06-20 10:38:23.893858\n",
      "Printing jpg 548 of 1772 at time 2024-06-20 10:38:28.342963\n",
      "Printing jpg 549 of 1772 at time 2024-06-20 10:38:32.960223\n",
      "Printing jpg 550 of 1772 at time 2024-06-20 10:38:37.377298\n",
      "Printing jpg 551 of 1772 at time 2024-06-20 10:38:41.843419\n",
      "Printing jpg 552 of 1772 at time 2024-06-20 10:38:46.276509\n",
      "Printing jpg 553 of 1772 at time 2024-06-20 10:38:50.794677\n",
      "Printing jpg 554 of 1772 at time 2024-06-20 10:38:55.350881\n",
      "Printing jpg 555 of 1772 at time 2024-06-20 10:38:59.994195\n",
      "Printing jpg 556 of 1772 at time 2024-06-20 10:39:04.643485\n",
      "Printing jpg 557 of 1772 at time 2024-06-20 10:39:09.126621\n",
      "Printing jpg 558 of 1772 at time 2024-06-20 10:39:13.977106\n",
      "Printing jpg 559 of 1772 at time 2024-06-20 10:39:18.793550\n",
      "Printing jpg 560 of 1772 at time 2024-06-20 10:39:23.610995\n",
      "Printing jpg 561 of 1772 at time 2024-06-20 10:39:28.427439\n",
      "Printing jpg 562 of 1772 at time 2024-06-20 10:39:33.259897\n",
      "Printing jpg 563 of 1772 at time 2024-06-20 10:39:38.093357\n",
      "Printing jpg 564 of 1772 at time 2024-06-20 10:39:42.977863\n",
      "Printing jpg 565 of 1772 at time 2024-06-20 10:39:47.943445\n",
      "Printing jpg 566 of 1772 at time 2024-06-20 10:39:52.211382\n",
      "Printing jpg 567 of 1772 at time 2024-06-20 10:39:56.443287\n",
      "Printing jpg 568 of 1772 at time 2024-06-20 10:40:00.744255\n",
      "Printing jpg 569 of 1772 at time 2024-06-20 10:40:05.160330\n",
      "Printing jpg 570 of 1772 at time 2024-06-20 10:40:09.878683\n",
      "Printing jpg 571 of 1772 at time 2024-06-20 10:40:14.493941\n",
      "Printing jpg 572 of 1772 at time 2024-06-20 10:40:22.277626\n",
      "Printing jpg 573 of 1772 at time 2024-06-20 10:40:27.277238\n",
      "Printing jpg 574 of 1772 at time 2024-06-20 10:40:31.895498\n",
      "Printing jpg 575 of 1772 at time 2024-06-20 10:40:36.695926\n",
      "Printing jpg 576 of 1772 at time 2024-06-20 10:40:41.311184\n",
      "Printing jpg 577 of 1772 at time 2024-06-20 10:40:45.778304\n",
      "Printing jpg 578 of 1772 at time 2024-06-20 10:40:50.311989\n",
      "Printing jpg 579 of 1772 at time 2024-06-20 10:40:54.877200\n",
      "Printing jpg 580 of 1772 at time 2024-06-20 10:40:59.244228\n",
      "Printing jpg 581 of 1772 at time 2024-06-20 10:41:03.878503\n",
      "Printing jpg 582 of 1772 at time 2024-06-20 10:41:08.511777\n",
      "Printing jpg 583 of 1772 at time 2024-06-20 10:41:13.161066\n",
      "Printing jpg 584 of 1772 at time 2024-06-20 10:41:17.794340\n",
      "Printing jpg 585 of 1772 at time 2024-06-20 10:41:22.294491\n",
      "Printing jpg 586 of 1772 at time 2024-06-20 10:41:31.495979\n",
      "Printing jpg 587 of 1772 at time 2024-06-20 10:41:36.229345\n",
      "Printing jpg 588 of 1772 at time 2024-06-20 10:41:43.677215\n",
      "Printing jpg 589 of 1772 at time 2024-06-20 10:41:48.111305\n",
      "Printing jpg 590 of 1772 at time 2024-06-20 10:41:52.428288\n",
      "Printing jpg 591 of 1772 at time 2024-06-20 10:41:56.745270\n",
      "Printing jpg 592 of 1772 at time 2024-06-20 10:42:01.061251\n",
      "Printing jpg 593 of 1772 at time 2024-06-20 10:42:05.478326\n",
      "Printing jpg 594 of 1772 at time 2024-06-20 10:42:09.895400\n",
      "Printing jpg 595 of 1772 at time 2024-06-20 10:42:14.312475\n",
      "Printing jpg 596 of 1772 at time 2024-06-20 10:42:18.728051\n",
      "Printing jpg 597 of 1772 at time 2024-06-20 10:42:23.128110\n",
      "Printing jpg 598 of 1772 at time 2024-06-20 10:42:27.562200\n",
      "Printing jpg 599 of 1772 at time 2024-06-20 10:42:31.978273\n",
      "Printing jpg 600 of 1772 at time 2024-06-20 10:42:36.612548\n",
      "Printing jpg 601 of 1772 at time 2024-06-20 10:42:41.244821\n",
      "Printing jpg 602 of 1772 at time 2024-06-20 10:42:45.996204\n",
      "Printing jpg 603 of 1772 at time 2024-06-20 10:42:50.628477\n",
      "Printing jpg 604 of 1772 at time 2024-06-20 10:42:55.296783\n",
      "Printing jpg 605 of 1772 at time 2024-06-20 10:43:02.979870\n",
      "Printing jpg 606 of 1772 at time 2024-06-20 10:43:07.645174\n",
      "Printing jpg 607 of 1772 at time 2024-06-20 10:43:12.178355\n",
      "Printing jpg 608 of 1772 at time 2024-06-20 10:43:16.728553\n",
      "Printing jpg 609 of 1772 at time 2024-06-20 10:43:21.095581\n",
      "Printing jpg 610 of 1772 at time 2024-06-20 10:43:25.479625\n",
      "Printing jpg 611 of 1772 at time 2024-06-20 10:43:29.944744\n",
      "Printing jpg 612 of 1772 at time 2024-06-20 10:43:34.979388\n",
      "Printing jpg 613 of 1772 at time 2024-06-20 10:43:39.963986\n",
      "Printing jpg 614 of 1772 at time 2024-06-20 10:43:44.912551\n",
      "Printing jpg 615 of 1772 at time 2024-06-20 10:43:49.895147\n",
      "Printing jpg 616 of 1772 at time 2024-06-20 10:43:54.862729\n",
      "Printing jpg 617 of 1772 at time 2024-06-20 10:43:59.945418\n",
      "Printing jpg 618 of 1772 at time 2024-06-20 10:44:05.031109\n",
      "Printing jpg 619 of 1772 at time 2024-06-20 10:44:09.812537\n",
      "Printing jpg 620 of 1772 at time 2024-06-20 10:44:14.545909\n",
      "Printing jpg 621 of 1772 at time 2024-06-20 10:44:19.179203\n",
      "Printing jpg 622 of 1772 at time 2024-06-20 10:44:23.780448\n",
      "Printing jpg 623 of 1772 at time 2024-06-20 10:44:28.378689\n",
      "Printing jpg 624 of 1772 at time 2024-06-20 10:44:36.145854\n",
      "Printing jpg 625 of 1772 at time 2024-06-20 10:44:43.780897\n",
      "Printing jpg 626 of 1772 at time 2024-06-20 10:44:48.496246\n",
      "Printing jpg 627 of 1772 at time 2024-06-20 10:44:56.114274\n",
      "Printing jpg 628 of 1772 at time 2024-06-20 10:45:00.880671\n",
      "Printing jpg 629 of 1772 at time 2024-06-20 10:45:05.529959\n",
      "Printing jpg 630 of 1772 at time 2024-06-20 10:45:10.046125\n",
      "Printing jpg 631 of 1772 at time 2024-06-20 10:45:14.830538\n",
      "Printing jpg 632 of 1772 at time 2024-06-20 10:45:19.579919\n",
      "Printing jpg 633 of 1772 at time 2024-06-20 10:45:24.329300\n",
      "Printing jpg 634 of 1772 at time 2024-06-20 10:45:29.315898\n",
      "Printing jpg 635 of 1772 at time 2024-06-20 10:45:34.430615\n",
      "Printing jpg 636 of 1772 at time 2024-06-20 10:45:39.396194\n",
      "Printing jpg 637 of 1772 at time 2024-06-20 10:45:44.363775\n",
      "Printing jpg 638 of 1772 at time 2024-06-20 10:45:48.948002\n",
      "Printing jpg 639 of 1772 at time 2024-06-20 10:45:53.546242\n",
      "Printing jpg 640 of 1772 at time 2024-06-20 10:45:58.263592\n",
      "Printing jpg 641 of 1772 at time 2024-06-20 10:46:02.996957\n",
      "Printing jpg 642 of 1772 at time 2024-06-20 10:46:07.580183\n",
      "Printing jpg 643 of 1772 at time 2024-06-20 10:46:12.330564\n",
      "Printing jpg 644 of 1772 at time 2024-06-20 10:46:17.080945\n",
      "Printing jpg 645 of 1772 at time 2024-06-20 10:46:21.696705\n",
      "Printing jpg 646 of 1772 at time 2024-06-20 10:46:31.264527\n",
      "Printing jpg 647 of 1772 at time 2024-06-20 10:46:36.047938\n",
      "Printing jpg 648 of 1772 at time 2024-06-20 10:46:40.715242\n",
      "Printing jpg 649 of 1772 at time 2024-06-20 10:46:45.563713\n",
      "Printing jpg 650 of 1772 at time 2024-06-20 10:46:50.282064\n",
      "Printing jpg 651 of 1772 at time 2024-06-20 10:46:54.966384\n",
      "Printing jpg 652 of 1772 at time 2024-06-20 10:46:59.665717\n",
      "Printing jpg 653 of 1772 at time 2024-06-20 10:47:04.381066\n",
      "Printing jpg 654 of 1772 at time 2024-06-20 10:47:09.013851\n",
      "Printing jpg 655 of 1772 at time 2024-06-20 10:47:13.631109\n",
      "Printing jpg 656 of 1772 at time 2024-06-20 10:47:18.280397\n",
      "Printing jpg 657 of 1772 at time 2024-06-20 10:47:22.897654\n",
      "Printing jpg 658 of 1772 at time 2024-06-20 10:47:27.547943\n",
      "Printing jpg 659 of 1772 at time 2024-06-20 10:47:32.164199\n",
      "Printing jpg 660 of 1772 at time 2024-06-20 10:47:36.947610\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "\n",
    "#Export jpgs. \n",
    "#This is a heavy operation that takes many hours for FSA.\n",
    "#It sometimes freezes mid-way and/or fails to create some jpg.\n",
    "#This process may later get improved.\n",
    "\n",
    "#If it keeps freezing, you can run this cell in isolation for the remaining pages by adding a slicer to the following (remember to remove it again):\n",
    "#for page_number in range(1, map_series.pageCount + 1)[600:]: (example where it previously froze at page 606)\n",
    "#If a restart of ArcGIS Pro is needed, then you have to\n",
    "\n",
    "try:\n",
    "#     def export_jpg(layout,output_filename):\n",
    "#         layout.exportToJPEG(output_filename, resolution=300)\n",
    "    if run_jpg:\n",
    "        aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "        project_path = aprx.filePath\n",
    "\n",
    "        jpg_folder = model_output_folder + r'\\jpg'\n",
    "        if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "\n",
    "        layouts = aprx.listLayouts()\n",
    "        export_fails = []\n",
    "\n",
    "        for layout in layouts:\n",
    "            if layout.mapSeries is not None:\n",
    "                map_series = layout.mapSeries\n",
    "                map_series.refresh()\n",
    "                \n",
    "                page_numbers = range(1, map_series.pageCount + 1)\n",
    "                reruns = 0\n",
    "                \n",
    "                # Loop through all pages in the map series\n",
    "#                 do while reruns < 100 and len(page_numbers) > 0:\n",
    "                for page_number in range(1, map_series.pageCount + 1)[1000:1500]:\n",
    "                    map_series.currentPageNumber = page_number\n",
    "                    output_filename = os.path.join(jpg_folder, f\"{map_series.pageRow.Drains_To}.jpg\")\n",
    "                    if map_series.pageRow.Drains_To in list(node_id_df['Node']):\n",
    "                        try:\n",
    "                            layout.exportToJPEG(output_filename, resolution=300)\n",
    "#                             export_jpg(layout,output_filename)\n",
    "#                             p = Process(target=export_jpg, args=(layout,output_filename,))\n",
    "#                             p.start()\n",
    "#                             p.join()\n",
    "\n",
    "\n",
    "                        except:\n",
    "                            print(f'WARNING! {map_series.pageRow.Drains_To} could not be made, try one more time')\n",
    "                            try:\n",
    "                                layout.exportToJPEG(output_filename, resolution=300)\n",
    "                                print(f'OK, {map_series.pageRow.Drains_To} made in second try')\n",
    "                            except:\n",
    "                                print(f'WARNING! {map_series.pageRow.Drains_To} could still not be made.')\n",
    "                                export_fails.append(map_series.pageRow.Drains_To)\n",
    "                        print (f'Printing jpg {page_number} of {map_series.pageCount} at time {datetime.datetime.now()}')\n",
    "                    else:\n",
    "                        print (f'Skipping (jpg {page_number} of {map_series.pageCount} at time {datetime.datetime.now()} (no output needed)')\n",
    "\n",
    "\n",
    "        print(f'the following pages failed: {export_fails}')            \n",
    "        print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "    \n",
    "    username = os.getlogin()\n",
    "    date_created = str(datetime.datetime.today()).split(' ')[0]\n",
    "    \n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    backup_folder = f'{excel_folder}\\\\Backup_{date_created}_{model}_V{model_version}_{pop_sheet}'\n",
    "    \n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    if not os.path.isdir(backup_folder): os.makedirs(backup_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "        if use_formula: #Replace spreadsheet values with formulas\n",
    "            value_startrow = 17\n",
    "            for i in node_single_df.index:\n",
    "                currentrow = i + value_startrow -1\n",
    "                for avg_calc_dict_key in avg_calc_dict:\n",
    "#                     currentrow = i + value_startrow\n",
    "                    inputs = avg_calc_dict[avg_calc_dict_key]\n",
    "                    header = inputs[0]\n",
    "                    subheader = inputs[2]\n",
    "                    unitflow_ref = inputs[3]\n",
    "                    col_ref = inputs[4]\n",
    "                    formula = f'{r\"=\"}{unitflow_ref}*{col_ref}{currentrow}/86400'\n",
    "                    node_single_df.loc[i,(header,subheader)] = formula                \n",
    "                \n",
    "                node_single_df.loc[i,('RESIDENTIAL','PEAK FLOW (L/s)')] = f'{r\"=\"}MAX(1.5,(1+(14/(4+((H{currentrow}/1000)^0.5)))))*I{currentrow}'\n",
    "                node_single_df.loc[i,('COMMERCIAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"com\"][3]}*K{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5))))*0.8)*L{currentrow}'\n",
    "                \n",
    "                ind_formula = f'{r\"=\"}MAX(1.5,0.8*(1+((14)/(4+(((N{currentrow}*{avg_calc_dict[\"ind\"][3]}/{avg_calc_dict[\"res\"][3]}'\n",
    "                ind_formula += f')/1000)^0.5))))*IF(N{currentrow}<121,1.7,(2.505-0.1673*LN(N{currentrow}))))*O{currentrow}'\n",
    "                node_single_df.loc[i,('INDUSTRIAL','PEAK FLOW (L/s)')] = ind_formula\n",
    "                \n",
    "                node_single_df.loc[i,('INSTITUTIONAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"inst\"][3]}*Q{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5)))))*R{currentrow}'\n",
    "\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','AREA (Ha)')] = f'{r\"=\"}G{currentrow}+K{currentrow}+N{currentrow}+Q{currentrow}'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFLOW (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infl\"][3]}/86400'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFILTRATION (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infi\"][3]}/86400'\n",
    "                \n",
    "                node_single_df.loc[i,('FLOWS','AVG. SAN. FLOW (L/s)')] = f'{r\"=\"}I{currentrow}+L{currentrow}+O{currentrow}+R{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','ADWF (L/s)')] = f'{r\"=\"}W{currentrow}+V{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','PWWF (L/s)')] = \\\n",
    "                    f'{r\"=\"}J{currentrow}+M{currentrow}+P{currentrow}+S{currentrow}+U{currentrow}+V{currentrow}'\n",
    "                \n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        id = id.replace('\\\\','_')\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 23 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13\n",
    "        \n",
    "        #Delete empty row between header and data\n",
    "        sheet1.delete_rows(16)\n",
    "        \n",
    "        #Find the minimum factor in the sheet. Array formulas do not work with openpyxl, do divisions one by one instead\n",
    "        cols = [['I','J'],['L','M'],['O','P'],['R','S']]\n",
    "        rows = list(range(16,currentrow + 1))      \n",
    "        formula = r'=ROUND(MIN('\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                formula += f'IFERROR({col[1]}{row}/{col[0]}{row},999),'\n",
    "        formula = formula[:-1] + '),2)'        \n",
    "        sheet1['J9'] = formula\n",
    "        sheet1['H9'] = 'Lowest peaking factor:'\n",
    "        sheet1['H10'] = 'Lowest peaking factors at lower limit 1.5 highlighted in yellow.'\n",
    "                      \n",
    "        #Color code cells where peaking facor is at minimum.\n",
    "        format_formula = 'J16/I16<1.501'\n",
    "        format_range = ''\n",
    "        for col in cols:\n",
    "            format_range += f'{col[1]}16:{col[1]}{currentrow} '\n",
    "        format_range = format_range[:-1] #Remove last space.               \n",
    "        fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "        rule = FormulaRule(formula=[format_formula], fill=fill)\n",
    "        sheet1.conditional_formatting.add(format_range, rule)\n",
    "        \n",
    "        decimals = [['#,##0','H'],['#,##0.0','IJLMOPRSUVWXY'],['#,##0.00','GKNQT']]\n",
    "        for decimal in decimals:\n",
    "            for col in decimal[1]:\n",
    "                for row in range(16,currentrow+1):\n",
    "                    sheet1[f'{col}{row}'].number_format = decimal[0]\n",
    "        \n",
    "        display_name = \"Open in Google Maps\"\n",
    "        google_map_string = 'https://maps.google.com/?q='\n",
    "        google_map_string += str(centroids_df.loc[muid,'Y']) + ', '\n",
    "        google_map_string += str(centroids_df.loc[muid,'X']) \n",
    "\n",
    "        # Adding a hyperlink with a display name\n",
    "        cell = sheet1.cell(row=10, column=3, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "                      \n",
    "        sheet = workbook[\"Map\"]\n",
    "        \n",
    "        #Add source info\n",
    "        source_infos = []\n",
    "        source_infos.append(['Created by:',username])\n",
    "        source_infos.append(['Date:',date_created])\n",
    "        source_infos.append(['Model area:',model])\n",
    "        source_infos.append(['Model version:',model_version])\n",
    "        source_infos.append(['Population file:',os.path.basename(pop_book)])\n",
    "        source_infos.append(['Population sheet:',pop_sheet ])\n",
    "        \n",
    "        for i, source_info in enumerate(source_infos):\n",
    "            sheet1[f'H{i+2}']  = source_info[0]\n",
    "            sheet1[f'J{i+2}']  = source_info[1]\n",
    "        \n",
    "        for row in sheet1['H2:H9']:\n",
    "            for cell in row:\n",
    "                cell.font = Font(bold=True)\n",
    "        \n",
    "\n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        sheet.add_image(img, 'B3')\n",
    "        \n",
    "        cell = sheet.cell(row=1, column=2, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "        shutil.copy(sheetpath,backup_folder + \"\\\\\" + id + \".xlsx\")\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
