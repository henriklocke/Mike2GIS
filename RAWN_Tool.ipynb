{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-06-03, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "# User Input, to move to separate sheet so no permanent cell\n",
    "#stop trace if more pipes than max_steps traced from catchment, must be an endless loop. \n",
    "\n",
    "model = 'NSSA'\n",
    "\n",
    "max_steps = 1000 \n",
    "\n",
    "check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "#                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "#                      A more robust check may be developed in the future.\n",
    "update_field_in_model = True\n",
    "update_field = 'Description'\n",
    "global_output_folder = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\Rawn_Tool\\Output\"\n",
    "\n",
    "if model == 'NSSA':   \n",
    "    model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "    model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "    pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "    pop_sheet = 'MPF Update 4'\n",
    "    scenario = 'Base'\n",
    "    \n",
    "if model == 'FSA':   \n",
    "    model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "    model_path = r''\n",
    "    pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "    pop_sheet = ''\n",
    "    scenario = '2030_Network'\n",
    "\n",
    "\n",
    "sewer_area = model\n",
    "gdb_name = 'RAWN.gdb'\n",
    "gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "#Options to skip time consuming steps during debug, must be True during production runs\n",
    "run_dissolve = True\n",
    "run_jpg = True\n",
    "run_import = True\n",
    "run_html = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "\n",
    "    years = [2060,2070,2080,2090,2100]\n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 5\n",
    "#Import population\n",
    "try:\n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "          raise ValueError(\"Error. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid = 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + altid\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "\n",
    "    sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "\n",
    "    sql = \"SELECT muid, acronym, assetname FROM msm_Node WHERE active = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(node_id_df.assetname.str[:2]=='MH') & (node_id_df.assetname.str.len() > 2) & (node_id_df.acronym.notna())]\n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]\n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Start catchment is '\" + catchment + \"'\")\n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                adwf += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]*0.8\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = (1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = (\n",
    "        0.8 * (1 + 14 / (4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5)) *\n",
    "        np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    )\n",
    "\n",
    "    node_df[('FLOWS','PWWF (L/s)')] = (\n",
    "        node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] +\n",
    "        node_df[('COMMERCIAL','PEAK FLOW (L/s)')] +\n",
    "        node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] +\n",
    "        node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] +\n",
    "        node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] +\n",
    "        node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    )\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msm_CatchCon\n",
      "msm_Catchment\n",
      "msm_Link\n",
      "msm_Node\n",
      "msm_Pump\n",
      "msm_Weir\n",
      "msm_Orifice\n",
      "msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "#Import GIS from the model\n",
    "\n",
    "try:\n",
    "    if run_import:\n",
    "\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "        if not os.path.isdir(out_path):\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "        arcpy.env.workspace = out_path\n",
    "        sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "        layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "        #     arcpy.conversion.FeatureClassToFeatureClass(model_path + '\\\\' + layer, out_path, layer)\n",
    "\n",
    "            arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "            if arcpy.Exists(layer):\n",
    "                arcpy.management.DeleteFeatures(layer)\n",
    "                arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "            else:    \n",
    "                arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer)\n",
    "                if layer == 'msm_Catchment':\n",
    "                    arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "\n",
    "            arcpy.management.Delete(\"temp_layer\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 10\n",
    "#Create merge_df,minimizing the number of computation heavy dissolves that need to be done.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    # rank_df.reset_index(inplace=True)\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    # rank_df.reset_index(inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "    #     rank_df.loc[index,'Catchments'] = catchments\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "    print(len(merge_set))\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merge_ID_98'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_df.loc['6786','Merge_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Merge_ID</th>\n",
       "      <th>Catchments</th>\n",
       "      <th>Catchment_Count</th>\n",
       "      <th>To_Dissolve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merge_ID_70</td>\n",
       "      <td>(2169,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2169]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merge_ID_98</td>\n",
       "      <td>(2122,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Merge_ID_38</td>\n",
       "      <td>(2052,)</td>\n",
       "      <td>1</td>\n",
       "      <td>[2052]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Merge_ID_67</td>\n",
       "      <td>(2119, 2122)</td>\n",
       "      <td>2</td>\n",
       "      <td>[2119, Merge_ID_98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merge_ID_35</td>\n",
       "      <td>(2128, 2136)</td>\n",
       "      <td>2</td>\n",
       "      <td>[2128, 2136]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Merge_ID_32</td>\n",
       "      <td>(2059, 2062, 2063, 2064, 2066, 2068, 2069, 207...</td>\n",
       "      <td>95</td>\n",
       "      <td>[Merge_ID_7, Merge_ID_53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Merge_ID_3</td>\n",
       "      <td>(2059, 2061, 2062, 2063, 2064, 2065, 2066, 206...</td>\n",
       "      <td>98</td>\n",
       "      <td>[2061, 2065, 2067, Merge_ID_32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Merge_ID_29</td>\n",
       "      <td>(2059, 2060, 2061, 2062, 2063, 2064, 2065, 206...</td>\n",
       "      <td>99</td>\n",
       "      <td>[2060, Merge_ID_3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Merge_ID_87</td>\n",
       "      <td>(10108, 10110, 2059, 2060, 2061, 2062, 2063, 2...</td>\n",
       "      <td>104</td>\n",
       "      <td>[10108, 10110, 2070, 2075, 2089, Merge_ID_29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Merge_ID_88</td>\n",
       "      <td>(10108, 10109, 10110, 2059, 2060, 2061, 2062, ...</td>\n",
       "      <td>105</td>\n",
       "      <td>[10109, Merge_ID_87]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Merge_ID  ...                                    To_Dissolve\n",
       "0    Merge_ID_70  ...                                         [2169]\n",
       "1    Merge_ID_98  ...                                         [2122]\n",
       "2    Merge_ID_38  ...                                         [2052]\n",
       "3    Merge_ID_67  ...                            [2119, Merge_ID_98]\n",
       "4    Merge_ID_35  ...                                   [2128, 2136]\n",
       "..           ...  ...                                            ...\n",
       "98   Merge_ID_32  ...                      [Merge_ID_7, Merge_ID_53]\n",
       "99    Merge_ID_3  ...                [2061, 2065, 2067, Merge_ID_32]\n",
       "100  Merge_ID_29  ...                             [2060, Merge_ID_3]\n",
       "101  Merge_ID_87  ...  [10108, 10110, 2070, 2075, 2089, Merge_ID_29]\n",
       "102  Merge_ID_88  ...                           [10109, Merge_ID_87]\n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_70, 0 of 102 at time 2024-06-03 15:20:24.531315\n",
      "Dissolving for Merge_ID_98, 1 of 102 at time 2024-06-03 15:20:47.293639\n",
      "Dissolving for Merge_ID_38, 2 of 102 at time 2024-06-03 15:21:15.431681\n",
      "Dissolving for Merge_ID_67, 3 of 102 at time 2024-06-03 15:21:30.354332\n",
      "Dissolving for Merge_ID_35, 4 of 102 at time 2024-06-03 15:22:02.655872\n",
      "Dissolving for Merge_ID_83, 5 of 102 at time 2024-06-03 15:22:28.903382\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "#Run dissolve\n",
    "\n",
    "gdb_name_dissolve = 'teeeest.gdb'\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_path)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "        #     arcpy.env.workspace = dissolve_path\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df[:6].iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 1 of 458 at time 2024-06-03 13:41:53.490588\n",
      "Printing jpg 2 of 458 at time 2024-06-03 13:41:57.274554\n",
      "Printing jpg 3 of 458 at time 2024-06-03 13:42:00.614611\n",
      "Printing jpg 4 of 458 at time 2024-06-03 13:42:04.145346\n",
      "Printing jpg 5 of 458 at time 2024-06-03 13:42:07.265202\n",
      "Printing jpg 6 of 458 at time 2024-06-03 13:42:10.769913\n",
      "Printing jpg 7 of 458 at time 2024-06-03 13:42:14.453284\n",
      "Printing jpg 8 of 458 at time 2024-06-03 13:42:17.646712\n",
      "Printing jpg 9 of 458 at time 2024-06-03 13:42:20.645457\n",
      "Printing jpg 10 of 458 at time 2024-06-03 13:42:23.862904\n",
      "Printing jpg 11 of 458 at time 2024-06-03 13:42:26.707508\n",
      "Printing jpg 12 of 458 at time 2024-06-03 13:42:29.270357\n",
      "Printing jpg 13 of 458 at time 2024-06-03 13:42:32.897677\n",
      "Printing jpg 14 of 458 at time 2024-06-03 13:42:36.051069\n",
      "Printing jpg 15 of 458 at time 2024-06-03 13:42:39.481209\n",
      "Printing jpg 16 of 458 at time 2024-06-03 13:42:42.583551\n",
      "Printing jpg 17 of 458 at time 2024-06-03 13:42:45.645354\n",
      "Printing jpg 18 of 458 at time 2024-06-03 13:42:48.850791\n",
      "Printing jpg 19 of 458 at time 2024-06-03 13:42:51.865550\n",
      "Printing jpg 20 of 458 at time 2024-06-03 13:42:54.802742\n",
      "Printing jpg 21 of 458 at time 2024-06-03 13:42:58.271918\n",
      "Printing jpg 22 of 458 at time 2024-06-03 13:43:00.678120\n",
      "Printing jpg 23 of 458 at time 2024-06-03 13:43:03.749435\n",
      "Printing jpg 24 of 458 at time 2024-06-03 13:43:07.589453\n",
      "Printing jpg 25 of 458 at time 2024-06-03 13:43:11.243798\n",
      "Printing jpg 26 of 458 at time 2024-06-03 13:43:15.397103\n",
      "Printing jpg 27 of 458 at time 2024-06-03 13:43:18.928336\n",
      "Printing jpg 28 of 458 at time 2024-06-03 13:43:22.856434\n",
      "Printing jpg 29 of 458 at time 2024-06-03 13:43:26.208005\n",
      "Printing jpg 30 of 458 at time 2024-06-03 13:43:29.676693\n",
      "Printing jpg 31 of 458 at time 2024-06-03 13:43:33.157879\n",
      "Printing jpg 32 of 458 at time 2024-06-03 13:43:36.500442\n",
      "Printing jpg 33 of 458 at time 2024-06-03 13:43:39.959609\n",
      "Printing jpg 34 of 458 at time 2024-06-03 13:43:42.396342\n",
      "Printing jpg 35 of 458 at time 2024-06-03 13:43:45.630302\n",
      "Printing jpg 36 of 458 at time 2024-06-03 13:43:49.305169\n",
      "Printing jpg 37 of 458 at time 2024-06-03 13:43:52.859423\n",
      "Printing jpg 38 of 458 at time 2024-06-03 13:43:55.897708\n",
      "Printing jpg 39 of 458 at time 2024-06-03 13:43:59.176710\n",
      "Printing jpg 40 of 458 at time 2024-06-03 13:44:01.708530\n",
      "Printing jpg 41 of 458 at time 2024-06-03 13:44:05.310827\n",
      "Printing jpg 42 of 458 at time 2024-06-03 13:44:09.115315\n",
      "Printing jpg 43 of 458 at time 2024-06-03 13:44:11.870837\n",
      "Printing jpg 44 of 458 at time 2024-06-03 13:44:14.928138\n",
      "Printing jpg 45 of 458 at time 2024-06-03 13:44:18.957827\n",
      "Printing jpg 46 of 458 at time 2024-06-03 13:44:22.209305\n",
      "Printing jpg 47 of 458 at time 2024-06-03 13:44:25.427251\n",
      "Printing jpg 48 of 458 at time 2024-06-03 13:44:28.613670\n",
      "Printing jpg 49 of 458 at time 2024-06-03 13:44:31.397217\n",
      "Printing jpg 50 of 458 at time 2024-06-03 13:44:34.149239\n",
      "Printing jpg 51 of 458 at time 2024-06-03 13:44:37.083925\n",
      "Printing jpg 52 of 458 at time 2024-06-03 13:44:40.313384\n",
      "Printing jpg 53 of 458 at time 2024-06-03 13:44:43.246068\n",
      "Printing jpg 54 of 458 at time 2024-06-03 13:44:46.095179\n",
      "Printing jpg 55 of 458 at time 2024-06-03 13:44:49.834104\n",
      "Printing jpg 56 of 458 at time 2024-06-03 13:44:52.769293\n",
      "Printing jpg 57 of 458 at time 2024-06-03 13:44:55.459756\n",
      "Printing jpg 58 of 458 at time 2024-06-03 13:44:58.335891\n",
      "Printing jpg 59 of 458 at time 2024-06-03 13:45:00.915252\n",
      "Printing jpg 60 of 458 at time 2024-06-03 13:45:03.397026\n",
      "Printing jpg 61 of 458 at time 2024-06-03 13:45:06.773116\n",
      "Printing jpg 62 of 458 at time 2024-06-03 13:45:09.615718\n",
      "Printing jpg 63 of 458 at time 2024-06-03 13:45:12.489851\n",
      "Printing jpg 64 of 458 at time 2024-06-03 13:45:15.864940\n",
      "Printing jpg 65 of 458 at time 2024-06-03 13:45:19.027338\n",
      "Printing jpg 66 of 458 at time 2024-06-03 13:45:22.646153\n",
      "Printing jpg 67 of 458 at time 2024-06-03 13:45:32.681841\n",
      "Printing jpg 68 of 458 at time 2024-06-03 13:45:36.709030\n",
      "Printing jpg 69 of 458 at time 2024-06-03 13:45:39.427518\n",
      "Printing jpg 70 of 458 at time 2024-06-03 13:45:42.459796\n",
      "Printing jpg 71 of 458 at time 2024-06-03 13:45:45.414500\n",
      "Printing jpg 72 of 458 at time 2024-06-03 13:45:48.116476\n",
      "Printing jpg 73 of 458 at time 2024-06-03 13:45:51.284376\n",
      "Printing jpg 74 of 458 at time 2024-06-03 13:45:53.803682\n",
      "Printing jpg 75 of 458 at time 2024-06-03 13:45:56.645786\n",
      "Printing jpg 76 of 458 at time 2024-06-03 13:45:59.591482\n",
      "Printing jpg 77 of 458 at time 2024-06-03 13:46:03.053154\n",
      "Printing jpg 78 of 458 at time 2024-06-03 13:46:05.990842\n",
      "Printing jpg 79 of 458 at time 2024-06-03 13:46:09.122211\n",
      "Printing jpg 80 of 458 at time 2024-06-03 13:46:12.274105\n",
      "Printing jpg 81 of 458 at time 2024-06-03 13:46:15.489551\n",
      "Printing jpg 82 of 458 at time 2024-06-03 13:46:19.270515\n",
      "Printing jpg 83 of 458 at time 2024-06-03 13:46:22.118121\n",
      "Printing jpg 84 of 458 at time 2024-06-03 13:46:25.371627\n",
      "Printing jpg 85 of 458 at time 2024-06-03 13:46:28.201746\n",
      "Printing jpg 86 of 458 at time 2024-06-03 13:46:31.122467\n",
      "Printing jpg 87 of 458 at time 2024-06-03 13:46:34.084726\n",
      "Printing jpg 88 of 458 at time 2024-06-03 13:46:37.085508\n",
      "Printing jpg 89 of 458 at time 2024-06-03 13:46:39.866599\n",
      "Printing jpg 90 of 458 at time 2024-06-03 13:46:42.552572\n",
      "Printing jpg 91 of 458 at time 2024-06-03 13:46:46.335591\n",
      "Printing jpg 92 of 458 at time 2024-06-03 13:46:49.500512\n",
      "Printing jpg 93 of 458 at time 2024-06-03 13:46:52.985752\n",
      "Printing jpg 94 of 458 at time 2024-06-03 13:46:56.203704\n",
      "Printing jpg 95 of 458 at time 2024-06-03 13:46:59.033294\n",
      "Printing jpg 96 of 458 at time 2024-06-03 13:47:01.992505\n",
      "Printing jpg 97 of 458 at time 2024-06-03 13:47:05.329560\n",
      "Printing jpg 98 of 458 at time 2024-06-03 13:47:08.590047\n",
      "Printing jpg 99 of 458 at time 2024-06-03 13:47:11.242475\n",
      "Printing jpg 100 of 458 at time 2024-06-03 13:47:14.555010\n",
      "Printing jpg 101 of 458 at time 2024-06-03 13:47:17.521725\n",
      "Printing jpg 102 of 458 at time 2024-06-03 13:47:20.773204\n",
      "Printing jpg 103 of 458 at time 2024-06-03 13:47:23.836007\n",
      "Printing jpg 104 of 458 at time 2024-06-03 13:47:26.804228\n",
      "Printing jpg 105 of 458 at time 2024-06-03 13:47:30.147287\n",
      "Printing jpg 106 of 458 at time 2024-06-03 13:47:33.647995\n",
      "Printing jpg 107 of 458 at time 2024-06-03 13:47:36.925995\n",
      "Printing jpg 108 of 458 at time 2024-06-03 13:47:39.792121\n",
      "Printing jpg 109 of 458 at time 2024-06-03 13:47:42.709792\n",
      "Printing jpg 110 of 458 at time 2024-06-03 13:47:45.766093\n",
      "Printing jpg 111 of 458 at time 2024-06-03 13:47:49.440456\n",
      "Printing jpg 112 of 458 at time 2024-06-03 13:47:52.524782\n",
      "Printing jpg 113 of 458 at time 2024-06-03 13:47:55.834812\n",
      "Printing jpg 114 of 458 at time 2024-06-03 13:47:58.616861\n",
      "Printing jpg 115 of 458 at time 2024-06-03 13:48:01.584577\n",
      "Printing jpg 116 of 458 at time 2024-06-03 13:48:05.085289\n",
      "Printing jpg 117 of 458 at time 2024-06-03 13:48:08.113060\n",
      "Printing jpg 118 of 458 at time 2024-06-03 13:48:11.493160\n",
      "Printing jpg 119 of 458 at time 2024-06-03 13:48:14.773162\n",
      "Printing jpg 120 of 458 at time 2024-06-03 13:48:18.330959\n",
      "Printing jpg 121 of 458 at time 2024-06-03 13:48:21.398270\n",
      "Printing jpg 122 of 458 at time 2024-06-03 13:48:24.680274\n",
      "Printing jpg 123 of 458 at time 2024-06-03 13:48:27.430294\n",
      "Printing jpg 124 of 458 at time 2024-06-03 13:48:30.772353\n",
      "Printing jpg 125 of 458 at time 2024-06-03 13:48:34.178974\n",
      "Printing jpg 126 of 458 at time 2024-06-03 13:48:37.648149\n",
      "Printing jpg 127 of 458 at time 2024-06-03 13:48:40.429700\n",
      "Printing jpg 128 of 458 at time 2024-06-03 13:48:43.180721\n",
      "Printing jpg 129 of 458 at time 2024-06-03 13:48:46.310088\n",
      "Printing jpg 130 of 458 at time 2024-06-03 13:48:49.117658\n",
      "Printing jpg 131 of 458 at time 2024-06-03 13:48:52.054849\n",
      "Printing jpg 132 of 458 at time 2024-06-03 13:48:55.585081\n",
      "Printing jpg 133 of 458 at time 2024-06-03 13:48:58.856578\n",
      "Printing jpg 134 of 458 at time 2024-06-03 13:49:01.866332\n",
      "Printing jpg 135 of 458 at time 2024-06-03 13:49:04.773496\n",
      "Printing jpg 136 of 458 at time 2024-06-03 13:49:07.988438\n",
      "Printing jpg 137 of 458 at time 2024-06-03 13:49:10.556788\n",
      "Printing jpg 138 of 458 at time 2024-06-03 13:49:13.305310\n",
      "Printing jpg 139 of 458 at time 2024-06-03 13:49:16.274027\n",
      "Printing jpg 140 of 458 at time 2024-06-03 13:49:19.150162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 141 of 458 at time 2024-06-03 13:49:21.931707\n",
      "Printing jpg 142 of 458 at time 2024-06-03 13:49:25.143149\n",
      "Printing jpg 143 of 458 at time 2024-06-03 13:49:27.940709\n",
      "Printing jpg 144 of 458 at time 2024-06-03 13:49:30.554608\n",
      "Printing jpg 145 of 458 at time 2024-06-03 13:49:33.712012\n",
      "Printing jpg 146 of 458 at time 2024-06-03 13:49:36.679235\n",
      "Printing jpg 147 of 458 at time 2024-06-03 13:49:39.774067\n",
      "Printing jpg 148 of 458 at time 2024-06-03 13:49:42.864398\n",
      "Printing jpg 149 of 458 at time 2024-06-03 13:49:45.366688\n",
      "Printing jpg 150 of 458 at time 2024-06-03 13:49:48.626175\n",
      "Printing jpg 151 of 458 at time 2024-06-03 13:49:51.022872\n",
      "Printing jpg 152 of 458 at time 2024-06-03 13:49:53.836473\n",
      "Printing jpg 153 of 458 at time 2024-06-03 13:49:56.160600\n",
      "Printing jpg 154 of 458 at time 2024-06-03 13:49:59.337507\n",
      "Printing jpg 155 of 458 at time 2024-06-03 13:50:02.179611\n",
      "Printing jpg 156 of 458 at time 2024-06-03 13:50:05.354517\n",
      "Printing jpg 157 of 458 at time 2024-06-03 13:50:08.530927\n",
      "Printing jpg 158 of 458 at time 2024-06-03 13:50:11.581719\n",
      "Printing jpg 159 of 458 at time 2024-06-03 13:50:14.381287\n",
      "Printing jpg 160 of 458 at time 2024-06-03 13:50:17.883995\n",
      "Printing jpg 161 of 458 at time 2024-06-03 13:50:21.617412\n",
      "Printing jpg 162 of 458 at time 2024-06-03 13:50:24.555604\n",
      "Printing jpg 163 of 458 at time 2024-06-03 13:50:27.852621\n",
      "Printing jpg 164 of 458 at time 2024-06-03 13:50:30.742769\n",
      "Printing jpg 165 of 458 at time 2024-06-03 13:50:33.712486\n",
      "Printing jpg 166 of 458 at time 2024-06-03 13:50:36.399946\n",
      "Printing jpg 167 of 458 at time 2024-06-03 13:50:39.307109\n",
      "Printing jpg 168 of 458 at time 2024-06-03 13:50:42.080647\n",
      "Printing jpg 169 of 458 at time 2024-06-03 13:50:44.631485\n",
      "Printing jpg 170 of 458 at time 2024-06-03 13:50:47.279908\n",
      "Printing jpg 171 of 458 at time 2024-06-03 13:50:50.084979\n",
      "Printing jpg 172 of 458 at time 2024-06-03 13:50:52.803467\n",
      "Printing jpg 173 of 458 at time 2024-06-03 13:50:56.398260\n",
      "Printing jpg 174 of 458 at time 2024-06-03 13:50:59.618207\n",
      "Printing jpg 175 of 458 at time 2024-06-03 13:51:02.525370\n",
      "Printing jpg 176 of 458 at time 2024-06-03 13:51:05.525115\n",
      "Printing jpg 177 of 458 at time 2024-06-03 13:51:08.368220\n",
      "Printing jpg 178 of 458 at time 2024-06-03 13:51:11.274880\n",
      "Printing jpg 179 of 458 at time 2024-06-03 13:51:14.306654\n",
      "Printing jpg 180 of 458 at time 2024-06-03 13:51:17.348941\n",
      "Printing jpg 181 of 458 at time 2024-06-03 13:51:20.530853\n",
      "Printing jpg 182 of 458 at time 2024-06-03 13:51:23.994526\n",
      "Printing jpg 183 of 458 at time 2024-06-03 13:51:26.743041\n",
      "Printing jpg 184 of 458 at time 2024-06-03 13:51:30.242747\n",
      "Printing jpg 185 of 458 at time 2024-06-03 13:51:33.457689\n",
      "Printing jpg 186 of 458 at time 2024-06-03 13:51:36.497975\n",
      "Printing jpg 187 of 458 at time 2024-06-03 13:51:39.087344\n",
      "Printing jpg 188 of 458 at time 2024-06-03 13:51:42.488960\n",
      "Printing jpg 189 of 458 at time 2024-06-03 13:51:45.724426\n",
      "Printing jpg 190 of 458 at time 2024-06-03 13:51:48.838780\n",
      "Printing jpg 191 of 458 at time 2024-06-03 13:51:51.931610\n",
      "Printing jpg 192 of 458 at time 2024-06-03 13:51:55.337230\n",
      "Printing jpg 193 of 458 at time 2024-06-03 13:51:58.932022\n",
      "Printing jpg 194 of 458 at time 2024-06-03 13:52:02.494282\n",
      "Printing jpg 195 of 458 at time 2024-06-03 13:52:05.556587\n",
      "Printing jpg 196 of 458 at time 2024-06-03 13:52:08.931676\n",
      "Printing jpg 197 of 458 at time 2024-06-03 13:52:12.211181\n",
      "Printing jpg 198 of 458 at time 2024-06-03 13:52:14.978220\n",
      "Printing jpg 199 of 458 at time 2024-06-03 13:52:17.901397\n",
      "Printing jpg 200 of 458 at time 2024-06-03 13:52:20.776028\n",
      "Printing jpg 201 of 458 at time 2024-06-03 13:52:23.869362\n",
      "Printing jpg 202 of 458 at time 2024-06-03 13:52:27.057280\n",
      "Printing jpg 203 of 458 at time 2024-06-03 13:52:30.274728\n",
      "Printing jpg 204 of 458 at time 2024-06-03 13:52:33.087302\n",
      "Printing jpg 205 of 458 at time 2024-06-03 13:52:36.024990\n",
      "Printing jpg 206 of 458 at time 2024-06-03 13:52:39.618783\n",
      "Printing jpg 207 of 458 at time 2024-06-03 13:52:42.933320\n",
      "Printing jpg 208 of 458 at time 2024-06-03 13:52:45.869006\n",
      "Printing jpg 209 of 458 at time 2024-06-03 13:52:48.743140\n",
      "Printing jpg 210 of 458 at time 2024-06-03 13:52:52.274371\n",
      "Printing jpg 211 of 458 at time 2024-06-03 13:52:55.243591\n",
      "Printing jpg 212 of 458 at time 2024-06-03 13:52:58.088699\n",
      "Printing jpg 213 of 458 at time 2024-06-03 13:53:01.086946\n",
      "Printing jpg 214 of 458 at time 2024-06-03 13:53:04.461034\n",
      "Printing jpg 215 of 458 at time 2024-06-03 13:53:07.307141\n",
      "Printing jpg 216 of 458 at time 2024-06-03 13:53:10.838373\n",
      "Printing jpg 217 of 458 at time 2024-06-03 13:53:13.555362\n",
      "Printing jpg 218 of 458 at time 2024-06-03 13:53:16.650194\n",
      "Printing jpg 219 of 458 at time 2024-06-03 13:53:19.461269\n",
      "Printing jpg 220 of 458 at time 2024-06-03 13:53:22.149730\n",
      "Printing jpg 221 of 458 at time 2024-06-03 13:53:25.049383\n",
      "Printing jpg 222 of 458 at time 2024-06-03 13:53:28.235802\n",
      "Printing jpg 223 of 458 at time 2024-06-03 13:53:31.076402\n",
      "Printing jpg 224 of 458 at time 2024-06-03 13:53:33.652262\n",
      "Printing jpg 225 of 458 at time 2024-06-03 13:53:36.870207\n",
      "Printing jpg 226 of 458 at time 2024-06-03 13:53:39.869455\n",
      "Printing jpg 227 of 458 at time 2024-06-03 13:53:43.274571\n",
      "Printing jpg 228 of 458 at time 2024-06-03 13:53:46.668182\n",
      "Printing jpg 229 of 458 at time 2024-06-03 13:53:49.534805\n",
      "Printing jpg 230 of 458 at time 2024-06-03 13:53:52.368902\n",
      "Printing jpg 231 of 458 at time 2024-06-03 13:53:55.276563\n",
      "Printing jpg 232 of 458 at time 2024-06-03 13:53:58.495011\n",
      "Printing jpg 233 of 458 at time 2024-06-03 13:54:01.804039\n",
      "Printing jpg 234 of 458 at time 2024-06-03 13:54:04.802286\n",
      "Printing jpg 235 of 458 at time 2024-06-03 13:54:08.055262\n",
      "Printing jpg 236 of 458 at time 2024-06-03 13:54:11.149597\n",
      "Printing jpg 237 of 458 at time 2024-06-03 13:54:14.381555\n",
      "Printing jpg 238 of 458 at time 2024-06-03 13:54:17.371296\n",
      "Printing jpg 239 of 458 at time 2024-06-03 13:54:20.589241\n",
      "Printing jpg 240 of 458 at time 2024-06-03 13:54:23.402318\n",
      "Printing jpg 241 of 458 at time 2024-06-03 13:54:26.149832\n",
      "Printing jpg 242 of 458 at time 2024-06-03 13:54:29.314231\n",
      "Printing jpg 243 of 458 at time 2024-06-03 13:54:32.368026\n",
      "Printing jpg 244 of 458 at time 2024-06-03 13:54:35.331240\n",
      "Printing jpg 245 of 458 at time 2024-06-03 13:54:38.532169\n",
      "Printing jpg 246 of 458 at time 2024-06-03 13:54:41.151566\n",
      "Printing jpg 247 of 458 at time 2024-06-03 13:54:43.839530\n",
      "Printing jpg 248 of 458 at time 2024-06-03 13:54:46.714160\n",
      "Printing jpg 249 of 458 at time 2024-06-03 13:54:49.652352\n",
      "Printing jpg 250 of 458 at time 2024-06-03 13:54:52.758194\n",
      "Printing jpg 251 of 458 at time 2024-06-03 13:54:55.922592\n",
      "Printing jpg 252 of 458 at time 2024-06-03 13:54:58.900317\n",
      "Printing jpg 253 of 458 at time 2024-06-03 13:55:02.183825\n",
      "Printing jpg 254 of 458 at time 2024-06-03 13:55:05.958279\n",
      "Printing jpg 255 of 458 at time 2024-06-03 13:55:09.504026\n",
      "Printing jpg 256 of 458 at time 2024-06-03 13:55:13.027754\n",
      "Printing jpg 257 of 458 at time 2024-06-03 13:55:16.433870\n",
      "Printing jpg 258 of 458 at time 2024-06-03 13:55:19.902547\n",
      "Printing jpg 259 of 458 at time 2024-06-03 13:55:23.343696\n",
      "Printing jpg 260 of 458 at time 2024-06-03 13:55:26.424018\n",
      "Printing jpg 261 of 458 at time 2024-06-03 13:55:29.307656\n",
      "Printing jpg 262 of 458 at time 2024-06-03 13:55:32.341936\n",
      "Printing jpg 263 of 458 at time 2024-06-03 13:55:35.618934\n",
      "Printing jpg 264 of 458 at time 2024-06-03 13:55:38.621185\n",
      "Printing jpg 265 of 458 at time 2024-06-03 13:55:42.245501\n",
      "Printing jpg 266 of 458 at time 2024-06-03 13:55:45.620092\n",
      "Printing jpg 267 of 458 at time 2024-06-03 13:55:49.153325\n",
      "Printing jpg 268 of 458 at time 2024-06-03 13:55:52.744114\n",
      "Printing jpg 269 of 458 at time 2024-06-03 13:55:55.839947\n",
      "Printing jpg 270 of 458 at time 2024-06-03 13:55:59.119451\n",
      "Printing jpg 271 of 458 at time 2024-06-03 13:56:01.965055\n",
      "Printing jpg 272 of 458 at time 2024-06-03 13:56:04.873220\n",
      "Printing jpg 273 of 458 at time 2024-06-03 13:56:08.151219\n",
      "Printing jpg 274 of 458 at time 2024-06-03 13:56:10.994323\n",
      "Printing jpg 275 of 458 at time 2024-06-03 13:56:13.762857\n",
      "Printing jpg 276 of 458 at time 2024-06-03 13:56:16.673528\n",
      "Printing jpg 277 of 458 at time 2024-06-03 13:56:19.624228\n",
      "Printing jpg 278 of 458 at time 2024-06-03 13:56:22.682530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 279 of 458 at time 2024-06-03 13:56:25.590190\n",
      "Printing jpg 280 of 458 at time 2024-06-03 13:56:28.369734\n",
      "Printing jpg 281 of 458 at time 2024-06-03 13:56:33.059528\n",
      "Printing jpg 282 of 458 at time 2024-06-03 13:56:40.069445\n",
      "Printing jpg 283 of 458 at time 2024-06-03 13:56:42.434112\n",
      "Printing jpg 284 of 458 at time 2024-06-03 13:56:45.338770\n",
      "Printing jpg 285 of 458 at time 2024-06-03 13:56:48.778420\n",
      "Printing jpg 286 of 458 at time 2024-06-03 13:56:52.138495\n",
      "Printing jpg 287 of 458 at time 2024-06-03 13:56:54.646293\n",
      "Printing jpg 288 of 458 at time 2024-06-03 13:56:57.431842\n",
      "Printing jpg 289 of 458 at time 2024-06-03 13:57:00.340006\n",
      "Printing jpg 290 of 458 at time 2024-06-03 13:57:03.182607\n",
      "Printing jpg 291 of 458 at time 2024-06-03 13:57:06.153326\n",
      "Printing jpg 292 of 458 at time 2024-06-03 13:57:08.933372\n",
      "Printing jpg 293 of 458 at time 2024-06-03 13:57:11.839031\n",
      "Printing jpg 294 of 458 at time 2024-06-03 13:57:14.996423\n",
      "Printing jpg 295 of 458 at time 2024-06-03 13:57:17.335564\n",
      "Printing jpg 296 of 458 at time 2024-06-03 13:57:20.121617\n",
      "Printing jpg 297 of 458 at time 2024-06-03 13:57:22.496791\n",
      "Printing jpg 298 of 458 at time 2024-06-03 13:57:25.521061\n",
      "Printing jpg 299 of 458 at time 2024-06-03 13:57:27.966298\n",
      "Printing jpg 300 of 458 at time 2024-06-03 13:57:31.307355\n",
      "Printing jpg 301 of 458 at time 2024-06-03 13:57:34.246548\n",
      "Printing jpg 302 of 458 at time 2024-06-03 13:57:36.682777\n",
      "Printing jpg 303 of 458 at time 2024-06-03 13:57:39.495854\n",
      "Printing jpg 304 of 458 at time 2024-06-03 13:57:42.371988\n",
      "Printing jpg 305 of 458 at time 2024-06-03 13:57:45.214093\n",
      "Printing jpg 306 of 458 at time 2024-06-03 13:57:48.260881\n",
      "Printing jpg 307 of 458 at time 2024-06-03 13:57:50.846750\n",
      "Printing jpg 308 of 458 at time 2024-06-03 13:57:53.715375\n",
      "Printing jpg 309 of 458 at time 2024-06-03 13:58:00.090712\n",
      "Printing jpg 310 of 458 at time 2024-06-03 13:58:02.964347\n",
      "Printing jpg 311 of 458 at time 2024-06-03 13:58:05.741889\n",
      "Printing jpg 312 of 458 at time 2024-06-03 13:58:09.651969\n",
      "Printing jpg 313 of 458 at time 2024-06-03 13:58:12.557628\n",
      "Printing jpg 314 of 458 at time 2024-06-03 13:58:15.434764\n",
      "Printing jpg 315 of 458 at time 2024-06-03 13:58:18.749805\n",
      "Printing jpg 316 of 458 at time 2024-06-03 13:58:21.309650\n",
      "Printing jpg 317 of 458 at time 2024-06-03 13:58:23.997110\n",
      "Printing jpg 318 of 458 at time 2024-06-03 13:58:27.382711\n",
      "Printing jpg 319 of 458 at time 2024-06-03 13:58:29.939050\n",
      "Printing jpg 320 of 458 at time 2024-06-03 13:58:33.121466\n",
      "Printing jpg 321 of 458 at time 2024-06-03 13:58:36.066160\n",
      "Printing jpg 322 of 458 at time 2024-06-03 13:58:38.903259\n",
      "Printing jpg 323 of 458 at time 2024-06-03 13:58:41.839946\n",
      "Printing jpg 324 of 458 at time 2024-06-03 13:58:44.684549\n",
      "Printing jpg 325 of 458 at time 2024-06-03 13:58:47.465597\n",
      "Printing jpg 326 of 458 at time 2024-06-03 13:58:50.402284\n",
      "Printing jpg 327 of 458 at time 2024-06-03 13:58:53.216362\n",
      "Printing jpg 328 of 458 at time 2024-06-03 13:58:56.361240\n",
      "Printing jpg 329 of 458 at time 2024-06-03 13:58:59.216356\n",
      "Printing jpg 330 of 458 at time 2024-06-03 13:59:02.060958\n",
      "Printing jpg 331 of 458 at time 2024-06-03 13:59:04.840006\n",
      "Printing jpg 332 of 458 at time 2024-06-03 13:59:08.059953\n",
      "Printing jpg 333 of 458 at time 2024-06-03 13:59:11.153286\n",
      "Printing jpg 334 of 458 at time 2024-06-03 13:59:14.596436\n",
      "Printing jpg 335 of 458 at time 2024-06-03 13:59:16.996640\n",
      "Printing jpg 336 of 458 at time 2024-06-03 13:59:20.152527\n",
      "Printing jpg 337 of 458 at time 2024-06-03 13:59:23.226844\n",
      "Printing jpg 338 of 458 at time 2024-06-03 13:59:26.590922\n",
      "Printing jpg 339 of 458 at time 2024-06-03 13:59:29.309535\n",
      "Printing jpg 340 of 458 at time 2024-06-03 13:59:31.871388\n",
      "Printing jpg 341 of 458 at time 2024-06-03 13:59:35.028292\n",
      "Printing jpg 342 of 458 at time 2024-06-03 13:59:38.028036\n",
      "Printing jpg 343 of 458 at time 2024-06-03 13:59:41.367597\n",
      "Printing jpg 344 of 458 at time 2024-06-03 13:59:44.246230\n",
      "Printing jpg 345 of 458 at time 2024-06-03 13:59:47.430647\n",
      "Printing jpg 346 of 458 at time 2024-06-03 13:59:50.870795\n",
      "Printing jpg 347 of 458 at time 2024-06-03 13:59:53.720905\n",
      "Printing jpg 348 of 458 at time 2024-06-03 13:59:57.154046\n",
      "Printing jpg 349 of 458 at time 2024-06-03 14:00:00.156297\n",
      "Printing jpg 350 of 458 at time 2024-06-03 14:00:03.534387\n",
      "Printing jpg 351 of 458 at time 2024-06-03 14:00:06.405517\n",
      "Printing jpg 352 of 458 at time 2024-06-03 14:00:09.405262\n",
      "Printing jpg 353 of 458 at time 2024-06-03 14:00:12.723801\n",
      "Printing jpg 354 of 458 at time 2024-06-03 14:00:15.588422\n",
      "Printing jpg 355 of 458 at time 2024-06-03 14:00:18.376979\n",
      "Printing jpg 356 of 458 at time 2024-06-03 14:00:21.117486\n",
      "Printing jpg 357 of 458 at time 2024-06-03 14:00:24.339938\n",
      "Printing jpg 358 of 458 at time 2024-06-03 14:00:27.153512\n",
      "Printing jpg 359 of 458 at time 2024-06-03 14:00:30.653217\n",
      "Printing jpg 360 of 458 at time 2024-06-03 14:00:33.497820\n",
      "Printing jpg 361 of 458 at time 2024-06-03 14:00:36.436013\n",
      "Printing jpg 362 of 458 at time 2024-06-03 14:00:39.783074\n",
      "Printing jpg 363 of 458 at time 2024-06-03 14:00:42.842376\n",
      "Printing jpg 364 of 458 at time 2024-06-03 14:00:46.406140\n",
      "Printing jpg 365 of 458 at time 2024-06-03 14:00:49.778728\n",
      "Printing jpg 366 of 458 at time 2024-06-03 14:00:53.018692\n",
      "Printing jpg 367 of 458 at time 2024-06-03 14:00:56.341235\n",
      "Printing jpg 368 of 458 at time 2024-06-03 14:00:59.473100\n",
      "Printing jpg 369 of 458 at time 2024-06-03 14:01:01.779715\n",
      "Printing jpg 370 of 458 at time 2024-06-03 14:01:04.510213\n",
      "Printing jpg 371 of 458 at time 2024-06-03 14:01:07.465421\n",
      "Printing jpg 372 of 458 at time 2024-06-03 14:01:10.466166\n",
      "Printing jpg 373 of 458 at time 2024-06-03 14:01:13.652081\n",
      "Printing jpg 374 of 458 at time 2024-06-03 14:01:16.216931\n",
      "Printing jpg 375 of 458 at time 2024-06-03 14:01:19.060533\n",
      "Printing jpg 376 of 458 at time 2024-06-03 14:01:22.886536\n",
      "Printing jpg 377 of 458 at time 2024-06-03 14:01:25.402839\n",
      "Printing jpg 378 of 458 at time 2024-06-03 14:01:28.841488\n",
      "Printing jpg 379 of 458 at time 2024-06-03 14:01:32.372719\n",
      "Printing jpg 380 of 458 at time 2024-06-03 14:01:35.842396\n",
      "Printing jpg 381 of 458 at time 2024-06-03 14:01:38.934728\n",
      "Printing jpg 382 of 458 at time 2024-06-03 14:01:42.279789\n",
      "Printing jpg 383 of 458 at time 2024-06-03 14:01:45.904608\n",
      "Printing jpg 384 of 458 at time 2024-06-03 14:01:49.161588\n",
      "Printing jpg 385 of 458 at time 2024-06-03 14:01:52.246915\n",
      "Printing jpg 386 of 458 at time 2024-06-03 14:01:55.153574\n",
      "Printing jpg 387 of 458 at time 2024-06-03 14:01:59.196777\n",
      "Printing jpg 388 of 458 at time 2024-06-03 14:02:02.628917\n",
      "Printing jpg 389 of 458 at time 2024-06-03 14:02:05.689220\n",
      "Printing jpg 390 of 458 at time 2024-06-03 14:02:12.217696\n",
      "Printing jpg 391 of 458 at time 2024-06-03 14:02:17.091658\n",
      "Printing jpg 392 of 458 at time 2024-06-03 14:02:21.909073\n",
      "Printing jpg 393 of 458 at time 2024-06-03 14:02:25.153544\n",
      "Printing jpg 394 of 458 at time 2024-06-03 14:02:29.097152\n",
      "Printing jpg 395 of 458 at time 2024-06-03 14:02:32.061367\n",
      "Printing jpg 396 of 458 at time 2024-06-03 14:02:35.275307\n",
      "Printing jpg 397 of 458 at time 2024-06-03 14:02:38.083380\n",
      "Printing jpg 398 of 458 at time 2024-06-03 14:02:40.592676\n",
      "Printing jpg 399 of 458 at time 2024-06-03 14:02:44.031325\n",
      "Printing jpg 400 of 458 at time 2024-06-03 14:02:47.622610\n",
      "Printing jpg 401 of 458 at time 2024-06-03 14:02:51.247429\n",
      "Printing jpg 402 of 458 at time 2024-06-03 14:02:58.186281\n",
      "Printing jpg 403 of 458 at time 2024-06-03 14:03:01.091940\n",
      "Printing jpg 404 of 458 at time 2024-06-03 14:03:04.260342\n",
      "Printing jpg 405 of 458 at time 2024-06-03 14:03:07.652950\n",
      "Printing jpg 406 of 458 at time 2024-06-03 14:03:20.441673\n",
      "Printing jpg 407 of 458 at time 2024-06-03 14:03:31.378192\n",
      "Printing jpg 408 of 458 at time 2024-06-03 14:03:36.561437\n",
      "Printing jpg 409 of 458 at time 2024-06-03 14:03:39.684300\n",
      "Printing jpg 410 of 458 at time 2024-06-03 14:03:42.194597\n",
      "Printing jpg 411 of 458 at time 2024-06-03 14:03:44.468180\n",
      "Printing jpg 412 of 458 at time 2024-06-03 14:03:46.838853\n",
      "Printing jpg 413 of 458 at time 2024-06-03 14:03:49.280590\n",
      "Printing jpg 414 of 458 at time 2024-06-03 14:03:51.590206\n",
      "Printing jpg 415 of 458 at time 2024-06-03 14:03:54.029438\n",
      "Printing jpg 416 of 458 at time 2024-06-03 14:03:56.342555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing jpg 417 of 458 at time 2024-06-03 14:03:58.873374\n",
      "Printing jpg 418 of 458 at time 2024-06-03 14:04:01.156462\n",
      "Printing jpg 419 of 458 at time 2024-06-03 14:04:09.686272\n",
      "Printing jpg 420 of 458 at time 2024-06-03 14:04:12.342702\n",
      "Printing jpg 421 of 458 at time 2024-06-03 14:04:17.093551\n",
      "Printing jpg 422 of 458 at time 2024-06-03 14:04:22.113149\n",
      "Printing jpg 423 of 458 at time 2024-06-03 14:04:25.716446\n",
      "Printing jpg 424 of 458 at time 2024-06-03 14:04:28.309321\n",
      "Printing jpg 425 of 458 at time 2024-06-03 14:04:32.404067\n",
      "Printing jpg 426 of 458 at time 2024-06-03 14:04:34.966915\n",
      "Printing jpg 427 of 458 at time 2024-06-03 14:04:37.422161\n",
      "Printing jpg 428 of 458 at time 2024-06-03 14:04:39.904432\n",
      "Printing jpg 429 of 458 at time 2024-06-03 14:04:42.816600\n",
      "Printing jpg 430 of 458 at time 2024-06-03 14:04:45.278853\n",
      "Printing jpg 431 of 458 at time 2024-06-03 14:04:47.841700\n",
      "Printing jpg 432 of 458 at time 2024-06-03 14:04:50.375018\n",
      "Printing jpg 433 of 458 at time 2024-06-03 14:04:53.093008\n",
      "Printing jpg 434 of 458 at time 2024-06-03 14:04:58.311782\n",
      "Printing jpg 435 of 458 at time 2024-06-03 14:05:00.906659\n",
      "Printing jpg 436 of 458 at time 2024-06-03 14:05:03.488021\n",
      "Printing jpg 437 of 458 at time 2024-06-03 14:05:06.261061\n",
      "Printing jpg 438 of 458 at time 2024-06-03 14:05:08.999566\n",
      "Printing jpg 439 of 458 at time 2024-06-03 14:05:11.560909\n",
      "Printing jpg 440 of 458 at time 2024-06-03 14:05:16.121584\n",
      "Printing jpg 441 of 458 at time 2024-06-03 14:05:30.480727\n",
      "Printing jpg 442 of 458 at time 2024-06-03 14:05:32.879424\n",
      "Printing jpg 443 of 458 at time 2024-06-03 14:05:35.436763\n",
      "Printing jpg 444 of 458 at time 2024-06-03 14:05:37.967582\n",
      "Printing jpg 445 of 458 at time 2024-06-03 14:05:40.474875\n",
      "Printing jpg 446 of 458 at time 2024-06-03 14:05:42.994180\n",
      "Printing jpg 447 of 458 at time 2024-06-03 14:05:45.749203\n",
      "Printing jpg 448 of 458 at time 2024-06-03 14:05:48.468691\n",
      "Printing jpg 449 of 458 at time 2024-06-03 14:05:51.154654\n",
      "Printing jpg 450 of 458 at time 2024-06-03 14:05:53.780056\n",
      "Printing jpg 451 of 458 at time 2024-06-03 14:05:56.348406\n",
      "Printing jpg 452 of 458 at time 2024-06-03 14:05:58.875220\n",
      "Printing jpg 453 of 458 at time 2024-06-03 14:06:01.436563\n",
      "Printing jpg 454 of 458 at time 2024-06-03 14:06:04.218618\n",
      "Printing jpg 455 of 458 at time 2024-06-03 14:06:06.782964\n",
      "Printing jpg 456 of 458 at time 2024-06-03 14:06:09.059550\n",
      "Printing jpg 457 of 458 at time 2024-06-03 14:06:11.469755\n",
      "Printing jpg 458 of 458 at time 2024-06-03 14:06:14.500528\n",
      "the following pages failed: []\n",
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "#Export jpgs\n",
    "if run_jpg:\n",
    "    aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "    project_path = aprx.filePath\n",
    "\n",
    "    jpg_folder = model_output_folder + r'\\jpg'\n",
    "    if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "\n",
    "    # project_directory = os.path.dirname(project_path)\n",
    "\n",
    "    layouts = aprx.listLayouts()\n",
    "    export_fails = []\n",
    "\n",
    "    for layout in layouts:\n",
    "\n",
    "        if layout.mapSeries is not None:\n",
    "            map_series = layout.mapSeries\n",
    "            # Loop through all pages in the map series\n",
    "            for page_number in range(1, map_series.pageCount + 1):\n",
    "                map_series.currentPageNumber = page_number\n",
    "                output_filename = os.path.join(jpg_folder, f\"{map_series.pageRow.Drains_To}.jpg\")\n",
    "                try:\n",
    "                    layout.exportToJPEG(output_filename, resolution=300)\n",
    "                except:\n",
    "                    print(f'WARNING! {map_series.pageRow.Drains_To} could not be made')\n",
    "                    export_fails.append(map_series.pageRow.Drains_To)\n",
    "                print (f'Printing jpg {page_number} of {map_series.pageCount} at time {datetime.datetime.now()}')\n",
    "\n",
    "    print(f'the following pages failed: {export_fails}')            \n",
    "    print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\\\\\prdsynfile01\\\\lws_modelling\\\\SEWER_AREA_MODELS\\\\NSSA\\\\04_ANALYSIS_WORK\\\\RAWN_From_Model\\\\jpg\\\\6786.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "In  \u001b[0;34m[200]\u001b[0m:\nLine \u001b[0;34m71\u001b[0m:    img = Image(img_path)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\hloecke\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-mike\\Lib\\site-packages\\openpyxl\\drawing\\image.py\u001b[0m, in \u001b[0;32m__init__\u001b[0m:\nLine \u001b[0;34m32\u001b[0m:    image = _import_image(img)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\hloecke\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-mike\\Lib\\site-packages\\openpyxl\\drawing\\image.py\u001b[0m, in \u001b[0;32m_import_image\u001b[0m:\nLine \u001b[0;34m16\u001b[0m:    img = PILImage.open(img)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34mC:\\Users\\hloecke\\AppData\\Local\\ESRI\\conda\\envs\\arcgispro-py3-mike\\Lib\\site-packages\\PIL\\Image.py\u001b[0m, in \u001b[0;32mopen\u001b[0m:\nLine \u001b[0;34m3131\u001b[0m:  fp = builtins.open(filename, \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\\\\\prdsynfile01\\\\lws_modelling\\\\SEWER_AREA_MODELS\\\\NSSA\\\\04_ANALYSIS_WORK\\\\RAWN_From_Model\\\\jpg\\\\6786.jpg'\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Permanent cell 13\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "\n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13     \n",
    "\n",
    "        sheet = workbook[\"Map\"]\n",
    "\n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        # Set the position for the image (e.g., 'B2' for cell B2)\n",
    "        sheet.add_image(img, 'B2')\n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 14\n",
    "#Create HTMLs\n",
    "\n",
    "try:\n",
    "    if run_html:\n",
    "        html_folder = model_output_folder + '\\\\HTML'\n",
    "        \n",
    "        shutil.copy2('style.css', html_folder + '\\\\style.css')\n",
    "        shutil.copy2('script.js', html_folder + '\\\\script.js')\n",
    "\n",
    "        for category in categories:\n",
    "            area_type = category[0]\n",
    "            area_names = category[1]\n",
    "            header_start = category[2]\n",
    "\n",
    "            f = open(html_folder + '\\\\Population_By_' + area_type + '_' + model_area + '.html', \"w\")\n",
    "            f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "            f.write('<script src=\"script.js\"></script>\\n')\n",
    "            f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "            f.write('<!DOCTYPE html>\\n')\n",
    "            f.write('<html>\\n')\n",
    "            f.write('<head>\\n')\n",
    "            f.write('<meta charset=\"utf-8\">\\n')\n",
    "            f.write('</head>\\n')\n",
    "            f.write('<body>\\n\\n')\n",
    "\n",
    "            f.write('<div class=\"tab\">\\n')\n",
    "            for area_name in area_names:\n",
    "                tab = area_name\n",
    "\n",
    "            #     color = ps_dict[first_year]\n",
    "            #     bg_color = color_dict[color][0]\n",
    "            #     text_color = color_dict[color][1]\n",
    "\n",
    "                f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "            f.write('</div>\\n')\n",
    "\n",
    "            pop_df = pop_dfss[0][2]\n",
    "\n",
    "            for area_name in area_names:\n",
    "\n",
    "                area_df = pop_df[pop_df[area_type]==area_name]\n",
    "                area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "\n",
    "                f.write('<div id=\"' + area_name + '\" class=\"tabcontent\">\\n') \n",
    "                f.write('<h1>' + area_name + '</h1>\\n')\n",
    "\n",
    "                f.write('<div class=\"sidenav\">\\n')\n",
    "\n",
    "                f.write('<table style=\\'width: 90%;\\'>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<th>Year</th>\\n')\n",
    "                f.write('<th>Population</th>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "\n",
    "                for index, row in area_df.iterrows():\n",
    "                    f.write('<tr>\\n')\n",
    "                    f.write('<td>'+ str(index) + '</td>\\n')\n",
    "                    population_with_separator = f\"{int(row['Population']):,}\"\n",
    "                    f.write('<td>'+ population_with_separator + '</td>\\n')\n",
    "\n",
    "                    f.write('</tr>\\n')\n",
    "                f.write('</table>\\n')\n",
    "\n",
    "                for i in range(4):\n",
    "                    f.write('<h1 style=\"color: white\">End of tables</h1>\\n')#Invisible, just to enable scroll to table bottoms\n",
    "\n",
    "                f.write('</div>\\n') #end sidenav\n",
    "\n",
    "\n",
    "                f.write('<div class=\"main\">\\n')\n",
    "\n",
    "                fig = go.Figure()\n",
    "\n",
    "\n",
    "                fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                             y = area_df.Population, \n",
    "                                             mode='lines',name=pop_dfss[0][0],line=dict(width=5)))\n",
    "\n",
    "                for pop_dfs in pop_dfss[1:]:\n",
    "                    pop_df_past = pop_dfs[2]\n",
    "                    area_df = pop_df_past[pop_df_past[area_type]==area_name]\n",
    "                    area_df = area_df[['Year','Population']].groupby(['Year']).sum()\n",
    "                    fig.add_trace(go.Scatter(x=area_df.index, \n",
    "                                             y = area_df.Population, \n",
    "                                             mode='lines',name=pop_dfs[0],line=dict(width=2)))\n",
    "\n",
    "                fig.update_layout(\n",
    "                    title=header_start + area_name,\n",
    "                    autosize=False,\n",
    "                    width = 1500,\n",
    "                    height=850,\n",
    "                    margin=dict(\n",
    "                        l=50,\n",
    "                        r=50,\n",
    "                        b=50,\n",
    "                        t=50,\n",
    "                        pad=4\n",
    "                        ),\n",
    "                        yaxis_title = 'Population'\n",
    "                    )\n",
    "\n",
    "                f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "                f.write('</div>\\n') #end div main  \n",
    "\n",
    "                f.write('</div>\\n')  #end div tab   \n",
    "\n",
    "                f.write('</body>\\n')\n",
    "            f.write('</html>\\n')\n",
    "            f.close()\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 14', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obsolete cell\n",
    "# #Dissolve catchments\n",
    "# arcpy.env.addOutputsToMap = False\n",
    "# if run_dissolve:\n",
    "#     arcpy.management.CreateFileGDB(output_folder, gdb_name_dissolve)\n",
    "#     dissolve_path = output_folder + '\\\\' + gdb_name_dissolve\n",
    "#     arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'msm_Catchment')\n",
    "#     nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "#     for i, node in enumerate(nodes):\n",
    "#         print('Dissolving for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "#         catchment_df = accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==node]\n",
    "#         catchments = list(catchment_df[('GENERAL INFO','CATCHMENT')].unique())\n",
    "#         arcpy.management.CalculateField(dissolve_path + '\\\\msm_Catchment', \"Drains_To\", \"''\", \"PYTHON3\")\n",
    "#         with arcpy.da.UpdateCursor(dissolve_path + '\\\\msm_catchment', ['muid', 'Drains_To']) as cursor:\n",
    "#             for row in cursor:\n",
    "#                 if row[0] in catchments:\n",
    "#                     row[1] = node\n",
    "#                     cursor.updateRow(row)\n",
    "\n",
    "#         query = \"Drains_To = 'Test'\"\n",
    "#         arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\msm_catchment', \"temp_layer\", \"Drains_To = '\" + node + \"'\")\n",
    "#         dissolve_output = dissolve_path + '\\\\msm_Catchment_Dissolve_Single'\n",
    "#         arcpy.management.Dissolve(\"temp_layer\", dissolve_output, \"Drains_To\", \"\", \"MULTI_PART\")\n",
    "#         arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "#         arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\msm_Catchment_Dissolve_Single', dissolve_path, 'Node_Catchment_' + node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obsolete cell\n",
    "#Append individual dissolved catchments to one layer.\n",
    "\n",
    "# if run_dissolve_append:\n",
    "#     nodes = list(accumulation_df[('GENERAL INFO','NODE')].unique())\n",
    "#     for i, node in enumerate(nodes):    \n",
    "#         print('Appending for node ' + str(i) + ' of ' + str(len(nodes)) + ' at time ' + str(datetime.datetime.now()))\n",
    "#         if i == 0:\n",
    "#             arcpy.conversion.FeatureClassToFeatureClass(dissolve_path + '\\\\Node_Catchment_' + node, out_path, 'Node_Catchment')\n",
    "#         else:\n",
    "#             arcpy.management.Append(dissolve_path + '\\\\Node_Catchment_' + node, \"Node_Catchment\", \"NO_TEST\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
