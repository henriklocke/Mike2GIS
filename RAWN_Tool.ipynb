{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool updated: 2024-06-24, Henrik Loecke\n",
    "\n",
    "Tool summary:\n",
    "This tool creates RAWN sheets, based on the MIKE+ model and MPF population file. It traces through the model to list the upstream catchments of each manhole (in flow splits, each split direction will both get 100% of the upstream flow). It then uses the MPF sheets to summarise population and residential/ICI areas for each catchment. Aggregated catchment contributions to each manhole are then used as input to RAWN calculations. For each manhole, the upstream catchments are dissolved together and added to a layer 'Node_Catchment'. This is used as input to 'Map Series' (similar to 'Data Driven Pages' in ArcGIS Desktop). This is used to create one jpg image per manhole, highlighting the manhole and all its upstream catchments. The RAWN calculations and images are added to RAWN spreadsheets which are similar in layout to the original RAWN sheets which are derived from Neural Network. One important difference in RAWN calculations to previous is that no peaking factor is allowed to go below 1.5, as per official RAWN guidelines. Peaking factors that reach the minimum are highlighted in yellow. Also a Google Maps link is added to each sheet. The user can choose between adding RAWN calculations as values or Excel formulas. Excel formulas are recommended for full transparency of the calculations to those using the sheets. However, python calculations are maintained in the code to enable future additional outputs (Power BI, HTML etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 1\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, numbers\n",
    "from openpyxl.formatting.rule import FormulaRule\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "import subprocess\n",
    "import gc\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 2\n",
    "\n",
    "#Create functions for to use SQL with the model sqlite database.\n",
    "\n",
    "def sql_to_df(sql,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def execute_sql(sqls,model):\n",
    "    con = sqlite3.connect(model)\n",
    "    cur = con.cursor()\n",
    "    if type(sqls) == list:\n",
    "        for sql in sqls:\n",
    "            cur.execute(sql)\n",
    "    else:         \n",
    "        cur.execute(sqls)\n",
    "    cur.close()\n",
    "    con.commit()\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 3\n",
    "\n",
    "#User Input\n",
    "\n",
    "model = 'FSA'\n",
    "\n",
    "max_steps = 1000 \n",
    "use_formula = True\n",
    "\n",
    "check_scenario = True #Scenario check is for max active altid. In some hypothetical setups it can be incorrect. \n",
    "#                      If you verify the scenario is correct (open model to check) but get the error, set this check to False\n",
    "#                      A more robust check may be developed in the future.\n",
    "\n",
    "global_output_folder = arcpy.mp.ArcGISProject(\"CURRENT\").homeFolder\n",
    "\n",
    "if model == 'NSSA':   \n",
    "    model_output_folder = r\"\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\NSSA\\04_ANALYSIS_WORK\\RAWN_From_Model\"\n",
    "    model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\Automation\\NSSA_Base_2018pop.sqlite\"\n",
    "    pop_book = r\"\\\\prdsynfile01\\LWS_Modelling\\SEWER_AREA_MODELS\\NSSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\MPF4_Temp_Hold\\NSSA_Master_Population_File_4_No_2237_ResArea.xlsx\"\n",
    "    pop_sheet = 'MPF Update 4'\n",
    "    scenario = 'Base'\n",
    "    wwtp_muid = '22602'\n",
    "    line_exclusions = []\n",
    "    years = [2060,2070,2080,2090,2100]\n",
    "    model_version = 89\n",
    "    excluded_acronyms_csv = ''\n",
    "    \n",
    "if model == 'FSA':   \n",
    "    model_output_folder = r'\\\\prdsynfile01\\lws_modelling\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model'\n",
    "    model_path = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\Model_Result_To_GIS\\FSA_Base_2021pop.sqlite\"\n",
    "    pop_book = r\"J:\\SEWER_AREA_MODELS\\FSA\\02_MODEL_COMPONENTS\\04_DATA\\01. POPULATION\\FSA_Master_Population_File.xlsx\"\n",
    "    pop_sheet = 'MPF Update 16'\n",
    "    scenario = '2030_Network'\n",
    "    wwtp_muid = '1162'\n",
    "    line_exclusions = ['GoldenEar_Dummy_Pump','GoldenEarSSOLink','GoldenEar_SSO_Tank_Cell_Dummy_Link3','MH35_Orifice','44473']\n",
    "    years = [2060,2070,2076]\n",
    "    model_version = 150\n",
    "    excluded_acronyms_csv = r\"J:\\SEWER_AREA_MODELS\\FSA\\04_ANALYSIS_WORK\\RAWN_From_Model\\FSA_Excluded_Acronyms.csv\"\n",
    "    \n",
    "    \n",
    "sewer_area = model\n",
    "gdb_name = 'RAWN.gdb'\n",
    "gdb_name_dissolve = 'RAWN_Dissolve.gdb' #To keep clutter out of main database\n",
    "\n",
    "#Options to skip time consuming steps during debug (by setting to False), must be True during production runs.\n",
    "run_dissolve = True\n",
    "run_jpg = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 4\n",
    "\n",
    "#Set up column names\n",
    "\n",
    "try:\n",
    "    \n",
    "    categories = ['res','com','ind','inst','infl','infi']\n",
    "\n",
    "    mpf_col_dict = {}\n",
    "\n",
    "    area_col_dict = {}\n",
    "    area_col_dict['res'] = 'Area_Res'\n",
    "    area_col_dict['com'] = 'Area_Com'\n",
    "    area_col_dict['ind'] = 'Area_Ind'\n",
    "    area_col_dict['inst'] = 'Area_Inst'\n",
    "    area_col_dict['ini'] = 'Area_Total'\n",
    "\n",
    "    per_unit_dict = {}\n",
    "    per_unit_dict['res'] = 320\n",
    "    per_unit_dict['com'] = 33700 \n",
    "    per_unit_dict['ind'] = 56200\n",
    "    per_unit_dict['inst'] = 33700\n",
    "    per_unit_dict['infl'] = 5600\n",
    "    per_unit_dict['infi'] = 5600\n",
    "\n",
    "    unit_dict = {}\n",
    "    unit_dict['res'] = 'L/c/d'\n",
    "    unit_dict['com'] = 'L/ha/d'\n",
    "    unit_dict['ind'] = 'L/ha/d'\n",
    "    unit_dict['inst'] = 'L/ha/d'\n",
    "    unit_dict['infl'] = 'L/ha/d'\n",
    "\n",
    "\n",
    "    header_dict = {}\n",
    "    # header_dict['gen'] = ['GENERAL INFO',['TYPE','MODELID','CATCHMENT','ID','YEAR','LOCATION']]\n",
    "    header_dict['gen'] = ['GENERAL INFO',['TYPE','CATCHMENT','YEAR','LOCATION']]\n",
    "    header_dict['res'] = ['RESIDENTIAL',['AREA (Ha)','POPULATION','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['com'] = ['COMMERCIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ind'] = ['INDUSTRIAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['inst'] = ['INSTITUTIONAL',['AREA (Ha)','AVG. FLOW (L/s)','PEAK FLOW (L/s)']]\n",
    "    header_dict['ini'] = ['INFLOW / INFILTRATION',['AREA (Ha)','INFLOW (L/s)','INFILTRATION (L/s)']]\n",
    "    header_dict['flow'] = ['FLOWS',['AVG. SAN. FLOW (L/s)','ADWF (L/s)','PWWF (L/s)']]\n",
    "\n",
    "    avg_calc_dict = {}\n",
    "    #Items: [Keyword (upper Excel header),Type ('lower Excel header'),Average(lower Excel header),Unit flow cell address, quantifyer column]\n",
    "    avg_calc_dict['res'] = ['RESIDENTIAL','POPULATION','AVG. FLOW (L/s)','$D$3','H']\n",
    "    avg_calc_dict['com'] = ['COMMERCIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$4','K']\n",
    "    avg_calc_dict['ind'] = ['INDUSTRIAL','AREA (Ha)','AVG. FLOW (L/s)','$D$5','N']\n",
    "    avg_calc_dict['inst'] = ['INSTITUTIONAL','AREA (Ha)','AVG. FLOW (L/s)','$D$6','Q']\n",
    "    avg_calc_dict['infl'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFLOW (L/s)','$D$7','T']\n",
    "    avg_calc_dict['infi'] = ['INFLOW / INFILTRATION','AREA (Ha)','INFILTRATION (L/s)','$D$7','T']\n",
    "\n",
    "    header_tuples = []\n",
    "    for header in header_dict:\n",
    "        for sub_header in (header_dict[header][1]):\n",
    "            header_tuples.append((header_dict[header][0],sub_header))\n",
    "    header_tuples\n",
    "\n",
    "    # columns_multiindex = pd.MultiIndex.from_tuples(header_tuples,names=['Category', 'Subcategory'])\n",
    "    columns_multiindex = pd.MultiIndex.from_tuples(header_tuples)\n",
    "    df_template = pd.DataFrame(columns=columns_multiindex)\n",
    "\n",
    "    info_list = []\n",
    "    for item in unit_dict:\n",
    "        info_list.append([avg_calc_dict[item][0],per_unit_dict[item],unit_dict[item]])\n",
    "    info_df = pd.DataFrame(info_list,columns=['DESCRIPTION','AVG. FLOW','UNITS'])\n",
    "    info_df.set_index('DESCRIPTION',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 4', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning. The sum of 'Population' (113071710) is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (112676302)\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 5\n",
    "\n",
    "#Import population\n",
    "\n",
    "try:\n",
    "    pop_df = pd.read_excel(pop_book,sheet_name=pop_sheet,dtype={'Catchment': str})#[['Catchment','Year','Pop_Total']]\n",
    "    pop_df.rename(columns={\"Pop_Total\": \"Population\"},inplace=True)\n",
    "    pop_df = pop_df[['Catchment','Year','Pop_ResLD','Pop_ResHD','Pop_Mixed','Population','Area_ResLD','Area_ResHD','Area_Mixed','Area_Com','Area_Ind','Area_Inst']]\n",
    "    pop_df.fillna(0, inplace=True) #Fill NA with 0 or the sum of all will be NA\n",
    "    pop_df['Area_Res'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed\n",
    "    pop_df['Area_Total'] = pop_df.Area_ResLD + pop_df.Area_ResHD + pop_df.Area_Mixed + pop_df.Area_Com + pop_df.Area_Ind + pop_df.Area_Inst\n",
    "    pop_df['Population_Sum_Check'] = pop_df.Pop_ResLD + pop_df.Pop_ResHD + pop_df.Pop_Mixed\n",
    "        \n",
    "    pop_sum_total_col = int(pop_df.Population.sum())\n",
    "    pop_sum_sub_cols = int(pop_df.Pop_ResLD.sum() + pop_df.Pop_ResHD.sum() + pop_df.Pop_Mixed.sum())\n",
    "    pop_df['Key'] = sewer_area + '@' + pop_df.Catchment + '@' + pop_df['Year'].astype(str)\n",
    "    pop_df.set_index('Key',inplace=True)\n",
    "\n",
    "    if pop_sum_total_col != pop_sum_sub_cols:\n",
    "          print(\"Warning. The sum of 'Population' (\" + str(pop_sum_total_col) + \") is different than the sum of 'Pop_ResLD' + 'Pop_ResHD' + 'Pop_Mixed' (\" + str(pop_sum_sub_cols) + \")\") \n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 5', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 6\n",
    "\n",
    "#Import model data\n",
    "\n",
    "try:\n",
    "    node_types = {}\n",
    "    node_types[1] = 'Manhole'\n",
    "    node_types[2] = 'Basin'\n",
    "    node_types[3] = 'Outlet'\n",
    "    node_types[4] = 'Junction'\n",
    "    node_types[5] = 'Soakaway'\n",
    "    node_types[6] = 'River Junction'\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Loadpoint WHERE Active = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    sql = \"SELECT max(AltID) FROM msm_Link WHERE Active = 1\"\n",
    "    altid = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if altid == 0:\n",
    "        active_scenario = 'Base'\n",
    "    else:\n",
    "        sql = \"SELECT MUID FROM m_ScenarioManagementAlternative WHERE GroupID = 'CS_Network' AND AltID = \" + str(altid)\n",
    "        active_scenario = sql_to_df(sql,model_path).iloc[0,0]\n",
    "\n",
    "    if scenario != active_scenario:\n",
    "        raise ValueError(f'Scenario {scenario} was requested in the user input but scenario {active_scenario} is active in the model')\n",
    "    \n",
    "    sql = \"SELECT catchid AS Catchment, nodeid AS Connected_Node FROM msm_Catchcon WHERE Active = 1\"\n",
    "    catchments = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], uplevel AS Outlet_Level FROM msm_Link WHERE Active = 1\"\n",
    "    lines = sql_to_df(sql,model_path)\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Orifice WHERE Active = 1\"\n",
    "    orifices = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,orifices])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], invertlevel AS Outlet_Level FROM msm_Valve WHERE Active = 1\"\n",
    "    valves = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,valves])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], crestlevel AS Outlet_Level FROM msm_Weir WHERE Active = 1\"\n",
    "    weirs = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,weirs])\n",
    "\n",
    "    sql = \"SELECT muid AS MUID, fromnodeid AS [From], tonodeid as [To], startlevel AS Outlet_Level FROM msm_Pump WHERE Active = 1\"\n",
    "    pumps = sql_to_df(sql,model_path)\n",
    "    lines = pd.concat([lines,pumps])\n",
    "\n",
    "    lines['Outlet_Level'].fillna(-9999, inplace=True)\n",
    "    \n",
    "    lines = lines[~lines['MUID'].isin(line_exclusions)]\n",
    "    \n",
    "    excluded_acronyms = []\n",
    "    if excluded_acronyms_csv != '':\n",
    "        excluded_acronyms = pd.read_csv(excluded_acronyms_csv)\n",
    "        excluded_acronyms = list(excluded_acronyms.iloc[:, 0])\n",
    "\n",
    "    sql = \"SELECT muid, acronym, owner, assetname FROM msm_Node WHERE active = 1\"\n",
    "    node_id_df = sql_to_df(sql,model_path)\n",
    "    node_id_df = node_id_df[(((node_id_df.assetname.str[:2]=='MH') & (node_id_df.acronym.notna()) &  \n",
    "        (node_id_df.owner=='GV') & (~node_id_df.acronym.isin(excluded_acronyms))) | (node_id_df.muid==wwtp_muid))]\n",
    "    \n",
    "    node_id_df.rename(columns={'muid':'Node'},inplace=True)\n",
    "    node_id_df['ID'] = node_id_df.acronym + '_' + node_id_df.assetname\n",
    "    node_id_df.loc[node_id_df['Node'] == wwtp_muid, 'ID'] = 'WWTP'\n",
    "    node_id_df = node_id_df[['Node','ID']]\n",
    "    \n",
    "    duplicate_ids = node_id_df[node_id_df.duplicated('ID', keep=False)]\n",
    "    node_id_df.loc[duplicate_ids.index, 'ID'] = node_id_df['ID'] + '_(' + node_id_df['Node'] + ')'\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 6', b'Error', 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 7\n",
    "#Trace the model\n",
    "\n",
    "try:\n",
    "    accumulated_catchment_set = set()\n",
    "    accumulated_node_set = set()\n",
    "\n",
    "    for index1, row1 in catchments.iterrows():\n",
    "        catchment = row1['Catchment']\n",
    "        nodes = [row1['Connected_Node']]\n",
    "        start_node = row1['Connected_Node']\n",
    "        steps = 0\n",
    "\n",
    "        accumulated_catchment_set.add((start_node,catchment))\n",
    "\n",
    "        while steps <= max_steps:\n",
    "            steps += 1\n",
    "            downstream_df = lines[lines['From'].isin(nodes)]  \n",
    "\n",
    "            if len(downstream_df) > 0:\n",
    "                nodes = list(downstream_df.To.unique())\n",
    "\n",
    "                nodes = [node for node in nodes if len(node)>0]\n",
    "                for node in nodes:\n",
    "                    accumulated_catchment_set.add((node,catchment))       \n",
    "            else:\n",
    "                break\n",
    "            if steps == max_steps:\n",
    "                raise ValueError(\"Maximum steps were reached, indicating a loop. Last node traced is '\" + node + \"'\")\n",
    "\n",
    "            accumulated_catchment_set.add((node,catchment))\n",
    "\n",
    "    accumulation_df = pd.DataFrame(accumulated_catchment_set,columns=['Node','Catchment'])\n",
    "    accumulation_df = pd.merge(accumulation_df,node_id_df,how='inner',on=['Node'])\n",
    "    data = {\n",
    "        ('GENERAL INFO', 'CATCHMENT'): accumulation_df.Catchment,\n",
    "        ('GENERAL INFO', 'NODE'): accumulation_df.Node,\n",
    "        ('GENERAL INFO', 'ID'): accumulation_df.ID,\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame with MultiIndex columns\n",
    "    accumulation_df = pd.DataFrame(data)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 8\n",
    "#Calculate RAWN\n",
    "\n",
    "try:\n",
    "    catchments = list(pop_df.Catchment.unique())\n",
    "\n",
    "    catchment_df = df_template.copy()\n",
    "    for catchment in catchments:\n",
    "        for year in years:\n",
    "            key = model + '@' + catchment + '@' + str(year)\n",
    "            catchment_df.loc[key,('GENERAL INFO','TYPE')] = 'Manhole'\n",
    "            catchment_df.loc[key,('GENERAL INFO','CATCHMENT')] = catchment\n",
    "            catchment_df.loc[key,('GENERAL INFO','YEAR')] = year\n",
    "            catchment_df.loc[key,('GENERAL INFO','LOCATION')] = model\n",
    "            for area_col_dict_key in area_col_dict:\n",
    "                catchment_df.loc[key,(header_dict[area_col_dict_key][0],'AREA (Ha)')] = pop_df.loc[key,area_col_dict[area_col_dict_key]]\n",
    "            catchment_df.loc[key,('RESIDENTIAL','POPULATION')] = pop_df.loc[key,'Population']\n",
    "            san_flow = 0\n",
    "            adwf = 0\n",
    "            for avg_calc_dict_key in avg_calc_dict:\n",
    "                input1 = catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][1])]\n",
    "                input2 = per_unit_dict[avg_calc_dict_key]\n",
    "                avg_flow = input1 * input2 / 86400\n",
    "                if avg_calc_dict_key not in ['infl','infi']:\n",
    "                    san_flow += avg_flow\n",
    "                if avg_calc_dict_key not in ['infl']:\n",
    "                    adwf += avg_flow    \n",
    "                catchment_df.loc[key,(avg_calc_dict[avg_calc_dict_key][0],avg_calc_dict[avg_calc_dict_key][2])] = avg_flow\n",
    "            catchment_df.loc[key,('FLOWS','AVG. SAN. FLOW (L/s)')] = san_flow\n",
    "            catchment_df.loc[key,('FLOWS','ADWF (L/s)')] = adwf\n",
    "\n",
    "\n",
    "    catchment_node_df = accumulation_df.merge(catchment_df,on=[('GENERAL INFO','CATCHMENT')],how='inner')\n",
    "    node_df = catchment_node_df.copy()\n",
    "    node_df.drop(columns=[('GENERAL INFO','CATCHMENT')],inplace=True)\n",
    "    node_df = node_df.groupby([('GENERAL INFO','NODE'),('GENERAL INFO','TYPE'),('GENERAL INFO','YEAR'),('GENERAL INFO','LOCATION'),('GENERAL INFO','ID')]).sum()\n",
    "    node_df.reset_index(inplace=True)\n",
    "    node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (node_df[('RESIDENTIAL','POPULATION')] / 1000) ** 0.5)),1.5) * node_df[('RESIDENTIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('COMMERCIAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['com'] * node_df[('COMMERCIAL','AREA (Ha)')]/(per_unit_dict['res'] * 1000)) ** 0.5))*0.8,1.5)*node_df[('COMMERCIAL','AVG. FLOW (L/s)')]\n",
    "    node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] = np.maximum((1 + 14 / (4 + (per_unit_dict['inst'] * node_df[('INSTITUTIONAL','AREA (Ha)')] / (per_unit_dict['res'] * 1000)) ** 0.5)),1.5) * node_df[('INSTITUTIONAL','AVG. FLOW (L/s)')]\n",
    "\n",
    "    \n",
    "    mask = node_df[('INDUSTRIAL', 'AREA (Ha)')] != 0 #Avoid error from log(0)\n",
    "    node_df.loc[mask, ('INDUSTRIAL', 'PEAK FLOW (L/s)')] = np.maximum(\n",
    "        0.8 * (\n",
    "            1 + 14 / (\n",
    "                4 + (node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] * per_unit_dict['ind'] / (per_unit_dict['res'] * 1000)) ** 0.5\n",
    "            )\n",
    "        ) * np.where(\n",
    "            node_df[('INDUSTRIAL', 'AREA (Ha)')][mask] < 121,\n",
    "            1.7,\n",
    "            2.505 - 0.1673 * np.log(node_df[('INDUSTRIAL', 'AREA (Ha)')][mask])\n",
    "        ), \n",
    "        1.5\n",
    "    ) * node_df[('INDUSTRIAL', 'AVG. FLOW (L/s)')][mask]\n",
    "    \n",
    "    node_df[('FLOWS','PWWF (L/s)')] = node_df[('RESIDENTIAL','PEAK FLOW (L/s)')] + node_df[('COMMERCIAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INDUSTRIAL','PEAK FLOW (L/s)')] + node_df[('INSTITUTIONAL','PEAK FLOW (L/s)')] \\\n",
    "        + node_df[('INFLOW / INFILTRATION','INFLOW (L/s)')] + node_df[('INFLOW / INFILTRATION','INFILTRATION (L/s)')]\n",
    "    \n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 8', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending msm_CatchCon\n",
      "Appending msm_Catchment\n",
      "Appending msm_Link\n",
      "Appending msm_Node\n",
      "Appending msm_Pump\n",
      "Appending msm_Weir\n",
      "Appending msm_Orifice\n",
      "Appending msm_Valve\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 9\n",
    "\n",
    "#Import GIS from the model\n",
    "#Note that if the RAWN database does not already exist with the layers inside, then it will be created \n",
    "#but in this casethe map symbology will be reset and Map Series reset as well, creating extra work.\n",
    "#An extra word of caution: The first import of e.g. msm_Catchment sets the maximum extend, meaning if you import NSSA in a fresh database,\n",
    "#then later import FSA, then many catchments will not be imported due to being outside the extend of the old import.\n",
    "#In the current database, an initial model with added elements outside the extend of all MV models was used to prime the layer. \n",
    "#If the database is not maintained, this will need to be done again.\n",
    "#Efforts to programmatically increase the layer extend were unsuccessful but may later get resolved.\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    out_path = global_output_folder + '\\\\' + gdb_name\n",
    "\n",
    "    if not os.path.isdir(out_path):\n",
    "        arcpy.management.CreateFileGDB(global_output_folder, gdb_name)\n",
    "\n",
    "    arcpy.env.workspace = out_path\n",
    "    sr = arcpy.SpatialReference(26910)\n",
    "\n",
    "    layers = ['msm_CatchCon','msm_Catchment','msm_Link','msm_Node','msm_Pump','msm_Weir','msm_Orifice','msm_Valve']\n",
    "\n",
    "    for layer in layers:\n",
    "\n",
    "        arcpy.management.MakeFeatureLayer(model_path + '\\\\' + layer, \"temp_layer\", \"Active = 1\")\n",
    "\n",
    "        if arcpy.Exists(out_path + '\\\\' + layer):\n",
    "            print('Appending ' + layer)\n",
    "            arcpy.management.DeleteFeatures(layer)\n",
    "            arcpy.management.Append(\"temp_layer\", layer, \"NO_TEST\")\n",
    "        else:  \n",
    "            print('Creating ' + layer)\n",
    "\n",
    "            arcpy.conversion.FeatureClassToFeatureClass(\"temp_layer\", out_path, layer + '_Test')\n",
    "            if layer == 'msm_Catchment':\n",
    "                arcpy.management.AddField('msm_catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.DefineProjection_management(layer, sr)\n",
    "\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        arcpy.Project_management('msm_Node', 'msm_Node_Google',arcpy.SpatialReference(4326))\n",
    "        centroids = arcpy.da.FeatureClassToNumPyArray('msm_Node_Google', (\"MUID\",\"SHAPE@X\",\"SHAPE@Y\"))\n",
    "        centroids = centroids.tolist()\n",
    "        centroids_df = pd.DataFrame(centroids, columns =['MUID','X','Y'])\n",
    "        centroids_df.set_index('MUID',inplace=True)\n",
    "\n",
    "            \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 9', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Permanent cell 10\n",
    "\n",
    "#Create merge_df (realizing the terms 'merge' and 'dissolve' are used here but they mean the same), \n",
    "#minimizing the number of computation heavy dissolves that need to be done.\n",
    "#It prevents the duplicate effort of merging the same catchments together multiple times,\n",
    "#by moving downstream and reusing upstream merges for downstream merges.\n",
    "\n",
    "try:\n",
    "    merge_set = set()\n",
    "\n",
    "    rank_df = accumulation_df[[('GENERAL INFO','NODE'),('GENERAL INFO','CATCHMENT')]].groupby([('GENERAL INFO','NODE')]).count()\n",
    "\n",
    "    rank_df.columns = ['Catchment_Count']\n",
    "    max_catchments = max(rank_df.Catchment_Count)\n",
    "    rank_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "\n",
    "    catchment_list = []\n",
    "    merge_set = set()\n",
    "    for index, row in rank_df.iterrows():\n",
    "\n",
    "        catchments = list(accumulation_df[accumulation_df[('GENERAL INFO','NODE')]==index][('GENERAL INFO','CATCHMENT')].unique())\n",
    "        catchments = tuple(sorted(catchments))\n",
    "        catchment_list.append(catchments)\n",
    "        merge_set.add(catchments)\n",
    "\n",
    "\n",
    "    rank_df['Catchments'] = catchment_list\n",
    "    rank_df['Node'] = rank_df.index\n",
    "\n",
    "\n",
    "    merge_list = []\n",
    "    for i, catchments in enumerate(merge_set):\n",
    "        merge_id = 'Merge_ID_' + str(i)\n",
    "        merge_list.append([merge_id,catchments])\n",
    "\n",
    "    merge_df = pd.DataFrame(merge_list,columns=['Merge_ID','Catchments'])\n",
    "    merge_df['Catchment_Count'] = merge_df['Catchments'].apply(len)\n",
    "    merge_df.sort_values(by=['Catchment_Count'],ascending=False,inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    simpler_merge = []\n",
    "    for index1, row1 in merge_df.iterrows():\n",
    "        catchments1 = list(row1['Catchments'])\n",
    "        for index2, row2 in merge_df[index1+1:].iterrows():\n",
    "            catchments2 = row2['Catchments']\n",
    "\n",
    "            if len(catchments1) >= len(catchments2):\n",
    "                if all(item in catchments1 for item in catchments2):\n",
    "                    catchments1 = [catchment for catchment in catchments1 if catchment not in catchments2]\n",
    "                    catchments1.append(row2['Merge_ID'])\n",
    "        simpler_merge.append(catchments1)\n",
    "\n",
    "    merge_df['To_Dissolve'] = simpler_merge\n",
    "    merge_df.sort_values(by=['Catchment_Count'],inplace=True)\n",
    "    merge_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    rank_df = pd.merge(rank_df,merge_df[['Merge_ID','Catchments']], on=['Catchments'],how='inner')\n",
    "    rank_df.set_index('Node',inplace=True)\n",
    "    \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 10', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_140, 0 of 373 at time 2024-06-24 15:30:10.837961\n",
      "Dissolving for Merge_ID_156, 1 of 373 at time 2024-06-24 15:30:35.732045\n",
      "Dissolving for Merge_ID_366, 2 of 373 at time 2024-06-24 15:31:15.336772\n",
      "Dissolving for Merge_ID_84, 3 of 373 at time 2024-06-24 15:31:54.998543\n",
      "Dissolving for Merge_ID_313, 4 of 373 at time 2024-06-24 15:32:18.096950\n",
      "Dissolving for Merge_ID_72, 5 of 373 at time 2024-06-24 15:33:10.586104\n",
      "Dissolving for Merge_ID_239, 6 of 373 at time 2024-06-24 15:33:36.681374\n",
      "Dissolving for Merge_ID_82, 7 of 373 at time 2024-06-24 15:33:54.822189\n",
      "Dissolving for Merge_ID_31, 8 of 373 at time 2024-06-24 15:34:59.937530\n",
      "Dissolving for Merge_ID_359, 9 of 373 at time 2024-06-24 15:35:38.740061\n",
      "Dissolving for Merge_ID_141, 10 of 373 at time 2024-06-24 15:35:53.076848\n",
      "Dissolving for Merge_ID_112, 11 of 373 at time 2024-06-24 15:36:21.086804\n",
      "Dissolving for Merge_ID_137, 12 of 373 at time 2024-06-24 15:36:56.872477\n",
      "Dissolving for Merge_ID_35, 13 of 373 at time 2024-06-24 15:38:02.208978\n",
      "Dissolving for Merge_ID_192, 14 of 373 at time 2024-06-24 15:38:16.215950\n",
      "Dissolving for Merge_ID_60, 15 of 373 at time 2024-06-24 15:38:31.150788\n",
      "Dissolving for Merge_ID_348, 16 of 373 at time 2024-06-24 15:39:27.122279\n",
      "Dissolving for Merge_ID_357, 17 of 373 at time 2024-06-24 15:39:44.582569\n",
      "Dissolving for Merge_ID_99, 18 of 373 at time 2024-06-24 15:40:02.438176\n",
      "Dissolving for Merge_ID_182, 19 of 373 at time 2024-06-24 15:40:29.499263\n",
      "Dissolving for Merge_ID_228, 20 of 373 at time 2024-06-24 15:40:56.095896\n",
      "Dissolving for Merge_ID_220, 21 of 373 at time 2024-06-24 15:41:22.143559\n",
      "Dissolving for Merge_ID_264, 22 of 373 at time 2024-06-24 15:41:43.237307\n",
      "Dissolving for Merge_ID_154, 23 of 373 at time 2024-06-24 15:42:07.285575\n",
      "Dissolving for Merge_ID_131, 24 of 373 at time 2024-06-24 15:43:10.029164\n",
      "Dissolving for Merge_ID_349, 25 of 373 at time 2024-06-24 15:43:34.288727\n",
      "Dissolving for Merge_ID_346, 26 of 373 at time 2024-06-24 15:43:57.311556\n",
      "Dissolving for Merge_ID_6, 27 of 373 at time 2024-06-24 15:45:09.903766\n",
      "Dissolving for Merge_ID_159, 28 of 373 at time 2024-06-24 15:45:23.223600\n",
      "Dissolving for Merge_ID_57, 29 of 373 at time 2024-06-24 15:46:01.698824\n",
      "Dissolving for Merge_ID_216, 30 of 373 at time 2024-06-24 15:46:15.162287\n",
      "Dissolving for Merge_ID_91, 31 of 373 at time 2024-06-24 15:46:36.082655\n",
      "Dissolving for Merge_ID_33, 32 of 373 at time 2024-06-24 15:47:21.146856\n",
      "Dissolving for Merge_ID_230, 33 of 373 at time 2024-06-24 15:47:49.465688\n",
      "Dissolving for Merge_ID_100, 34 of 373 at time 2024-06-24 15:48:06.157128\n",
      "Dissolving for Merge_ID_318, 35 of 373 at time 2024-06-24 15:48:19.510985\n",
      "Dissolving for Merge_ID_309, 36 of 373 at time 2024-06-24 15:48:43.875028\n",
      "Dissolving for Merge_ID_238, 37 of 373 at time 2024-06-24 15:49:15.387682\n",
      "Dissolving for Merge_ID_331, 38 of 373 at time 2024-06-24 15:49:39.202217\n",
      "Dissolving for Merge_ID_262, 39 of 373 at time 2024-06-24 15:50:11.034668\n",
      "Dissolving for Merge_ID_211, 40 of 373 at time 2024-06-24 15:50:24.104263\n",
      "Dissolving for Merge_ID_274, 41 of 373 at time 2024-06-24 15:50:48.588423\n",
      "Dissolving for Merge_ID_223, 42 of 373 at time 2024-06-24 15:51:24.560218\n",
      "Dissolving for Merge_ID_297, 43 of 373 at time 2024-06-24 15:51:41.266793\n",
      "Dissolving for Merge_ID_225, 44 of 373 at time 2024-06-24 15:52:37.175998\n",
      "Dissolving for Merge_ID_364, 45 of 373 at time 2024-06-24 15:53:01.343348\n",
      "Dissolving for Merge_ID_246, 46 of 373 at time 2024-06-24 15:53:17.646926\n",
      "Dissolving for Merge_ID_334, 47 of 373 at time 2024-06-24 15:53:31.111376\n",
      "Dissolving for Merge_ID_259, 48 of 373 at time 2024-06-24 15:53:55.449581\n",
      "Dissolving for Merge_ID_45, 49 of 373 at time 2024-06-24 15:54:08.524672\n",
      "Dissolving for Merge_ID_178, 50 of 373 at time 2024-06-24 15:55:04.415857\n",
      "Dissolving for Merge_ID_163, 51 of 373 at time 2024-06-24 15:55:37.476928\n",
      "Dissolving for Merge_ID_96, 52 of 373 at time 2024-06-24 15:55:50.736826\n",
      "Dissolving for Merge_ID_301, 53 of 373 at time 2024-06-24 15:56:15.543766\n",
      "Dissolving for Merge_ID_172, 54 of 373 at time 2024-06-24 15:56:36.582223\n",
      "Dissolving for Merge_ID_340, 55 of 373 at time 2024-06-24 15:56:57.827870\n",
      "Dissolving for Merge_ID_149, 56 of 373 at time 2024-06-24 15:57:14.448733\n",
      "Dissolving for Merge_ID_135, 57 of 373 at time 2024-06-24 15:57:51.400449\n",
      "Dissolving for Merge_ID_213, 58 of 373 at time 2024-06-24 15:58:15.933123\n",
      "Dissolving for Merge_ID_276, 59 of 373 at time 2024-06-24 15:58:33.253642\n",
      "Dissolving for Merge_ID_190, 60 of 373 at time 2024-06-24 15:58:46.796166\n",
      "Dissolving for Merge_ID_214, 61 of 373 at time 2024-06-24 15:59:18.943377\n",
      "Dissolving for Merge_ID_347, 62 of 373 at time 2024-06-24 15:59:40.486948\n",
      "Dissolving for Merge_ID_325, 63 of 373 at time 2024-06-24 15:59:53.809267\n",
      "Dissolving for Merge_ID_316, 64 of 373 at time 2024-06-24 16:00:19.151192\n",
      "Dissolving for Merge_ID_320, 65 of 373 at time 2024-06-24 16:00:32.634652\n",
      "Dissolving for Merge_ID_360, 66 of 373 at time 2024-06-24 16:00:53.722659\n",
      "Dissolving for Merge_ID_333, 67 of 373 at time 2024-06-24 16:01:07.115036\n",
      "Dissolving for Merge_ID_75, 68 of 373 at time 2024-06-24 16:01:24.196827\n",
      "Dissolving for Merge_ID_365, 69 of 373 at time 2024-06-24 16:01:49.683911\n",
      "Dissolving for Merge_ID_304, 70 of 373 at time 2024-06-24 16:02:22.866065\n",
      "Dissolving for Merge_ID_257, 71 of 373 at time 2024-06-24 16:02:56.120280\n",
      "Dissolving for Merge_ID_231, 72 of 373 at time 2024-06-24 16:03:17.396926\n",
      "Dissolving for Merge_ID_319, 73 of 373 at time 2024-06-24 16:03:42.668260\n",
      "Dissolving for Merge_ID_368, 74 of 373 at time 2024-06-24 16:04:05.021699\n",
      "Dissolving for Merge_ID_24, 75 of 373 at time 2024-06-24 16:04:38.200334\n",
      "Dissolving for Merge_ID_305, 76 of 373 at time 2024-06-24 16:05:10.728369\n",
      "Dissolving for Merge_ID_155, 77 of 373 at time 2024-06-24 16:05:27.966286\n",
      "Dissolving for Merge_ID_167, 78 of 373 at time 2024-06-24 16:05:41.729995\n",
      "Dissolving for Merge_ID_69, 79 of 373 at time 2024-06-24 16:05:59.252678\n",
      "Dissolving for Merge_ID_186, 80 of 373 at time 2024-06-24 16:06:20.576870\n",
      "Dissolving for Merge_ID_152, 81 of 373 at time 2024-06-24 16:07:13.965157\n",
      "Dissolving for Merge_ID_158, 82 of 373 at time 2024-06-24 16:07:31.184047\n",
      "Dissolving for Merge_ID_120, 83 of 373 at time 2024-06-24 16:08:00.561164\n",
      "Dissolving for Merge_ID_61, 84 of 373 at time 2024-06-24 16:08:38.752924\n",
      "Dissolving for Merge_ID_202, 85 of 373 at time 2024-06-24 16:09:24.079755\n",
      "Dissolving for Merge_ID_312, 86 of 373 at time 2024-06-24 16:09:57.463569\n",
      "Dissolving for Merge_ID_59, 87 of 373 at time 2024-06-24 16:10:23.415519\n",
      "Dissolving for Merge_ID_338, 88 of 373 at time 2024-06-24 16:10:56.888410\n",
      "Dissolving for Merge_ID_332, 89 of 373 at time 2024-06-24 16:11:26.210470\n",
      "Dissolving for Merge_ID_261, 90 of 373 at time 2024-06-24 16:11:44.139016\n",
      "Dissolving for Merge_ID_161, 91 of 373 at time 2024-06-24 16:12:31.235470\n",
      "Dissolving for Merge_ID_21, 92 of 373 at time 2024-06-24 16:12:49.290125\n",
      "Dissolving for Merge_ID_329, 93 of 373 at time 2024-06-24 16:13:14.915763\n",
      "Dissolving for Merge_ID_321, 94 of 373 at time 2024-06-24 16:13:36.584266\n",
      "Dissolving for Merge_ID_1, 95 of 373 at time 2024-06-24 16:13:55.674516\n",
      "Dissolving for Merge_ID_310, 96 of 373 at time 2024-06-24 16:14:21.684510\n",
      "Dissolving for Merge_ID_28, 97 of 373 at time 2024-06-24 16:14:41.316620\n",
      "Dissolving for Merge_ID_122, 98 of 373 at time 2024-06-24 16:14:55.378591\n",
      "Dissolving for Merge_ID_76, 99 of 373 at time 2024-06-24 16:15:09.266402\n",
      "Dissolving for Merge_ID_311, 100 of 373 at time 2024-06-24 16:15:47.301488\n",
      "Dissolving for Merge_ID_93, 101 of 373 at time 2024-06-24 16:16:17.933248\n",
      "Dissolving for Merge_ID_207, 102 of 373 at time 2024-06-24 16:18:05.580022\n",
      "Dissolving for Merge_ID_188, 103 of 373 at time 2024-06-24 16:18:49.172412\n",
      "Dissolving for Merge_ID_144, 104 of 373 at time 2024-06-24 16:19:08.829537\n",
      "Dissolving for Merge_ID_287, 105 of 373 at time 2024-06-24 16:20:02.142198\n",
      "Dissolving for Merge_ID_11, 106 of 373 at time 2024-06-24 16:20:22.234724\n",
      "Dissolving for Merge_ID_4, 107 of 373 at time 2024-06-24 16:20:58.583240\n",
      "Dissolving for Merge_ID_49, 108 of 373 at time 2024-06-24 16:21:17.112325\n",
      "Dissolving for Merge_ID_92, 109 of 373 at time 2024-06-24 16:21:48.561323\n",
      "Dissolving for Merge_ID_36, 110 of 373 at time 2024-06-24 16:22:07.301100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_138, 111 of 373 at time 2024-06-24 16:22:38.713051\n",
      "Dissolving for Merge_ID_115, 112 of 373 at time 2024-06-24 16:23:52.805326\n",
      "Dissolving for Merge_ID_86, 113 of 373 at time 2024-06-24 16:24:06.827753\n",
      "Dissolving for Merge_ID_134, 114 of 373 at time 2024-06-24 16:24:21.305096\n",
      "Dissolving for Merge_ID_55, 115 of 373 at time 2024-06-24 16:25:02.226368\n",
      "Dissolving for Merge_ID_272, 116 of 373 at time 2024-06-24 16:25:16.877872\n",
      "Dissolving for Merge_ID_283, 117 of 373 at time 2024-06-24 16:25:31.392249\n",
      "Dissolving for Merge_ID_244, 118 of 373 at time 2024-06-24 16:25:46.085792\n",
      "Dissolving for Merge_ID_44, 119 of 373 at time 2024-06-24 16:26:04.868607\n",
      "Dissolving for Merge_ID_162, 120 of 373 at time 2024-06-24 16:26:27.193182\n",
      "Dissolving for Merge_ID_95, 121 of 373 at time 2024-06-24 16:26:41.432306\n",
      "Dissolving for Merge_ID_184, 122 of 373 at time 2024-06-24 16:27:25.101545\n",
      "Dissolving for Merge_ID_15, 123 of 373 at time 2024-06-24 16:27:48.376990\n",
      "Dissolving for Merge_ID_101, 124 of 373 at time 2024-06-24 16:28:06.712387\n",
      "Dissolving for Merge_ID_263, 125 of 373 at time 2024-06-24 16:29:23.400127\n",
      "Dissolving for Merge_ID_354, 126 of 373 at time 2024-06-24 16:29:42.220467\n",
      "Dissolving for Merge_ID_292, 127 of 373 at time 2024-06-24 16:30:01.209963\n",
      "Dissolving for Merge_ID_277, 128 of 373 at time 2024-06-24 16:30:15.520652\n",
      "Dissolving for Merge_ID_302, 129 of 373 at time 2024-06-24 16:31:15.591540\n",
      "Dissolving for Merge_ID_250, 130 of 373 at time 2024-06-24 16:31:47.137605\n",
      "Dissolving for Merge_ID_251, 131 of 373 at time 2024-06-24 16:32:01.574904\n",
      "Dissolving for Merge_ID_197, 132 of 373 at time 2024-06-24 16:32:16.251924\n",
      "Dissolving for Merge_ID_275, 133 of 373 at time 2024-06-24 16:32:30.619156\n",
      "Dissolving for Merge_ID_278, 134 of 373 at time 2024-06-24 16:32:53.860561\n",
      "Dissolving for Merge_ID_269, 135 of 373 at time 2024-06-24 16:33:17.997790\n",
      "Dissolving for Merge_ID_212, 136 of 373 at time 2024-06-24 16:33:41.029001\n",
      "Dissolving for Merge_ID_194, 137 of 373 at time 2024-06-24 16:33:55.721532\n",
      "Dissolving for Merge_ID_342, 138 of 373 at time 2024-06-24 16:34:15.526275\n",
      "Dissolving for Merge_ID_165, 139 of 373 at time 2024-06-24 16:34:34.729960\n",
      "Dissolving for Merge_ID_80, 140 of 373 at time 2024-06-24 16:34:58.551900\n",
      "Dissolving for Merge_ID_266, 141 of 373 at time 2024-06-24 16:35:43.986743\n",
      "Dissolving for Merge_ID_248, 142 of 373 at time 2024-06-24 16:36:07.432839\n",
      "Dissolving for Merge_ID_85, 143 of 373 at time 2024-06-24 16:36:36.368488\n",
      "Dissolving for Merge_ID_315, 144 of 373 at time 2024-06-24 16:37:00.316541\n",
      "Dissolving for Merge_ID_308, 145 of 373 at time 2024-06-24 16:37:24.165498\n",
      "Dissolving for Merge_ID_196, 146 of 373 at time 2024-06-24 16:37:43.434238\n",
      "Dissolving for Merge_ID_215, 147 of 373 at time 2024-06-24 16:38:03.156396\n",
      "Dissolving for Merge_ID_179, 148 of 373 at time 2024-06-24 16:38:31.860326\n",
      "Dissolving for Merge_ID_279, 149 of 373 at time 2024-06-24 16:38:47.510735\n",
      "Dissolving for Merge_ID_289, 150 of 373 at time 2024-06-24 16:39:15.521524\n",
      "Dissolving for Merge_ID_169, 151 of 373 at time 2024-06-24 16:39:30.715512\n",
      "Dissolving for Merge_ID_132, 152 of 373 at time 2024-06-24 16:39:59.599104\n",
      "Dissolving for Merge_ID_166, 153 of 373 at time 2024-06-24 16:40:28.477194\n",
      "Dissolving for Merge_ID_136, 154 of 373 at time 2024-06-24 16:41:05.648417\n",
      "Dissolving for Merge_ID_258, 155 of 373 at time 2024-06-24 16:41:20.830394\n",
      "Dissolving for Merge_ID_121, 156 of 373 at time 2024-06-24 16:41:54.597482\n",
      "Dissolving for Merge_ID_26, 157 of 373 at time 2024-06-24 16:42:25.017980\n",
      "Dissolving for Merge_ID_107, 158 of 373 at time 2024-06-24 16:42:46.730962\n",
      "Dissolving for Merge_ID_48, 159 of 373 at time 2024-06-24 16:43:04.023876\n",
      "Dissolving for Merge_ID_327, 160 of 373 at time 2024-06-24 16:43:35.141512\n",
      "Dissolving for Merge_ID_110, 161 of 373 at time 2024-06-24 16:44:49.473420\n",
      "Dissolving for Merge_ID_232, 162 of 373 at time 2024-06-24 16:45:16.174992\n",
      "Dissolving for Merge_ID_37, 163 of 373 at time 2024-06-24 16:45:38.839850\n",
      "Dissolving for Merge_ID_38, 164 of 373 at time 2024-06-24 16:46:05.822681\n",
      "Dissolving for Merge_ID_104, 165 of 373 at time 2024-06-24 16:46:28.710247\n",
      "Dissolving for Merge_ID_322, 166 of 373 at time 2024-06-24 16:47:16.515233\n",
      "Dissolving for Merge_ID_317, 167 of 373 at time 2024-06-24 16:48:11.709009\n",
      "Dissolving for Merge_ID_240, 168 of 373 at time 2024-06-24 16:48:28.484945\n",
      "Dissolving for Merge_ID_77, 169 of 373 at time 2024-06-24 16:49:02.101872\n",
      "Dissolving for Merge_ID_109, 170 of 373 at time 2024-06-24 16:49:25.966827\n",
      "Dissolving for Merge_ID_117, 171 of 373 at time 2024-06-24 16:49:44.080490\n",
      "Dissolving for Merge_ID_370, 172 of 373 at time 2024-06-24 16:50:42.969169\n",
      "Dissolving for Merge_ID_119, 173 of 373 at time 2024-06-24 16:51:27.356003\n",
      "Dissolving for Merge_ID_303, 174 of 373 at time 2024-06-24 16:52:31.606603\n",
      "Dissolving for Merge_ID_16, 175 of 373 at time 2024-06-24 16:52:49.450013\n",
      "Dissolving for Merge_ID_193, 176 of 373 at time 2024-06-24 16:53:43.999695\n",
      "Dissolving for Merge_ID_53, 177 of 373 at time 2024-06-24 16:54:36.328323\n",
      "Dissolving for Merge_ID_20, 178 of 373 at time 2024-06-24 16:55:28.543343\n",
      "Dissolving for Merge_ID_351, 179 of 373 at time 2024-06-24 16:55:56.517069\n",
      "Dissolving for Merge_ID_350, 180 of 373 at time 2024-06-24 16:56:19.449663\n",
      "Dissolving for Merge_ID_170, 181 of 373 at time 2024-06-24 16:57:12.406360\n",
      "Dissolving for Merge_ID_0, 182 of 373 at time 2024-06-24 16:57:56.305719\n",
      "Dissolving for Merge_ID_288, 183 of 373 at time 2024-06-24 16:58:13.808811\n",
      "Dissolving for Merge_ID_235, 184 of 373 at time 2024-06-24 16:58:52.297215\n",
      "Dissolving for Merge_ID_256, 185 of 373 at time 2024-06-24 16:59:09.104667\n",
      "Dissolving for Merge_ID_372, 186 of 373 at time 2024-06-24 16:59:26.194378\n",
      "Dissolving for Merge_ID_74, 187 of 373 at time 2024-06-24 17:00:01.969429\n",
      "Dissolving for Merge_ID_217, 188 of 373 at time 2024-06-24 17:00:48.417635\n",
      "Dissolving for Merge_ID_265, 189 of 373 at time 2024-06-24 17:01:11.135521\n",
      "Dissolving for Merge_ID_209, 190 of 373 at time 2024-06-24 17:02:24.470436\n",
      "Dissolving for Merge_ID_39, 191 of 373 at time 2024-06-24 17:03:01.968900\n",
      "Dissolving for Merge_ID_290, 192 of 373 at time 2024-06-24 17:03:50.877880\n",
      "Dissolving for Merge_ID_341, 193 of 373 at time 2024-06-24 17:04:30.574363\n",
      "Dissolving for Merge_ID_336, 194 of 373 at time 2024-06-24 17:05:10.096687\n",
      "Dissolving for Merge_ID_145, 195 of 373 at time 2024-06-24 17:05:44.923695\n",
      "Dissolving for Merge_ID_314, 196 of 373 at time 2024-06-24 17:06:24.298889\n",
      "Dissolving for Merge_ID_237, 197 of 373 at time 2024-06-24 17:07:04.674995\n",
      "Dissolving for Merge_ID_285, 198 of 373 at time 2024-06-24 17:07:56.374500\n",
      "Dissolving for Merge_ID_282, 199 of 373 at time 2024-06-24 17:08:29.578513\n",
      "Dissolving for Merge_ID_58, 200 of 373 at time 2024-06-24 17:09:20.693481\n",
      "Dissolving for Merge_ID_293, 201 of 373 at time 2024-06-24 17:10:02.743119\n",
      "Dissolving for Merge_ID_195, 202 of 373 at time 2024-06-24 17:10:38.673637\n",
      "Dissolving for Merge_ID_97, 203 of 373 at time 2024-06-24 17:11:09.476941\n",
      "Dissolving for Merge_ID_229, 204 of 373 at time 2024-06-24 17:11:32.508104\n",
      "Dissolving for Merge_ID_353, 205 of 373 at time 2024-06-24 17:12:34.100190\n",
      "Dissolving for Merge_ID_205, 206 of 373 at time 2024-06-24 17:13:54.375441\n",
      "Dissolving for Merge_ID_199, 207 of 373 at time 2024-06-24 17:15:33.282797\n",
      "Dissolving for Merge_ID_291, 208 of 373 at time 2024-06-24 17:16:24.674003\n",
      "Dissolving for Merge_ID_252, 209 of 373 at time 2024-06-24 17:16:42.311707\n",
      "Dissolving for Merge_ID_371, 210 of 373 at time 2024-06-24 17:17:00.607511\n",
      "Dissolving for Merge_ID_198, 211 of 373 at time 2024-06-24 17:17:24.959873\n",
      "Dissolving for Merge_ID_43, 212 of 373 at time 2024-06-24 17:18:02.079959\n",
      "Dissolving for Merge_ID_50, 213 of 373 at time 2024-06-24 17:18:30.620183\n",
      "Dissolving for Merge_ID_13, 214 of 373 at time 2024-06-24 17:18:52.831578\n",
      "Dissolving for Merge_ID_126, 215 of 373 at time 2024-06-24 17:19:13.113202\n",
      "Dissolving for Merge_ID_294, 216 of 373 at time 2024-06-24 17:19:37.823893\n",
      "Dissolving for Merge_ID_247, 217 of 373 at time 2024-06-24 17:20:09.253754\n",
      "Dissolving for Merge_ID_253, 218 of 373 at time 2024-06-24 17:20:28.528453\n",
      "Dissolving for Merge_ID_242, 219 of 373 at time 2024-06-24 17:21:28.348887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_187, 220 of 373 at time 2024-06-24 17:21:59.541529\n",
      "Dissolving for Merge_ID_139, 221 of 373 at time 2024-06-24 17:22:24.053032\n",
      "Dissolving for Merge_ID_19, 222 of 373 at time 2024-06-24 17:22:48.278776\n",
      "Dissolving for Merge_ID_330, 223 of 373 at time 2024-06-24 17:23:31.239743\n",
      "Dissolving for Merge_ID_153, 224 of 373 at time 2024-06-24 17:23:56.563992\n",
      "Dissolving for Merge_ID_98, 225 of 373 at time 2024-06-24 17:24:51.058525\n",
      "Dissolving for Merge_ID_241, 226 of 373 at time 2024-06-24 17:25:21.790739\n",
      "Dissolving for Merge_ID_78, 227 of 373 at time 2024-06-24 17:26:19.948131\n",
      "Dissolving for Merge_ID_245, 228 of 373 at time 2024-06-24 17:26:38.636705\n",
      "Dissolving for Merge_ID_191, 229 of 373 at time 2024-06-24 17:27:04.195167\n",
      "Dissolving for Merge_ID_271, 230 of 373 at time 2024-06-24 17:27:32.103780\n",
      "Dissolving for Merge_ID_268, 231 of 373 at time 2024-06-24 17:28:42.614995\n",
      "Dissolving for Merge_ID_150, 232 of 373 at time 2024-06-24 17:29:15.059772\n",
      "Dissolving for Merge_ID_173, 233 of 373 at time 2024-06-24 17:30:33.871102\n",
      "Dissolving for Merge_ID_89, 234 of 373 at time 2024-06-24 17:30:53.830924\n",
      "Dissolving for Merge_ID_226, 235 of 373 at time 2024-06-24 17:31:14.376780\n",
      "Dissolving for Merge_ID_47, 236 of 373 at time 2024-06-24 17:32:12.420046\n",
      "Dissolving for Merge_ID_108, 237 of 373 at time 2024-06-24 17:32:31.988001\n",
      "Dissolving for Merge_ID_125, 238 of 373 at time 2024-06-24 17:32:52.106965\n",
      "Dissolving for Merge_ID_233, 239 of 373 at time 2024-06-24 17:33:12.939079\n",
      "Dissolving for Merge_ID_219, 240 of 373 at time 2024-06-24 17:33:52.690600\n",
      "Dissolving for Merge_ID_123, 241 of 373 at time 2024-06-24 17:34:13.333542\n",
      "Dissolving for Merge_ID_299, 242 of 373 at time 2024-06-24 17:34:39.714251\n",
      "Dissolving for Merge_ID_298, 243 of 373 at time 2024-06-24 17:35:23.982870\n",
      "Dissolving for Merge_ID_23, 244 of 373 at time 2024-06-24 17:35:44.021257\n",
      "Dissolving for Merge_ID_222, 245 of 373 at time 2024-06-24 17:36:34.976011\n",
      "Dissolving for Merge_ID_306, 246 of 373 at time 2024-06-24 17:36:56.055352\n",
      "Dissolving for Merge_ID_8, 247 of 373 at time 2024-06-24 17:40:06.791327\n",
      "Dissolving for Merge_ID_284, 248 of 373 at time 2024-06-24 17:40:40.061848\n",
      "Dissolving for Merge_ID_105, 249 of 373 at time 2024-06-24 17:41:12.632230\n",
      "Dissolving for Merge_ID_174, 250 of 373 at time 2024-06-24 17:41:45.869721\n",
      "Dissolving for Merge_ID_369, 251 of 373 at time 2024-06-24 17:43:05.461027\n",
      "Dissolving for Merge_ID_30, 252 of 373 at time 2024-06-24 17:45:15.010862\n",
      "Dissolving for Merge_ID_68, 253 of 373 at time 2024-06-24 17:46:09.123493\n",
      "Dissolving for Merge_ID_7, 254 of 373 at time 2024-06-24 17:46:31.550062\n",
      "Dissolving for Merge_ID_160, 255 of 373 at time 2024-06-24 17:46:54.168309\n",
      "Dissolving for Merge_ID_79, 256 of 373 at time 2024-06-24 17:47:28.990240\n",
      "Dissolving for Merge_ID_87, 257 of 373 at time 2024-06-24 17:47:56.372889\n",
      "Dissolving for Merge_ID_243, 258 of 373 at time 2024-06-24 17:48:29.371661\n",
      "Dissolving for Merge_ID_63, 259 of 373 at time 2024-06-24 17:48:56.019519\n",
      "Dissolving for Merge_ID_70, 260 of 373 at time 2024-06-24 17:49:55.759298\n",
      "Dissolving for Merge_ID_81, 261 of 373 at time 2024-06-24 17:50:49.876425\n",
      "Dissolving for Merge_ID_157, 262 of 373 at time 2024-06-24 17:51:16.907211\n",
      "Dissolving for Merge_ID_40, 263 of 373 at time 2024-06-24 17:52:12.007768\n",
      "Dissolving for Merge_ID_176, 264 of 373 at time 2024-06-24 17:53:00.976095\n",
      "Dissolving for Merge_ID_113, 265 of 373 at time 2024-06-24 17:53:44.963946\n",
      "Dissolving for Merge_ID_339, 266 of 373 at time 2024-06-24 17:54:22.667511\n",
      "Dissolving for Merge_ID_171, 267 of 373 at time 2024-06-24 17:54:52.512069\n",
      "Dissolving for Merge_ID_29, 268 of 373 at time 2024-06-24 17:55:16.097691\n",
      "Dissolving for Merge_ID_51, 269 of 373 at time 2024-06-24 17:56:38.902604\n",
      "Dissolving for Merge_ID_362, 270 of 373 at time 2024-06-24 17:57:43.643310\n",
      "Dissolving for Merge_ID_355, 271 of 373 at time 2024-06-24 17:58:22.674085\n",
      "Dissolving for Merge_ID_344, 272 of 373 at time 2024-06-24 17:58:46.161612\n",
      "Dissolving for Merge_ID_254, 273 of 373 at time 2024-06-24 17:59:31.027818\n",
      "Dissolving for Merge_ID_326, 274 of 373 at time 2024-06-24 18:00:09.038657\n",
      "Dissolving for Merge_ID_181, 275 of 373 at time 2024-06-24 18:00:47.171608\n",
      "Dissolving for Merge_ID_18, 276 of 373 at time 2024-06-24 18:01:38.819434\n",
      "Dissolving for Merge_ID_249, 277 of 373 at time 2024-06-24 18:02:20.513645\n",
      "Dissolving for Merge_ID_286, 278 of 373 at time 2024-06-24 18:03:50.170465\n",
      "Dissolving for Merge_ID_206, 279 of 373 at time 2024-06-24 18:05:06.091539\n",
      "Dissolving for Merge_ID_189, 280 of 373 at time 2024-06-24 18:05:28.157760\n",
      "Dissolving for Merge_ID_54, 281 of 373 at time 2024-06-24 18:05:51.052740\n",
      "Dissolving for Merge_ID_88, 282 of 373 at time 2024-06-24 18:06:56.612319\n",
      "Dissolving for Merge_ID_168, 283 of 373 at time 2024-06-24 18:07:19.451299\n",
      "Dissolving for Merge_ID_227, 284 of 373 at time 2024-06-24 18:08:10.591151\n",
      "Dissolving for Merge_ID_204, 285 of 373 at time 2024-06-24 18:08:41.990432\n",
      "Dissolving for Merge_ID_323, 286 of 373 at time 2024-06-24 18:09:12.801481\n",
      "Dissolving for Merge_ID_210, 287 of 373 at time 2024-06-24 18:09:36.154877\n",
      "Dissolving for Merge_ID_280, 288 of 373 at time 2024-06-24 18:10:36.151844\n",
      "Dissolving for Merge_ID_83, 289 of 373 at time 2024-06-24 18:11:57.534011\n",
      "Dissolving for Merge_ID_296, 290 of 373 at time 2024-06-24 18:12:20.950462\n",
      "Dissolving for Merge_ID_356, 291 of 373 at time 2024-06-24 18:12:44.315866\n",
      "Dissolving for Merge_ID_324, 292 of 373 at time 2024-06-24 18:13:13.360097\n",
      "Dissolving for Merge_ID_64, 293 of 373 at time 2024-06-24 18:13:56.968570\n",
      "Dissolving for Merge_ID_9, 294 of 373 at time 2024-06-24 18:14:43.801472\n",
      "Dissolving for Merge_ID_218, 295 of 373 at time 2024-06-24 18:15:20.603622\n",
      "Dissolving for Merge_ID_345, 296 of 373 at time 2024-06-24 18:15:52.093469\n",
      "Dissolving for Merge_ID_52, 297 of 373 at time 2024-06-24 18:16:31.635692\n",
      "Dissolving for Merge_ID_300, 298 of 373 at time 2024-06-24 18:17:18.250892\n",
      "Dissolving for Merge_ID_180, 299 of 373 at time 2024-06-24 18:17:49.451468\n",
      "Dissolving for Merge_ID_221, 300 of 373 at time 2024-06-24 18:18:21.745044\n",
      "Dissolving for Merge_ID_177, 301 of 373 at time 2024-06-24 18:18:45.911177\n",
      "Dissolving for Merge_ID_102, 302 of 373 at time 2024-06-24 18:19:17.406608\n",
      "Dissolving for Merge_ID_56, 303 of 373 at time 2024-06-24 18:20:15.225563\n",
      "Dissolving for Merge_ID_14, 304 of 373 at time 2024-06-24 18:20:57.266066\n",
      "Dissolving for Merge_ID_130, 305 of 373 at time 2024-06-24 18:21:49.502552\n",
      "Dissolving for Merge_ID_62, 306 of 373 at time 2024-06-24 18:23:01.889842\n",
      "Dissolving for Merge_ID_25, 307 of 373 at time 2024-06-24 18:23:31.816715\n",
      "Dissolving for Merge_ID_111, 308 of 373 at time 2024-06-24 18:24:01.686069\n",
      "Dissolving for Merge_ID_273, 309 of 373 at time 2024-06-24 18:24:25.751107\n",
      "Dissolving for Merge_ID_337, 310 of 373 at time 2024-06-24 18:24:57.374066\n",
      "Dissolving for Merge_ID_65, 311 of 373 at time 2024-06-24 18:25:38.296940\n",
      "Dissolving for Merge_ID_295, 312 of 373 at time 2024-06-24 18:26:48.737447\n",
      "Dissolving for Merge_ID_307, 313 of 373 at time 2024-06-24 18:27:18.403070\n",
      "Dissolving for Merge_ID_328, 314 of 373 at time 2024-06-24 18:27:42.000675\n",
      "Dissolving for Merge_ID_352, 315 of 373 at time 2024-06-24 18:28:59.051220\n",
      "Dissolving for Merge_ID_12, 316 of 373 at time 2024-06-24 18:29:23.399192\n",
      "Dissolving for Merge_ID_46, 317 of 373 at time 2024-06-24 18:30:34.081907\n",
      "Dissolving for Merge_ID_373, 318 of 373 at time 2024-06-24 18:30:57.487336\n",
      "Dissolving for Merge_ID_114, 319 of 373 at time 2024-06-24 18:31:47.042224\n",
      "Dissolving for Merge_ID_175, 320 of 373 at time 2024-06-24 18:32:10.643831\n",
      "Dissolving for Merge_ID_90, 321 of 373 at time 2024-06-24 18:32:46.835964\n",
      "Dissolving for Merge_ID_367, 322 of 373 at time 2024-06-24 18:34:01.821625\n",
      "Dissolving for Merge_ID_66, 323 of 373 at time 2024-06-24 18:34:52.806300\n",
      "Dissolving for Merge_ID_128, 324 of 373 at time 2024-06-24 18:35:20.857521\n",
      "Dissolving for Merge_ID_67, 325 of 373 at time 2024-06-24 18:36:00.626929\n",
      "Dissolving for Merge_ID_267, 326 of 373 at time 2024-06-24 18:36:22.032525\n",
      "Dissolving for Merge_ID_363, 327 of 373 at time 2024-06-24 18:36:54.486235\n",
      "Dissolving for Merge_ID_146, 328 of 373 at time 2024-06-24 18:37:16.416840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolving for Merge_ID_270, 329 of 373 at time 2024-06-24 18:38:01.312932\n",
      "Dissolving for Merge_ID_260, 330 of 373 at time 2024-06-24 18:38:41.118878\n",
      "Dissolving for Merge_ID_185, 331 of 373 at time 2024-06-24 18:39:07.493017\n",
      "Dissolving for Merge_ID_5, 332 of 373 at time 2024-06-24 18:39:47.109065\n",
      "Dissolving for Merge_ID_71, 333 of 373 at time 2024-06-24 18:40:18.180503\n",
      "Dissolving for Merge_ID_208, 334 of 373 at time 2024-06-24 18:40:55.472636\n",
      "Dissolving for Merge_ID_151, 335 of 373 at time 2024-06-24 18:41:43.617286\n",
      "Dissolving for Merge_ID_41, 336 of 373 at time 2024-06-24 18:42:26.382424\n",
      "Dissolving for Merge_ID_124, 337 of 373 at time 2024-06-24 18:43:23.725005\n",
      "Dissolving for Merge_ID_129, 338 of 373 at time 2024-06-24 18:43:55.229386\n",
      "Dissolving for Merge_ID_164, 339 of 373 at time 2024-06-24 18:44:27.271710\n",
      "Dissolving for Merge_ID_2, 340 of 373 at time 2024-06-24 18:45:03.046450\n",
      "Dissolving for Merge_ID_224, 341 of 373 at time 2024-06-24 18:45:22.628028\n",
      "Dissolving for Merge_ID_203, 342 of 373 at time 2024-06-24 18:45:57.571006\n",
      "Dissolving for Merge_ID_361, 343 of 373 at time 2024-06-24 18:46:16.911706\n",
      "Dissolving for Merge_ID_10, 344 of 373 at time 2024-06-24 18:46:42.177829\n",
      "Dissolving for Merge_ID_3, 345 of 373 at time 2024-06-24 18:47:33.279160\n",
      "Dissolving for Merge_ID_143, 346 of 373 at time 2024-06-24 18:48:24.613116\n",
      "Dissolving for Merge_ID_183, 347 of 373 at time 2024-06-24 18:49:43.709071\n",
      "Dissolving for Merge_ID_73, 348 of 373 at time 2024-06-24 18:50:49.162965\n",
      "Dissolving for Merge_ID_34, 349 of 373 at time 2024-06-24 18:51:25.514929\n",
      "Dissolving for Merge_ID_17, 350 of 373 at time 2024-06-24 18:52:02.667925\n",
      "Dissolving for Merge_ID_127, 351 of 373 at time 2024-06-24 18:52:23.961405\n",
      "Dissolving for Merge_ID_133, 352 of 373 at time 2024-06-24 18:53:02.108305\n",
      "Dissolving for Merge_ID_42, 353 of 373 at time 2024-06-24 18:54:47.001643\n",
      "Dissolving for Merge_ID_255, 354 of 373 at time 2024-06-24 18:56:05.162927\n",
      "Dissolving for Merge_ID_106, 355 of 373 at time 2024-06-24 18:56:55.973411\n",
      "Dissolving for Merge_ID_147, 356 of 373 at time 2024-06-24 18:57:22.779933\n",
      "Dissolving for Merge_ID_118, 357 of 373 at time 2024-06-24 18:58:00.210873\n",
      "Dissolving for Merge_ID_142, 358 of 373 at time 2024-06-24 19:01:45.501252\n",
      "Dissolving for Merge_ID_200, 359 of 373 at time 2024-06-24 19:02:51.314450\n",
      "Dissolving for Merge_ID_236, 360 of 373 at time 2024-06-24 19:05:07.645702\n",
      "Dissolving for Merge_ID_148, 361 of 373 at time 2024-06-24 19:06:02.234435\n",
      "Dissolving for Merge_ID_281, 362 of 373 at time 2024-06-24 19:07:04.776141\n",
      "Dissolving for Merge_ID_103, 363 of 373 at time 2024-06-24 19:08:24.789129\n",
      "Dissolving for Merge_ID_27, 364 of 373 at time 2024-06-24 19:09:09.215760\n",
      "Dissolving for Merge_ID_358, 365 of 373 at time 2024-06-24 19:09:40.977564\n",
      "Dissolving for Merge_ID_234, 366 of 373 at time 2024-06-24 19:10:05.886344\n",
      "Dissolving for Merge_ID_116, 367 of 373 at time 2024-06-24 19:10:38.597261\n",
      "Dissolving for Merge_ID_22, 368 of 373 at time 2024-06-24 19:11:04.335800\n",
      "Dissolving for Merge_ID_32, 369 of 373 at time 2024-06-24 19:11:43.466595\n",
      "Dissolving for Merge_ID_94, 370 of 373 at time 2024-06-24 19:12:15.482874\n",
      "Dissolving for Merge_ID_343, 371 of 373 at time 2024-06-24 19:13:04.009285\n",
      "Dissolving for Merge_ID_335, 372 of 373 at time 2024-06-24 19:13:37.877263\n",
      "Dissolving for Merge_ID_201, 373 of 373 at time 2024-06-24 19:14:10.678259\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 11\n",
    "\n",
    "#Run dissolve (also referred to as merge).\n",
    "\n",
    "try:\n",
    "    if run_dissolve:\n",
    "        out_path = global_output_folder + '\\\\' + gdb_name\n",
    "        arcpy.env.workspace = out_path  \n",
    "        arcpy.env.addOutputsToMap = False\n",
    "        if run_dissolve:\n",
    "            if arcpy.Exists(gdb_name_dissolve):\n",
    "                arcpy.management.Delete(gdb_name_dissolve)\n",
    "            arcpy.management.CreateFileGDB(global_output_folder, gdb_name_dissolve)\n",
    "            dissolve_path = global_output_folder + '\\\\' + gdb_name_dissolve\n",
    "            arcpy.conversion.FeatureClassToFeatureClass('msm_Catchment', dissolve_path, 'Node_Catchment')\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Drains_To\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"TEXT\")\n",
    "            arcpy.management.AddField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"TEXT\")\n",
    "            arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID\", \"!muid!\", \"PYTHON3\")\n",
    "            for index, row in merge_df.iterrows():\n",
    "                arcpy.management.CalculateField(dissolve_path + '\\\\Node_Catchment', \"Merge_ID_Temp\", \"''\", \"PYTHON3\")\n",
    "                nodes = list(rank_df[rank_df.Merge_ID==row[\"Merge_ID\"]].index)\n",
    "                print(f'Dissolving for {row[\"Merge_ID\"]}, {index} of {max(merge_df.index)} at time {datetime.datetime.now()}')\n",
    "                if row['Catchment_Count'] == 1:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    where_clause = f\"muid = '{row['To_Dissolve'][0]}'\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.conversion.FeatureClassToFeatureClass('temp_layer', dissolve_path, 'Dissolve_Temp')\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    \n",
    "                else:\n",
    "                    arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "                    catchments = row['To_Dissolve']\n",
    "                    catchments_sql = ', '.join([f\"'{muid}'\" for muid in catchments])\n",
    "                    where_clause = f\"Merge_ID in ({catchments_sql})\"\n",
    "                    arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "                    arcpy.management.CalculateField(\"temp_layer\", \"Merge_ID_Temp\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Dissolve(\"temp_layer\",dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID_Temp\", \"\", \"MULTI_PART\")\n",
    "                    arcpy.management.Delete(\"temp_layer\")\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Merge_ID\", f\"'{row['Merge_ID']}'\", \"PYTHON3\")\n",
    "\n",
    "                for node in nodes:\n",
    "                    arcpy.management.CalculateField(dissolve_path + '\\\\Dissolve_Temp', \"Drains_To\", f\"'{node}'\", \"PYTHON3\")\n",
    "                    arcpy.management.Append(dissolve_path + '\\\\Dissolve_Temp', dissolve_path + '\\\\Node_Catchment', \"NO_TEST\")\n",
    "\n",
    "                arcpy.management.Delete(dissolve_path + '\\\\Dissolve_Temp')\n",
    "\n",
    "\n",
    "\n",
    "        #Delete the features without a Drains_To\n",
    "        arcpy.management.MakeFeatureLayer(dissolve_path + '\\\\Node_Catchment', \"temp_layer\")\n",
    "        where_clause = f\"Drains_To IS NULL\"\n",
    "        arcpy.management.SelectLayerByAttribute(\"temp_layer\", \"NEW_SELECTION\", where_clause)\n",
    "        arcpy.management.DeleteFeatures(\"temp_layer\")  \n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "\n",
    "        #Append the features into the official Node_Catchment layer\n",
    "        arcpy.management.MakeFeatureLayer(\"Node_Catchment\", \"Temp_Layer\")\n",
    "        arcpy.management.DeleteFeatures(\"Temp_Layer\")\n",
    "        arcpy.management.Append(dissolve_path + '\\\\Node_Catchment', \"Temp_Layer\", \"NO_TEST\")\n",
    "        arcpy.management.Delete(\"temp_layer\")\n",
    "        \n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 11', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete.\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 12\n",
    "\n",
    "#Export jpgs. \n",
    "#This process was to heavy to run inside this notebook, causing failed exports and freeze of the program.\n",
    "#It was therefore moved to an external script, called from here by writing a batch file and executing it.\n",
    "\n",
    "try:\n",
    "       \n",
    "    aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "    project_path = aprx.filePath\n",
    "    project_folder = os.path.dirname(project_path)\n",
    "\n",
    "    jpg_folder = model_output_folder + r'\\jpg'\n",
    "    if not os.path.isdir(jpg_folder): os.makedirs(jpg_folder) \n",
    "\n",
    "    jpg_script = project_folder + '\\\\JPG_Subprocess.py'\n",
    "    bat_file_path = project_folder + '\\\\Execute_JPG.bat'\n",
    "    bat_file = open(bat_file_path, \"w\")\n",
    "    python_installation = sys.executable\n",
    "    python_installation = os.path.dirname(sys.executable) + r'\\Python\\envs\\arcgispro-py3\\python.exe'\n",
    "\n",
    "    bat_file_text = '@echo off\\n'\n",
    "    bat_file_text += 'set PYTHON_PATH=\"' + python_installation + '\"\\n'\n",
    "    bat_file_text += 'set SCRIPT_PATH=\"JPG_Subprocess.py\"\\n'\n",
    "    bat_file_text += 'set ARG1=\"' + project_path + '\"\\n'\n",
    "    bat_file_text += 'set ARG2=\"' + jpg_folder + '\"\\n'\n",
    "    bat_file_text += '%PYTHON_PATH% %SCRIPT_PATH% %ARG1% %ARG2%\\n'\n",
    "\n",
    "    bat_file.write(bat_file_text)\n",
    "    bat_file.close()\n",
    "    result = subprocess.call([bat_file_path]) \n",
    "\n",
    "    if result == 1: #Error\n",
    "        raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "        then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "\n",
    "    print(\"Export complete.\")\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 12', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permanent cell 13\n",
    "\n",
    "#Create spreadsheets\n",
    "\n",
    "try:\n",
    "    hex_blue = \"ADD8E6\"\n",
    "    hex_yellow = \"FFFACD\"\n",
    "    border_style = Side(style='thin', color='000000')\n",
    "    border = Border(top=border_style, bottom=border_style, left=border_style, right=border_style)\n",
    "    border_style_none = Side(style=None, color='000000')\n",
    "    border_none = Border(top=border_style_none, bottom=border_style_none, left=border_style_none, right=border_style_none)\n",
    "    \n",
    "    username = os.getlogin()\n",
    "    date_created = str(datetime.datetime.today()).split(' ')[0]\n",
    "    \n",
    "    excel_folder = model_output_folder + '\\\\Excel'\n",
    "    img_folder = model_output_folder + '\\\\jpg'\n",
    "    backup_folder = f'{excel_folder}\\\\Backup_{date_created}_{model}_V{model_version}_{pop_sheet}'\n",
    "    \n",
    "    if not os.path.isdir(excel_folder): os.makedirs(excel_folder) \n",
    "    if not os.path.isdir(img_folder): os.makedirs(img_folder) \n",
    "    if not os.path.isdir(backup_folder): os.makedirs(backup_folder) \n",
    "    for id in node_df[('GENERAL INFO','ID')].unique():    \n",
    "        node_single_df = node_df[node_df[('GENERAL INFO','ID')]==id].copy()\n",
    "        node_single_df.reset_index(drop=True,inplace=True)\n",
    "       \n",
    "        if use_formula: #Replace spreadsheet values with formulas\n",
    "            value_startrow = 17\n",
    "            for i in node_single_df.index:\n",
    "                currentrow = i + value_startrow -1\n",
    "                for avg_calc_dict_key in avg_calc_dict:\n",
    "#                     currentrow = i + value_startrow\n",
    "                    inputs = avg_calc_dict[avg_calc_dict_key]\n",
    "                    header = inputs[0]\n",
    "                    subheader = inputs[2]\n",
    "                    unitflow_ref = inputs[3]\n",
    "                    col_ref = inputs[4]\n",
    "                    formula = f'{r\"=\"}{unitflow_ref}*{col_ref}{currentrow}/86400'\n",
    "                    node_single_df.loc[i,(header,subheader)] = formula                \n",
    "                \n",
    "                node_single_df.loc[i,('RESIDENTIAL','PEAK FLOW (L/s)')] = f'{r\"=\"}MAX(1.5,(1+(14/(4+((H{currentrow}/1000)^0.5)))))*I{currentrow}'\n",
    "                node_single_df.loc[i,('COMMERCIAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"com\"][3]}*K{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5))))*0.8)*L{currentrow}'\n",
    "                \n",
    "                ind_formula = f'{r\"=\"}MAX(1.5,0.8*(1+((14)/(4+(((N{currentrow}*{avg_calc_dict[\"ind\"][3]}/{avg_calc_dict[\"res\"][3]}'\n",
    "                ind_formula += f')/1000)^0.5))))*IF(N{currentrow}<121,1.7,(2.505-0.1673*LN(N{currentrow}))))*O{currentrow}'\n",
    "                node_single_df.loc[i,('INDUSTRIAL','PEAK FLOW (L/s)')] = ind_formula\n",
    "                \n",
    "                node_single_df.loc[i,('INSTITUTIONAL','PEAK FLOW (L/s)')] = \\\n",
    "                    f'{r\"=\"}MAX(1.5,(1+(14/(4+((({avg_calc_dict[\"inst\"][3]}*Q{currentrow})/({avg_calc_dict[\"res\"][3]}*1000))^0.5)))))*R{currentrow}'\n",
    "\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','AREA (Ha)')] = f'{r\"=\"}G{currentrow}+K{currentrow}+N{currentrow}+Q{currentrow}'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFLOW (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infl\"][3]}/86400'\n",
    "                node_single_df.loc[i,('INFLOW / INFILTRATION','INFILTRATION (L/s)')] = f'{r\"=\"}T{currentrow}*{avg_calc_dict[\"infi\"][3]}/86400'\n",
    "                \n",
    "                node_single_df.loc[i,('FLOWS','AVG. SAN. FLOW (L/s)')] = f'{r\"=\"}I{currentrow}+L{currentrow}+O{currentrow}+R{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','ADWF (L/s)')] = f'{r\"=\"}W{currentrow}+V{currentrow}'\n",
    "                node_single_df.loc[i,('FLOWS','PWWF (L/s)')] = \\\n",
    "                    f'{r\"=\"}J{currentrow}+M{currentrow}+P{currentrow}+S{currentrow}+U{currentrow}+V{currentrow}'\n",
    "                \n",
    "        id = id if not '/' in id else id.replace('/','_')\n",
    "        id = id.replace('\\\\','_')\n",
    "        muid = node_single_df.iloc[0,0]\n",
    "\n",
    "        sheetpath = excel_folder + \"\\\\\" + id + \".xlsx\"\n",
    "        startrow = 13\n",
    "        with pd.ExcelWriter(sheetpath) as writer:\n",
    "            node_single_df.to_excel(writer, sheet_name=id,startrow=startrow)\n",
    "            info_df.to_excel(writer, sheet_name=id,startrow=1,startcol=2)\n",
    "\n",
    "            workbook = writer.book\n",
    "            workbook.create_sheet(\"Map\")\n",
    "\n",
    "        workbook = load_workbook(sheetpath)    \n",
    "        sheet1 = workbook[id]\n",
    "\n",
    "        #Format infobox\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=3,max_col=5):\n",
    "            for cell in col[1:2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[2:7]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "                cell.fill = PatternFill(start_color=hex_yellow, end_color=hex_yellow, fill_type=\"solid\")\n",
    "                cell.border = border\n",
    "        sheet1.column_dimensions['C'].width = 22 \n",
    "        sheet1.column_dimensions['D'].width = 11 \n",
    "\n",
    "        #Remove index\n",
    "        for row in sheet1.iter_rows():\n",
    "            for cell in row[:1]:\n",
    "                cell.value = ''\n",
    "                cell.border = border_none\n",
    "        #Format main table header rows\n",
    "        merged_range = sheet1.merged_cells\n",
    "        for col in sheet1.iter_cols(min_col=2):\n",
    "            for cell in col[startrow+1:startrow+2]:\n",
    "                cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\",wrap_text=True)\n",
    "                cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")\n",
    "            for cell in col[startrow:startrow+1]:\n",
    "                if cell.coordinate in sheet1.merged_cells:\n",
    "                    cell.fill = PatternFill(start_color=hex_blue, end_color=hex_blue, fill_type=\"solid\")        \n",
    "\n",
    "        sheet1.column_dimensions['C'].width = 23 \n",
    "        sheet1.column_dimensions['D'].width = 11   \n",
    "        sheet1.column_dimensions['F'].width = 13\n",
    "        sheet1.column_dimensions['H'].width = 13  \n",
    "        sheet1.column_dimensions['V'].width = 13\n",
    "        \n",
    "        #Delete empty row between header and data\n",
    "        sheet1.delete_rows(16)\n",
    "        \n",
    "        #Find the minimum factor in the sheet. Array formulas do not work with openpyxl, do divisions one by one instead\n",
    "        cols = [['I','J'],['L','M'],['O','P'],['R','S']]\n",
    "        rows = list(range(16,currentrow + 1))      \n",
    "        formula = r'=ROUND(MIN('\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                formula += f'IFERROR({col[1]}{row}/{col[0]}{row},999),'\n",
    "        formula = formula[:-1] + '),2)'        \n",
    "        sheet1['J9'] = formula\n",
    "        sheet1['H9'] = 'Lowest peaking factor:'\n",
    "        sheet1['H10'] = 'Lowest peaking factors at lower limit 1.5 highlighted in yellow.'\n",
    "                      \n",
    "        #Color code cells where peaking facor is at minimum.\n",
    "        format_formula = 'J16/I16<1.501'\n",
    "        format_range = ''\n",
    "        for col in cols:\n",
    "            format_range += f'{col[1]}16:{col[1]}{currentrow} '\n",
    "        format_range = format_range[:-1] #Remove last space.               \n",
    "        fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "        rule = FormulaRule(formula=[format_formula], fill=fill)\n",
    "        sheet1.conditional_formatting.add(format_range, rule)\n",
    "        \n",
    "        decimals = [['#,##0','H'],['#,##0.0','IJLMOPRSUVWXY'],['#,##0.00','GKNQT']]\n",
    "        for decimal in decimals:\n",
    "            for col in decimal[1]:\n",
    "                for row in range(16,currentrow+1):\n",
    "                    sheet1[f'{col}{row}'].number_format = decimal[0]\n",
    "        \n",
    "        display_name = \"Open in Google Maps\"\n",
    "        google_map_string = 'https://maps.google.com/?q='\n",
    "        google_map_string += str(centroids_df.loc[muid,'Y']) + ', '\n",
    "        google_map_string += str(centroids_df.loc[muid,'X']) \n",
    "\n",
    "        # Adding a hyperlink with a display name\n",
    "        cell = sheet1.cell(row=10, column=3, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "                      \n",
    "        sheet = workbook[\"Map\"]\n",
    "        \n",
    "        #Add source info\n",
    "        source_infos = []\n",
    "        source_infos.append(['Created by:',username])\n",
    "        source_infos.append(['Date:',date_created])\n",
    "        source_infos.append(['Model area:',model])\n",
    "        source_infos.append(['Model version:',model_version])\n",
    "        source_infos.append(['Population file:',os.path.basename(pop_book)])\n",
    "        source_infos.append(['Population sheet:',pop_sheet ])\n",
    "        \n",
    "        for i, source_info in enumerate(source_infos):\n",
    "            sheet1[f'H{i+2}']  = source_info[0]\n",
    "            sheet1[f'J{i+2}']  = source_info[1]\n",
    "        \n",
    "        for row in sheet1['H2:H9']:\n",
    "            for cell in row:\n",
    "                cell.font = Font(bold=True)\n",
    "        \n",
    "\n",
    "        # Add an image to the sheet\n",
    "        img_path = img_folder + '\\\\' + muid + '.jpg'  # Replace with the path to your image\n",
    "        img = Image(img_path)\n",
    "\n",
    "        sheet.add_image(img, 'B3')\n",
    "        \n",
    "        cell = sheet.cell(row=1, column=2, value=display_name)\n",
    "        cell.hyperlink = google_map_string\n",
    "        cell.font = Font(color=\"0000FF\", underline=\"single\") \n",
    "\n",
    "        workbook.save(sheetpath)\n",
    "        shutil.copy(sheetpath,backup_folder + \"\\\\\" + id + \".xlsx\")\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 13', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
