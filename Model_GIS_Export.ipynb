{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "#Tool updated May 14 2024\n",
    "##Info\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Model_GIS_Export_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "import shutil\n",
    "from datetime import datetime as dt, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3779f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No FacilityID found for MH1 in ARDR\n",
      "Warning: No FacilityID found for MH2 in ARDR\n",
      "Warning: No FacilityID found for MH3 in ARDR\n",
      "Warning: No FacilityID found for MH27 in HLY\n",
      "Warning: No FacilityID found for MH1 in SEBD\n",
      "Warning: No FacilityID found for MH10 in SEBD\n",
      "Warning: No FacilityID found for MH2 in SEBD\n",
      "Warning: No FacilityID found for MH3 in SEBD\n",
      "Warning: No FacilityID found for MH4 in SEBD\n",
      "Warning: No FacilityID found for MH5 in SEBD\n",
      "Warning: No FacilityID found for MH6 in SEBD\n",
      "Warning: No FacilityID found for MH7 in SEBD\n",
      "Warning: No FacilityID found for MH8 in SEBD\n",
      "Warning: No FacilityID found for MH9 in SEBD\n"
     ]
    }
   ],
   "source": [
    "#Permanent cell 2\n",
    "#Import RAWN\n",
    "\n",
    "columns = ['Key','SewerageArea', 'Acronym', 'MHName', 'FacilityID']\n",
    "for rawn_year in rawn_years:\n",
    "    columns.append('PWWF_' + str(rawn_year))\n",
    "rawn_output_df = pd.DataFrame(columns=columns)\n",
    "rawn_output_df.set_index('Key',inplace=True)\n",
    "\n",
    "if rawn_input_from_model:\n",
    "    #How many to rows to skip for this spreadsheet type\n",
    "    rawn_input_skiprows = 1\n",
    "    #The PWWF column name in this spreadsheet type\n",
    "    rawn_pwwf_column = 'PWWF (L/s)'\n",
    "else:\n",
    "    rawn_input_skiprows = 13\n",
    "    rawn_pwwf_column = 'P.W.W.F.'\n",
    "\n",
    "rawn_list = []\n",
    "\n",
    "#This csv files contains matched facilityID to MUID, where a match was found in tool 1.\n",
    "node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "\n",
    "rawn_list = []\n",
    "if not rawn_input_from_model: #Read csv file\n",
    "    rawn_input_df = pd.read_csv(rawn_csv)\n",
    "else: #create dataframe similar to the rawn csv file by looping through the RAWN Excel file names\n",
    "    for rawn_inputfolder in rawn_inputfolders:\n",
    "        model_area = rawn_inputfolder[0]\n",
    "        folder = rawn_inputfolder[1]\n",
    "        for f in os.listdir(folder):\n",
    "            if f[-5:]=='.xlsx':\n",
    "                filepath = folder + '\\\\' + f\n",
    "                comb_name =  os.path.splitext(f)[0]\n",
    "                acronym = comb_name.split('_')[0]\n",
    "                mh_name = comb_name.split('_')[1]\n",
    "                rawn_list.append([model_area,acronym,mh_name,\"Sheet1\",filepath])\n",
    "    rawn_input_df = pd.DataFrame(rawn_list,columns=['Sewer_Area', 'Acronym', 'MH_Name', 'Tab', 'Sheet_Path'])\n",
    "                                   \n",
    "\n",
    "for index1, row1 in rawn_input_df.iterrows():\n",
    "    if rawn_input_from_model:\n",
    "        #This type has column 'NODE' which i set to string.\n",
    "        rawn_single_df = pd.read_excel(row1['Sheet_Path'],sheet_name=row1['Tab'],skiprows=rawn_input_skiprows,\n",
    "                                       dtype={'NODE': str})\n",
    "        #This type has an empty row to be removed, may improve in later version\n",
    "        rawn_single_df.dropna(how='all', inplace=True)\n",
    "        #Reset the index to start at 0 \n",
    "        rawn_single_df.reset_index(inplace=True,drop=True)\n",
    "    else:\n",
    "        rawn_single_df = pd.read_excel(row1['Sheet_Path'],sheet_name=row1['Tab'],skiprows=rawn_input_skiprows)\n",
    "    sewer_area = row1['Sewer_Area']\n",
    "    acronym = row1['Acronym']\n",
    "    mh_name = row1['MH_Name']\n",
    "    index_val = acronym + '_' + mh_name\n",
    "    if rawn_input_from_model:\n",
    "        #The muid are in all rows, the below just takes the one from the first row. \n",
    "        muid = rawn_single_df.loc[0,'NODE']\n",
    "        node_match_df = node_df.loc[(node_df.muid==muid) & (node_df.sewer_area)]\n",
    "    else:\n",
    "        node_match_df = node_df.loc[(node_df.acronym==acronym) & (node_df.mhname==mh_name) & (node_df.sewer_area)]\n",
    "    node_match_df.reset_index(inplace=True)\n",
    "    if len(node_match_df) == 0:\n",
    "        print('Warning: No FacilityID found for ' + mh_name + ' in ' + acronym)\n",
    "        facilityid = 'Not Found'\n",
    "    else:\n",
    "        facilityid = node_match_df.loc[0,'facilityid']\n",
    "        \n",
    "    rawn_output_df.loc[index_val,['SewerageArea','Acronym','MHName','FacilityID']]=[sewer_area,acronym,mh_name,facilityid]\n",
    "    \n",
    "    \n",
    "    #For traditional rawn sheets, the first 4 columns have headers one higher up than the following columns \n",
    "    #(which are under merged cells) so this adjustment below are only for traditional RAWN sheets.\n",
    "    if not rawn_input_from_model:\n",
    "        #In this version, the column names are in different rows, this is corrected below.\n",
    "        for i in range(4):\n",
    "            col_name = rawn_single_df.columns[i]\n",
    "            #Transfer the column name to the first row to be consistent with other columns\n",
    "            rawn_single_df.loc[0,col_name]=col_name\n",
    "        for col in rawn_single_df.columns:\n",
    "            #Set the column names to equal the value in the first row\n",
    "            rawn_single_df.rename(columns={col:rawn_single_df.loc[0,col]},inplace=True)\n",
    "        #Drop the first row which previously had the column names.\n",
    "        rawn_single_df.drop([0,1],inplace=True)\n",
    "    for index2, row2 in rawn_single_df.iterrows():\n",
    "        year = int(row2['YEAR'])\n",
    "        pwwf = row2[rawn_pwwf_column]\n",
    "        if year in rawn_years:\n",
    "            #Input in columns named after the year. This will create the column when it does not exist, otherwise just fill.\n",
    "            rawn_output_df.loc[index_val,'PWWF_' + str(year)] = pwwf\n",
    "            \n",
    "if len(acronym_filter) > 0:\n",
    "    #If only some acronyms are to be created\n",
    "    rawn_output_df = rawn_output_df[rawn_output_df.Acronym.isin(acronym_filter)]\n",
    "\n",
    "#Create the output csv\n",
    "rawn_output_df.to_csv(output_folder + '\\\\RAWN_Nodes.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5691a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process FSA_DWF_2021-07-22_4d_2025pop_BaseDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2025p_Base-DSS1Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2025p_Base-DSS2Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2025p_Base-DSS3Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2025p_Base-DSS16Default_Network_HD.res1d\n",
      "process FSA_DWF_2021-07-22_4d_2030pop_2030_NetworkDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2030p_F_2030_Network-DSS4Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2030p_F_2030_Network-DSS5Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2030p_F_2030_Network-DSS6Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2030p_F_2030_Network-DSS17Default_Network_HD.res1d\n",
      "process NSSA_DWF_2018-07-26_4d_2025pop_BaseDefault_Network_HD.res1d\n",
      "process NSSA_GA_EX-2y-24h-AES_2025p_Base-DSS1Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-5y-24h-AES_2025p_Base-DSS2Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-10y-24h-AES_2025p_Base-DSS3Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-25y-24h-AES_2025p_Base-DSS10Default_Network_HD.res1d\n",
      "process NSSA_DWF_2018-07-26_4d_2030pop_BaseDefault_Network_HD.res1d\n",
      "process NSSA_GA_EX-2y-24h-AES_2030p_Base-DSS1Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-5y-24h-AES_2030p_Base-DSS2Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-10y-24h-AES_2030p_Base-DSS3Default_Network_HD.res1d\n",
      "process NSSA_GA_EX-25y-24h-AES_2030p_Base-DSS10Default_Network_HD.res1d\n",
      "process LISA_DWF_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-2yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-5yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-10yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-25yr-24hr-SCS_2025pop_2025Default_Network_HD.res1d\n",
      "process LISA_DWF_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-2yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-5yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-10yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "process LISA_WWF_EX-25yr-24hr-SCS_2030pop_2030Default_Network_HD.res1d\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Import model results\n",
    "for m_index, m in enumerate(master_list):\n",
    "  \n",
    "    model_area = m[0]\n",
    "    model = m[1]\n",
    "    model_folder = m[2]\n",
    "    output_folder = m[3]\n",
    "    result_list = m[4]\n",
    "    \n",
    "    db_type = os.path.splitext(model)[1][1:]\n",
    "    \n",
    "    node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "    node_df.rename(columns={'sewer_area':'SewerageArea','facilityid':'FacilityID','muid':'ModelID','match_code':'Match_Code','acronym':'Acronym','mhname':'MHName'},inplace=True)\n",
    "    node_df = node_df[node_df.SewerageArea==model_area]\n",
    "    \n",
    "    pipe_df = pd.read_csv(model_pipe_csv, dtype={'facilityid': str,'muid': str})\n",
    "    pipe_df.rename(columns={'sewer_area':'SewerageArea','facilityid':'FacilityID','muid':'ModelID','match_code':'Match_Code','acronym':'Acronym'},inplace=True)\n",
    "    pipe_df = pipe_df[['SewerageArea','FacilityID','Acronym','ModelID']]\n",
    "    pipe_df = pipe_df[pipe_df.SewerageArea==model_area]\n",
    "\n",
    "    for r in result_list:\n",
    "        description = r[0]\n",
    "        pop_year = r[1]\n",
    "        result_file = r[2]\n",
    "        #If mdb (VSA), the results are in the same folder as the model\n",
    "        if db_type.lower() == 'mdb':\n",
    "            result_path = model_folder + '\\\\' + result_file\n",
    "            if not os.path.exists(result_path):\n",
    "                #This will end the script execution with an error.\n",
    "                raise ValueError(\"The following result file was not found in the model folder: \" + result_file)     \n",
    "        #If sqlite (MIKE+: FSA, NSSA, LISA), then the results are found in sub folders auto-named after the model.\n",
    "        elif db_type.lower() == 'sqlite':\n",
    "            file_found = False\n",
    "            for f1 in os.listdir(model_folder):\n",
    "                if f1[-7:] == '.sqlite':\n",
    "                    #browse subfolder\n",
    "                    result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                    for f2 in os.listdir(model_folder + '\\\\' + result_subfolder):\n",
    "                        if os.path.basename(f2) == result_file:\n",
    "                            result_path = model_folder + '\\\\' + result_subfolder + '\\\\' + f2\n",
    "                            file_found = True\n",
    "            if not file_found:\n",
    "                #This will end the script execution with an error.\n",
    "                raise ValueError(\"The following result file was not found: \" + result_file)\n",
    "        else:\n",
    "            #This will end the script execution with an error.\n",
    "            raise ValueError(\"The variable 'db_type' must be 'mdb' or 'sqlite'.\")\n",
    " \n",
    "        res1d = Res1D(result_path)\n",
    "        print('process ' + result_file)\n",
    "        sim_start = res1d.time_index.min()\n",
    "        start = sim_start + timedelta(days=1)\n",
    "        end = res1d.time_index.max()\n",
    "        sim_seconds = (end - sim_start).total_seconds()\n",
    "        #Number of timesteps\n",
    "        timesteps = len(res1d.time_index)-1\n",
    "        #Number of seconds in one timestep\n",
    "        timestep_seconds = sim_seconds / timesteps\n",
    "        #Number of timesteps in one day (Used to crop first day and for ADWF only include last day)\n",
    "        one_day_steps = int(86400 / timestep_seconds)\n",
    "        \n",
    "        #@@@@@@@@@@@@@@@@@@@@@HGL\n",
    "        if description.lower() == 'dwf':\n",
    "            #Build column names and prefill with empty (nan) values.\n",
    "            hgl_name = 'PDWF_' + str(pop_year) + '_HGL'\n",
    "            hgl_avg_name = 'ADWF_' + str(pop_year) + '_HGL'\n",
    "            node_df[hgl_avg_name] = np.nan\n",
    "            hgl_min_name = 'MDWF_' + str(pop_year) + '_HGL'\n",
    "            node_df[hgl_min_name] = np.nan\n",
    "        else:\n",
    "            hgl_name = description + '_' + str(pop_year) + '_HGL'\n",
    "        node_df[hgl_name] = np.nan\n",
    "        nodes = [node.Id for node in res1d.data.Nodes]\n",
    "        for index, row in node_df.iterrows():\n",
    "            muid = row['ModelID']\n",
    "            if muid in nodes:\n",
    "                #List hgl values with first day removed.\n",
    "                hgl = max(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                node_df.loc[index,hgl_name] = hgl\n",
    "                if description.lower() == 'dwf':\n",
    "                    #For average, use only last day to calculate average\n",
    "                    hgl_avg = sum(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[-one_day_steps:])/one_day_steps\n",
    "                    node_df.loc[index,hgl_avg_name] = hgl_avg\n",
    "                    hgl_min = min(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                    node_df.loc[index,hgl_min_name] = hgl_min\n",
    "                                    \n",
    "                \n",
    "        #@@@@@@@@@@@@@@@@@@@@@ Q and V\n",
    "        #The pipe muids are given a suffix (hyphen and incrementing counter) in mikeio1d which is removed here\n",
    "        pipes = [pipe.Id[:pipe.Id.rfind('-')] for pipe in res1d.data.Reaches]\n",
    "        if description.lower() == 'dwf':\n",
    "            #Build column names and prefill with empty (nan) values.\n",
    "            q_name = 'PDWF_' + str(pop_year) + '_Q'\n",
    "            q_avg_name = 'ADWF_' + str(pop_year) + '_Q'\n",
    "            pipe_df[q_avg_name] = np.nan           \n",
    "            q_min_name = 'MDWF_' + str(pop_year) + '_Q'\n",
    "            pipe_df[q_min_name] = np.nan\n",
    "            \n",
    "            v_name = 'PDWF_' + str(pop_year) + '_V'\n",
    "            v_avg_name = 'ADWF_' + str(pop_year) + '_V'\n",
    "            pipe_df[v_avg_name] = np.nan\n",
    "            v_min_name = 'MDWF_' + str(pop_year) + '_V'\n",
    "            pipe_df[v_min_name] = np.nan\n",
    "            \n",
    "        else:\n",
    "            q_name = description + '_' + str(pop_year) + '_Q'\n",
    "            v_name = description + '_' + str(pop_year) + '_V'\n",
    "        pipe_df[q_name] = np.nan\n",
    "        pipe_df[v_name] = np.nan\n",
    "        for index, row in pipe_df.iterrows():\n",
    "            muid = row['ModelID']\n",
    "            if muid in pipes:\n",
    "                #List of flow values with first day cropped/\n",
    "                q_list = list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[one_day_steps:]\n",
    "                #Convert to L/s\n",
    "                q_list = [q*1000 for q in q_list]\n",
    "                v_list = list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[one_day_steps:]\n",
    "                \n",
    "                if absolute_velocity_discharge:\n",
    "                    #Get the absolute maxes and mins (every value absolute)\n",
    "                    q = max([abs(q) for q in q_list])\n",
    "                    v = max([abs(v) for v in v_list])\n",
    "                    q_min = min([abs(q) for q in q_list])\n",
    "                    v_min = min([abs(v) for v in v_list])\n",
    "                    \n",
    "                else:\n",
    "                    q = max(q_list)\n",
    "                    v = max(v_list)\n",
    "                    q_min = min(q_list)\n",
    "                    v_min = min(v_list)\n",
    "                    \n",
    "                pipe_df.loc[index,q_name] = q\n",
    "                pipe_df.loc[index,v_name] = v\n",
    "                \n",
    "                if description.lower() == 'dwf':\n",
    "                    pipe_df.loc[index,q_min_name] = q_min\n",
    "                    pipe_df.loc[index,v_min_name] = v_min\n",
    "                    \n",
    "                    q_avg = sum(list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[-one_day_steps:])/one_day_steps*1000\n",
    "                    if absolute_velocity_discharge:\n",
    "                        #Only take the absolute of the final average, not every timestep, otherwise it will be skewed.\n",
    "                        q_avg = abs(q_avg)\n",
    "                    pipe_df.loc[index,q_avg_name] = q_avg\n",
    "                    \n",
    "                    v_avg = sum(list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[-one_day_steps:])/one_day_steps\n",
    "                    if absolute_velocity_discharge:\n",
    "                        #Only take the absolute of the final average, not every timestep, otherwise it will be skewed.\n",
    "                        v_avg = abs(v_avg)\n",
    "                    pipe_df.loc[index,v_avg_name] = v_avg\n",
    "                    \n",
    "\n",
    "    if m_index == 0:\n",
    "        #Create the table with all values\n",
    "        node_df_all = node_df.copy()\n",
    "        pipe_df_all = pipe_df.copy()\n",
    "    else:\n",
    "        #Append to the table with all value (alread created at m_index == 0)\n",
    "        node_df_all = pd.concat([node_df_all,node_df])\n",
    "        pipe_df_all = pd.concat([pipe_df_all,pipe_df])\n",
    "    \n",
    "if len(acronym_filter) > 0:\n",
    "    #If only some acronyms are to be created\n",
    "    node_df_all = node_df_all[node_df_all.Acronym.isin(acronym_filter)]\n",
    "    pipe_df_all = pipe_df_all[pipe_df_all.Acronym.isin(acronym_filter)]\n",
    "    \n",
    "node_df_all.drop(columns=['Match_Code'],inplace=True)\n",
    "\n",
    "#Replace column name hyphens with underscores as requested by GIS\n",
    "node_df_all.rename(columns=lambda x: x.replace('-', '_'), inplace=True)\n",
    "pipe_df_all.rename(columns=lambda x: x.replace('-', '_'), inplace=True)\n",
    "\n",
    "#Create the csv files\n",
    "node_df_all.to_csv(output_folder + '\\\\Model_Nodes.csv', index=False)   \n",
    "pipe_df_all.to_csv(output_folder + '\\\\Model_Pipes.csv', index=False) \n",
    "print('Done')                \n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
