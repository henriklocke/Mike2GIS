{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfa4f1",
   "metadata": {},
   "source": [
    "#Tool updated March 11 2024\n",
    "##Info\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "Workflow:\n",
    "    1. Sum:\n",
    "        WW\n",
    "        GWI\n",
    "        I/I\n",
    "        Runoff\n",
    "        dfs0 inflow\n",
    "    2. Sum:\n",
    "        WWTP flow\n",
    "        MH Spilling\n",
    "        Outfalls\n",
    "        Delta volume\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb6340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ctypes\n",
    "import traceback\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "from Model_GIS_Export_Variables import *\n",
    "import subprocess\n",
    "import sqlite3\n",
    "import shutil\n",
    "from datetime import datetime as dt, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3779f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Key','Sewer_Area', 'Acronym', 'MHName', 'FacilityID']\n",
    "for rawn_year in rawn_years:\n",
    "    columns.append('PWWF_' + str(rawn_year))\n",
    "rawn_output_df = pd.DataFrame(columns=columns)\n",
    "rawn_output_df.set_index('Key',inplace=True)\n",
    "\n",
    "rawn_list = []\n",
    "#Import RAWN\n",
    "node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "rawn_input_df = pd.read_csv(rawn_csv)\n",
    "for index1, row1 in rawn_input_df.iterrows():\n",
    "    sewer_area = row1['Sewer_Area']\n",
    "    acronym = row1['Acronym']\n",
    "    mh_name = row1['MH_Name']\n",
    "    index_val = acronym + '_' + mh_name\n",
    "    node_match_df = node_df.loc[(node_df.acronym==acronym) & (node_df.mhname==mh_name) & (node_df.sewer_area)]\n",
    "    node_match_df.reset_index(inplace=True)\n",
    "    if len(node_match_df) == 0:\n",
    "        print('Warning: No FacilityID found for ' + mh_name + ' in ' + acronym)\n",
    "        facilityid = 'Not Found'\n",
    "    else:\n",
    "        facilityid = node_match_df.loc[0,'facilityid']\n",
    "        \n",
    "    rawn_output_df.loc[index_val,['Sewer_Area','Acronym','MHName','FacilityID']]=[sewer_area,acronym,mh_name,facilityid]\n",
    "    rawn_single_df = pd.read_excel(row1['Sheet_Path'],sheet_name=row1['Tab'],skiprows=13)\n",
    "    for i in range(4):\n",
    "        col_name = rawn_single_df.columns[i]\n",
    "        rawn_single_df.loc[0,col_name]=col_name\n",
    "    for col in rawn_single_df.columns:\n",
    "        rawn_single_df.rename(columns={col:rawn_single_df.loc[0,col]},inplace=True)\n",
    "    rawn_single_df.drop([0,1],inplace=True)\n",
    "    for index2, row2 in rawn_single_df.iterrows():\n",
    "        year = int(row2['YEAR'])\n",
    "        pwwf = row2['P.W.W.F.']\n",
    "#         pwwf = round(pwwf,q_decimals)\n",
    "        if year in rawn_years:\n",
    "            rawn_output_df.loc[index_val,'PWWF_' + str(year)] = pwwf\n",
    "            \n",
    "if len(acronym_filter) > 0:\n",
    "    rawn_output_df = rawn_output_df[rawn_output_df.Acronym.isin(acronym_filter)]\n",
    "    \n",
    "rawn_output_df.to_csv(output_folder + '\\\\RAWN_Nodes.csv', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5691a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process FSA_DWF_2021-07-22_4d_2025pop_BaseDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2025p_Base-DSS1Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2025p_Base-DSS2Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2025p_Base-DSS3Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2025p_Base-DSS16Default_Network_HD.res1d\n",
      "process FSA_DWF_2021-07-22_4d_2030pop_2030_NetworkDefault_Network_HD.res1d\n",
      "process FSA_GA_EX-2y-24h-AES_2030p_F_2030_Network-DSS4Default_Network_HD.res1d\n",
      "process FSA_GA_EX-5y-24h-AES_2030p_F_2030_Network-DSS5Default_Network_HD.res1d\n",
      "process FSA_GA_EX-10y-24h-AES_2030p_F_2030_Network-DSS6Default_Network_HD.res1d\n",
      "process FSA_GA_EX-25y-24h-AES_2030p_F_2030_Network-DSS17Default_Network_HD.res1d\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Import model results\n",
    "for m_index, m in enumerate(master_list):\n",
    "  \n",
    "    model_area = m[0]\n",
    "    db_type = m[1]\n",
    "    model_folder = m[2]\n",
    "    output_folder = m[3]\n",
    "    result_list = m[4]\n",
    "    \n",
    "    node_df = pd.read_csv(model_manhole_csv, dtype={'facilityid': str,'muid': str})\n",
    "    node_df.rename(columns={'sewer_area':'Sewer_Area','facilityid':'FacilityID','muid':'MUID','match_code':'Match_Code','acronym':'Acronym','mhname':'MHName'},inplace=True)\n",
    "    node_df = node_df[node_df.Sewer_Area==model_area]\n",
    "    \n",
    "    pipe_df = pd.read_csv(model_pipe_csv, dtype={'facilityid': str,'muid': str})\n",
    "    pipe_df.rename(columns={'sewer_area':'Sewer_Area','facilityid':'FacilityID','muid':'MUID','match_code':'Match_Code','acronym':'Acronym'},inplace=True)\n",
    "    pipe_df = pipe_df[['Sewer_Area','FacilityID','Acronym','MUID']]\n",
    "    pipe_df = pipe_df[pipe_df.Sewer_Area==model_area]\n",
    "\n",
    "    for r in result_list:\n",
    "        description = r[0]\n",
    "        pop_year = r[1]\n",
    "        result_file = r[2]\n",
    "        if db_type.lower() == 'mdb':\n",
    "            result_path = model_folder + '\\\\' + result_file\n",
    "            if not os.path.exists(result_path):\n",
    "                raise ValueError(\"The following result file was not found in the model folder: \" + result_file)     \n",
    "        elif db_type.lower() == 'sqlite':\n",
    "            file_found = False\n",
    "            for f1 in os.listdir(model_folder):\n",
    "                if f1[-7:] == '.sqlite':\n",
    "                    #browse subfolder\n",
    "                    result_subfolder = os.path.basename(f1)[:-7] + '_m1d - Result Files'\n",
    "                    for f2 in os.listdir(model_folder + '\\\\' + result_subfolder):\n",
    "                        if os.path.basename(f2) == result_file:\n",
    "                            result_path = model_folder + '\\\\' + result_subfolder + '\\\\' + f2\n",
    "                            file_found = True\n",
    "            if not file_found:\n",
    "                raise ValueError(\"The following result file was not found: \" + result_file)\n",
    "        else:\n",
    "            raise ValueError(\"The variable 'db_type' must be 'mdb' or 'sqlite'.\")\n",
    " \n",
    "        res1d = Res1D(result_path)\n",
    "        print('process ' + result_file)\n",
    "        sim_start = res1d.time_index.min()\n",
    "        start = sim_start + timedelta(days=1)\n",
    "        end = res1d.time_index.max()\n",
    "        sim_seconds = (end - sim_start).total_seconds()\n",
    "        timesteps = len(res1d.time_index)-1\n",
    "        timestep_seconds = sim_seconds / timesteps\n",
    "        one_day_steps = int(86400 / timestep_seconds)\n",
    "        \n",
    "        #@@@@@@@@@@@@@@@@@@@@@HGL\n",
    "        if description.lower() == 'dwf':\n",
    "            hgl_name = 'PDWF_' + str(pop_year) + '_HGL'\n",
    "            hgl_avg_name = 'ADWF_' + str(pop_year) + '_HGL'\n",
    "            node_df[hgl_avg_name] = np.nan\n",
    "            hgl_min_name = 'MDWF_' + str(pop_year) + '_HGL'\n",
    "            node_df[hgl_min_name] = np.nan\n",
    "        else:\n",
    "            hgl_name = description + '_' + str(pop_year) + '_HGL'\n",
    "        node_df[hgl_name] = np.nan\n",
    "        nodes = [node.Id for node in res1d.data.Nodes]\n",
    "        for index, row in node_df.iterrows():\n",
    "            muid = row['MUID']\n",
    "            if muid in nodes:\n",
    "                hgl = max(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                node_df.loc[index,hgl_name] = hgl\n",
    "                if description.lower() == 'dwf':\n",
    "                    hgl_avg = sum(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[-one_day_steps:])/one_day_steps\n",
    "                    node_df.loc[index,hgl_avg_name] = hgl_avg\n",
    "                    hgl_min = min(list(res1d.query.GetNodeValues(muid, \"WaterLevel\"))[one_day_steps:])\n",
    "                    node_df.loc[index,hgl_min_name] = hgl_min\n",
    "                                    \n",
    "                \n",
    "        #@@@@@@@@@@@@@@@@@@@@@ Q and V\n",
    "        pipes = [pipe.Id[:pipe.Id.rfind('-')] for pipe in res1d.data.Reaches]\n",
    "        if description.lower() == 'dwf':\n",
    "            q_name = 'PDWF_' + str(pop_year) + '_Q'\n",
    "            q_avg_name = 'ADWF_' + str(pop_year) + '_Q'\n",
    "            pipe_df[q_avg_name] = np.nan           \n",
    "            q_min_name = 'MDWF_' + str(pop_year) + '_Q'\n",
    "            pipe_df[q_min_name] = np.nan\n",
    "            \n",
    "            v_name = 'PDWF_' + str(pop_year) + '_V'\n",
    "            v_avg_name = 'ADWF_' + str(pop_year) + '_V'\n",
    "            pipe_df[v_avg_name] = np.nan\n",
    "            v_min_name = 'MDWF_' + str(pop_year) + '_V'\n",
    "            pipe_df[v_min_name] = np.nan\n",
    "            \n",
    "        else:\n",
    "            q_name = description + '_' + str(pop_year) + '_Q'\n",
    "            v_name = description + '_' + str(pop_year) + '_V'\n",
    "        pipe_df[q_name] = np.nan\n",
    "        pipe_df[v_name] = np.nan\n",
    "        for index, row in pipe_df.iterrows():\n",
    "            muid = row['MUID']\n",
    "            if muid in pipes:\n",
    "                q_list = list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[one_day_steps:]\n",
    "                q_list = [q*1000 for q in q_list]\n",
    "                v_list = list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[one_day_steps:]\n",
    "                \n",
    "                if absolute_velocity_discharge:\n",
    "                    q = max([abs(q) for q in q_list])\n",
    "                    v = max([abs(v) for v in v_list])\n",
    "                    q_min = min([abs(q) for q in q_list])\n",
    "                    v_min = min([abs(v) for v in v_list])\n",
    "                    \n",
    "                else:\n",
    "                    q = max(q_list)\n",
    "                    v = max(v_list)\n",
    "                    q_min = min(q_list)\n",
    "                    v_min = min(v_list)\n",
    "                    \n",
    "                pipe_df.loc[index,q_name] = q\n",
    "                pipe_df.loc[index,v_name] = v\n",
    "                \n",
    "                if description.lower() == 'dwf':\n",
    "                    pipe_df.loc[index,q_min_name] = q_min\n",
    "                    pipe_df.loc[index,v_min_name] = v_min\n",
    "                    \n",
    "                    q_avg = sum(list(res1d.query.GetReachStartValues(muid, \"Discharge\"))[-one_day_steps:])/one_day_steps*1000\n",
    "                    pipe_df.loc[index,q_avg_name] = q_avg\n",
    "                    v_avg = sum(list(res1d.query.GetReachStartValues(muid, \"FlowVelocity\"))[-one_day_steps:])/one_day_steps\n",
    "                    pipe_df.loc[index,v_avg_name] = v_avg\n",
    "                    \n",
    "\n",
    "if m_index == 0:\n",
    "    node_df_all = node_df.copy()\n",
    "    pipe_df_all = pipe_df.copy()\n",
    "else:\n",
    "    node_df_all = pd.concat([node_df_all,node_df])\n",
    "    pipe_df_all = pd.concat([pipe_df_all,pipe_df])\n",
    "    \n",
    "if len(acronym_filter) > 0:\n",
    "    node_df_all = node_df_all[node_df_all.Acronym.isin(acronym_filter)]\n",
    "    pipe_df_all = pipe_df_all[pipe_df_all.Acronym.isin(acronym_filter)]\n",
    "    \n",
    "node_df_all.drop(columns=['Match_Code'],inplace=True)\n",
    "                \n",
    "node_df_all.to_csv(output_folder + '\\\\Model_Nodes.csv', index=False)   \n",
    "pipe_df_all.to_csv(output_folder + '\\\\Model_Pipes.csv', index=False) \n",
    "print('Done')                \n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
